This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-10-31T07:12:53.492Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
src/
  data/
    data_loader.py
  models/
    data_classes.py
  nlp/
    analyzer.py
    topic-modeling.py
  processors/
    comment_processor.py
    conversation_processor.py
    post_processor.py
  utils/
    text_processor.py
  data_processor.py
  requirements.txt
main.py
package.json

================================================================
Repository Files
================================================================

================
File: src/data/data_loader.py
================
import json
import logging
from typing import Tuple, List, Dict, Any
from pathlib import Path

logger = logging.getLogger(__name__)

class DataLoader:
    def __init__(self, posts_file: str, comments_file: str):
        self.posts_file = Path(posts_file)
        self.comments_file = Path(comments_file)

    def load_data(self) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """Load posts and comments from JSON files in chunks"""
        try:
            logger.info(f"Loading posts from {self.posts_file}")
            posts = []
            with open(self.posts_file, 'r') as f:
                for line in f:
                    if line.strip():
                        try:
                            posts.append(json.loads(line.strip()))
                        except json.JSONDecodeError:
                            logger.warning(f"Skipped invalid JSON line in posts file")
                    if len(posts) % 1000 == 0:
                        logger.info(f"Loaded {len(posts)} posts...")
                        
            logger.info(f"Loading comments from {self.comments_file}")
            comments = []
            with open(self.comments_file, 'r') as f:
                for line in f:
                    if line.strip():
                        try:
                            comments.append(json.loads(line.strip()))
                        except json.JSONDecodeError:
                            logger.warning(f"Skipped invalid JSON line in comments file")
                    if len(comments) % 5000 == 0:
                        logger.info(f"Loaded {len(comments)} comments...")
                        
            logger.info(f"Loaded {len(posts)} posts and {len(comments)} comments")
            return posts, comments
        except Exception as e:
            logger.error(f"Error loading data: {e}")
            raise

================
File: src/models/data_classes.py
================
from dataclasses import dataclass, field
from datetime import datetime
from typing import List

@dataclass
class RedditComment:
    """Data class for storing normalized Reddit comment data"""
    comment_id: str
    post_id: str  # The ID of the parent post (link_id)
    parent_id: str  # Could be post_id or another comment_id
    content: str
    author: str
    timestamp: datetime
    score: int
    edited: bool
    intent: str = None
    sentiment: float = None
     # New topic-related fields
    topics: List[Dict[str, float]] = field(default_factory=list)  # List of {topic_id: probability} mappings
    dominant_topic: Optional[int] = None  # ID of the topic with highest probability
    topic_probabilities: Dict[int, float] = field(default_factory=dict)  # Full topic distribution


@dataclass
class RedditPost:
    """Data class for storing normalized Reddit post data"""
    post_id: str
    title: str
    content: str
    author: str
    timestamp: datetime
    score: int
    num_comments: int
    upvote_ratio: float
    over_18: bool
    edited: bool
    comments: List[RedditComment] = field(default_factory=list)
    intent: str = None
    sentiment: float = None
     # New topic-related fields
    topics: List[Dict[str, float]] = field(default_factory=list)  # List of {topic_id: probability} mappings
    title_topics: List[Dict[str, float]] = field(default_factory=list)  # Topic distribution for title
    content_topics: List[Dict[str, float]] = field(default_factory=list)  # Topic distribution for content
    dominant_topic: Optional[int] = None  # ID of the topic with highest probability
    topic_probabilities: Dict[int, float] = field(default_factory=dict)  # Full topic distribution

================
File: src/nlp/analyzer.py
================
from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import logging
from typing import List, Any

logger = logging.getLogger(__name__)

class NLPAnalyzer:
    def __init__(self, batch_size: int = 64, use_gpu: bool = True):
        self.batch_size = batch_size
        self.device = torch.device("cuda" if torch.cuda.is_available() and use_gpu else "cpu")
        
        logger.info(f"Initializing NLP models on {self.device}")
        
        # Initialize tokenizer for length checking
        self.tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
        self.max_length = 512  # Maximum sequence length for the model
        
        self.sentiment_analyzer = pipeline(
            "sentiment-analysis",
            model="distilbert-base-uncased-finetuned-sst-2-english",
            device=0 if torch.cuda.is_available() and use_gpu else -1,
            batch_size=self.batch_size
        )
        
        self.intent_classifier = pipeline(
            "zero-shot-classification",
            model="facebook/bart-large-mnli",
            device=0 if torch.cuda.is_available() and use_gpu else -1,
            batch_size=self.batch_size
        )

    def _truncate_text(self, text: str) -> str:
        """Truncate text to fit within model's maximum sequence length"""
        tokens = self.tokenizer.encode(text, add_special_tokens=True)
        if len(tokens) > self.max_length:
            logger.debug(f"Truncating text from {len(tokens)} tokens to {self.max_length} tokens")
            truncated_tokens = tokens[:self.max_length - 1] + [tokens[-1]]  # Keep [SEP] token
            return self.tokenizer.decode(truncated_tokens, skip_special_tokens=True)
        return text

    def detect_intent(self, text: str) -> str:
        """Detect the intent of the text using zero-shot classification"""
        if not text.strip():
            return "unknown"
            
        try:
            truncated_text = self._truncate_text(text)
            result = self.intent_classifier(
                truncated_text,
                candidate_labels=["question", "opinion", "answer", "discussion"],
                hypothesis_template="This text is expressing a {}."
            )
            return result['labels'][0]
        except Exception as e:
            logger.warning(f"Error detecting intent: {e}")
            return "unknown"
    
    def analyze_sentiment(self, text: str) -> float:
        """Analyze the sentiment of the text"""
        if not text.strip():
            return 0.0
            
        try:
            truncated_text = self._truncate_text(text)
            result = self.sentiment_analyzer(truncated_text)
            score = result[0]['score']
            return score if result[0]['label'] == 'POSITIVE' else -score
        except Exception as e:
            logger.warning(f"Error analyzing sentiment: {e}")
            return 0.0

    def process_batch(self, texts: List[str], processor_fn) -> List[Any]:
        """Process a batch of texts using the specified processor function"""
        results = []
        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]
            batch_results = [processor_fn(text) for text in batch]
            results.extend(batch_results)
        return results

================
File: src/nlp/topic-modeling.py
================
# src/nlp/topic_modeling.py

import logging
from typing import List, Dict, Any, Tuple
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
from sklearn.preprocessing import normalize
import pandas as pd
from tqdm import tqdm

logger = logging.getLogger(__name__)

class TopicModeler:
    def __init__(self, 
                 method: str = 'lda',
                 n_topics: int = 10,
                 max_features: int = 10000,
                 batch_size: int = 128,
                 n_jobs: int = -1):
        """
        Initialize the topic modeling system.
        
        Args:
            method (str): Topic modeling method ('lda' or 'nmf')
            n_topics (int): Number of topics to extract
            max_features (int): Maximum number of features for vocabulary
            batch_size (int): Batch size for processing
            n_jobs (int): Number of jobs for parallel processing
        """
        self.method = method.lower()
        self.n_topics = n_topics
        self.max_features = max_features
        self.batch_size = batch_size
        self.n_jobs = n_jobs
        
        # Initialize vectorizers
        self.count_vectorizer = CountVectorizer(
            max_features=max_features,
            stop_words='english',
            max_df=0.95,  # Ignore terms that appear in >95% of docs
            min_df=2      # Ignore terms that appear in <2 documents
        )
        
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=max_features,
            stop_words='english',
            max_df=0.95,
            min_df=2
        )
        
        # Initialize topic model
        if self.method == 'lda':
            self.model = LatentDirichletAllocation(
                n_components=n_topics,
                max_iter=20,
                learning_method='online',
                batch_size=batch_size,
                n_jobs=n_jobs,
                random_state=42
            )
        elif self.method == 'nmf':
            self.model = NMF(
                n_components=n_topics,
                init='nndsvd',
                max_iter=200,
                random_state=42
            )
        else:
            raise ValueError(f"Unsupported method: {method}")
            
        self.feature_names = None
        self.document_topics = None
        self.topic_terms = None
        
    def preprocess_texts(self, texts: List[str]) -> np.ndarray:
        """
        Preprocess and vectorize the input texts.
        
        Args:
            texts: List of text documents
            
        Returns:
            Document-term matrix
        """
        logger.info(f"Preprocessing {len(texts)} documents...")
        
        try:
            if self.method == 'lda':
                # Use count vectorization for LDA
                dtm = self.count_vectorizer.fit_transform(texts)
                self.feature_names = self.count_vectorizer.get_feature_names_out()
            else:
                # Use TF-IDF for NMF
                dtm = self.tfidf_vectorizer.fit_transform(texts)
                self.feature_names = self.tfidf_vectorizer.get_feature_names_out()
                
            logger.info(f"Vocabulary size: {len(self.feature_names)}")
            return dtm
            
        except Exception as e:
            logger.error(f"Error in preprocessing: {e}")
            raise
            
    def fit_transform(self, texts: List[str]) -> np.ndarray:
        """
        Fit the topic model and transform documents.
        
        Args:
            texts: List of text documents
            
        Returns:
            Document-topic matrix
        """
        try:
            # Preprocess texts
            dtm = self.preprocess_texts(texts)
            
            logger.info("Fitting topic model...")
            self.document_topics = self.model.fit_transform(dtm)
            
            # Get topic-term matrix
            self.topic_terms = self.model.components_
            
            return self.document_topics
            
        except Exception as e:
            logger.error(f"Error in fit_transform: {e}")
            raise
            
    def get_topic_terms(self, n_terms: int = 10) -> List[List[str]]:
        """
        Get the top terms for each topic.
        
        Args:
            n_terms: Number of terms to return per topic
            
        Returns:
            List of top terms for each topic
        """
        topics = []
        for topic_idx in range(self.n_topics):
            top_term_indices = self.topic_terms[topic_idx].argsort()[:-n_terms-1:-1]
            topics.append([
                self.feature_names[i] for i in top_term_indices
            ])
        return topics
        
    def get_document_topics(self, 
                          threshold: float = 0.1) -> List[List[Tuple[int, float]]]:
        """
        Get topic assignments for each document.
        
        Args:
            threshold: Minimum probability threshold for topic assignment
            
        Returns:
            List of (topic_id, probability) tuples for each document
        """
        doc_topics = []
        for doc_topic_dist in self.document_topics:
            # Normalize probabilities
            probs = normalize(doc_topic_dist.reshape(1, -1))[0]
            
            # Get topics above threshold
            topic_probs = [
                (topic_idx, prob) 
                for topic_idx, prob in enumerate(probs)
                if prob > threshold
            ]
            
            # Sort by probability
            topic_probs.sort(key=lambda x: x[1], reverse=True)
            doc_topics.append(topic_probs)
            
        return doc_topics
        
    def get_topic_summary(self, n_terms: int = 10) -> List[Dict[str, Any]]:
        """
        Get a summary of all topics with their top terms.
        
        Args:
            n_terms: Number of terms to include per topic
            
        Returns:
            List of topic summaries with terms and term weights
        """
        summaries = []
        for topic_idx in range(self.n_topics):
            # Get top terms and their weights
            term_weights = self.topic_terms[topic_idx]
            top_term_indices = term_weights.argsort()[:-n_terms-1:-1]
            
            terms = []
            for idx in top_term_indices:
                terms.append({
                    'term': self.feature_names[idx],
                    'weight': float(term_weights[idx])
                })
            
            summaries.append({
                'topic_id': topic_idx,
                'terms': terms
            })
            
        return summaries
        
    def save_topic_model(self, output_dir: str):
        """
        Save topic model results to files.
        
        Args:
            output_dir: Directory to save files
        """
        try:
            # Save topic terms
            topic_terms = self.get_topic_terms()
            pd.DataFrame(topic_terms).to_csv(
                f"{output_dir}/topic_terms.csv",
                index=True,
                header=[f"term_{i}" for i in range(len(topic_terms[0]))]
            )
            
            # Save topic summaries
            summaries = self.get_topic_summary()
            pd.DataFrame(summaries).to_json(
                f"{output_dir}/topic_summaries.json",
                orient='records',
                indent=2
            )
            
            logger.info(f"Saved topic model results to {output_dir}")
            
        except Exception as e:
            logger.error(f"Error saving topic model: {e}")
            raise

================
File: src/processors/comment_processor.py
================
from datetime import datetime
from typing import Dict, Any
import logging
from ..models.data_classes import RedditComment
from ..utils.text_processor import TextProcessor
from ..nlp.analyzer import NLPAnalyzer

logger = logging.getLogger(__name__)

class CommentProcessor:
    def __init__(self, nlp_analyzer: NLPAnalyzer):
        self.nlp_analyzer = nlp_analyzer

    def process_comment(self, comment_data: Dict[str, Any]) -> RedditComment:
        """Process a single Reddit comment"""
        try:
            comment_id = TextProcessor.parse_reddit_id(comment_data.get('name', ''))
            post_id = TextProcessor.parse_reddit_id(comment_data.get('link_id', ''))
            parent_id = TextProcessor.parse_reddit_id(comment_data.get('parent_id', ''))
            
            # Clean the content
            cleaned_content = TextProcessor.clean_text(comment_data.get('body', ''))
            
            comment = RedditComment(
                comment_id=comment_id,
                post_id=post_id,
                parent_id=parent_id,
                content=cleaned_content,
                author=comment_data.get('author', '[deleted]'),
                timestamp=datetime.fromtimestamp(comment_data.get('created_utc', 0)),
                score=comment_data.get('score', 0),
                edited=bool(comment_data.get('edited', False))
            )
            
            # Add intent and sentiment analysis
            if cleaned_content:
                comment.intent = self.nlp_analyzer.detect_intent(cleaned_content)
                comment.sentiment = self.nlp_analyzer.analyze_sentiment(cleaned_content)
                
            return comment
        except Exception as e:
            logger.error(f"Error processing comment {comment_data.get('name', 'unknown')}: {e}")
            raise

================
File: src/processors/conversation_processor.py
================
from typing import Dict, List, Any
from ..models.data_classes import RedditPost, RedditComment

class ConversationProcessor:
    @staticmethod
    def build_conversation_tree(posts: List[RedditPost], comments: List[RedditComment]) -> Dict[str, RedditPost]:
        """Build conversation trees by linking comments to their posts and parent comments"""
        # Create a dictionary of posts for easy lookup
        posts_dict = {post.post_id: post for post in posts}
        
        # Create a dictionary of comments for easy lookup
        comments_dict = {comment.comment_id: comment for comment in comments}
        
        # Link comments to their parents
        for comment in comments:
            if comment.post_id in posts_dict:
                # If parent is a post, add to post's comments
                if comment.parent_id == comment.post_id:
                    posts_dict[comment.post_id].comments.append(comment)
                # If parent is another comment, it's a reply
                elif comment.parent_id in comments_dict:
                    parent_comment = comments_dict[comment.parent_id]
                    if not hasattr(parent_comment, 'replies'):
                        parent_comment.replies = []
                    parent_comment.replies.append(comment)
        
        return posts_dict

    @staticmethod
    def create_conversation_pairs(posts_dict: Dict[str, RedditPost]) -> List[Dict[str, Any]]:
        """Create conversation pairs from posts and comments"""
        conversation_pairs = []
        
        for post in posts_dict.values():
            # Create pairs between post and direct comments
            for comment in post.comments:
                pair = {
                    'post_id': post.post_id,
                    'context': post.content,
                    'response': comment.content,
                    'context_intent': post.intent,
                    'response_intent': comment.intent,
                    'context_sentiment': post.sentiment,
                    'response_sentiment': comment.sentiment,
                    'context_author': post.author,
                    'response_author': comment.author,
                    'score': comment.score,
                    'timestamp': comment.timestamp.isoformat()
                }
                conversation_pairs.append(pair)
                
                # If comment has replies, create pairs between comment and replies
                if hasattr(comment, 'replies'):
                    for reply in comment.replies:
                        pair = {
                            'post_id': post.post_id,
                            'context': comment.content,
                            'response': reply.content,
                            'context_intent': comment.intent,
                            'response_intent': reply.intent,
                            'context_sentiment': comment.sentiment,
                            'response_sentiment': reply.sentiment,
                            'context_author': comment.author,
                            'response_author': reply.author,
                            'score': reply.score,
                            'timestamp': reply.timestamp.isoformat()
                        }
                        conversation_pairs.append(pair)
        
        return conversation_pairs

================
File: src/processors/post_processor.py
================
from datetime import datetime
from typing import Dict, Any
import logging
from ..models.data_classes import RedditPost
from ..utils.text_processor import TextProcessor
from ..nlp.analyzer import NLPAnalyzer

logger = logging.getLogger(__name__)

class PostProcessor:
    def __init__(self, nlp_analyzer: NLPAnalyzer):
        self.nlp_analyzer = nlp_analyzer

    def process_post(self, post_data: Dict[str, Any]) -> RedditPost:
        """Process a single Reddit post"""
        try:
            post_id = TextProcessor.parse_reddit_id(post_data.get('name', ''))
            
            # Clean the content
            cleaned_content = TextProcessor.clean_text(post_data.get('selftext', ''))
            cleaned_title = TextProcessor.clean_text(post_data.get('title', ''))
            
            post = RedditPost(
                post_id=post_id,
                title=cleaned_title,
                content=cleaned_content,
                author=post_data.get('author', '[deleted]'),
                timestamp=datetime.fromtimestamp(post_data.get('created_utc', 0)),
                score=post_data.get('score', 0),
                num_comments=post_data.get('num_comments', 0),
                upvote_ratio=post_data.get('upvote_ratio', 0.0),
                over_18=post_data.get('over_18', False),
                edited=bool(post_data.get('edited', False)),
                comments=[]
            )
            
            # Add intent and sentiment analysis
            if cleaned_content:
                post.intent = self.nlp_analyzer.detect_intent(cleaned_content)
                post.sentiment = self.nlp_analyzer.analyze_sentiment(cleaned_content)
                
            return post
        except Exception as e:
            logger.error(f"Error processing post {post_data.get('name', 'unknown')}: {e}")
            raise

================
File: src/utils/text_processor.py
================
import re
import emoji
from typing import List, Any
import logging

logger = logging.getLogger(__name__)

class TextProcessor:
    @staticmethod
    def clean_text(text: str) -> str:
        """Clean and normalize text content"""
        if not isinstance(text, str):
            return ""
            
        # Remove URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        
        # Remove emoji but keep the text representation
        text = emoji.demojize(text)
        
        # Remove special characters but keep basic punctuation
        text = re.sub(r'[^\w\s.,!?]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()

    @staticmethod
    def parse_reddit_id(full_id: str) -> str:
        """Extract the base ID from Reddit's fullname format"""
        if full_id and '_' in full_id:
            return full_id.split('_')[1]
        return full_id

================
File: src/data_processor.py
================
# src/data_processor.py

import logging
import json
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any, Generator, Iterator, Optional
from sklearn.model_selection import train_test_split
from dataclasses import asdict
import torch
from tqdm import tqdm
import gc
from .nlp.topic_modeling import TopicModeler


from .data.data_loader import DataLoader
from .nlp.analyzer import NLPAnalyzer
from .processors.post_processor import PostProcessor
from .processors.comment_processor import CommentProcessor
from .processors.conversation_processor import ConversationProcessor
from .models.data_classes import RedditPost, RedditComment

logger = logging.getLogger(__name__)

class GPUOptimizedProcessor:
    def __init__(self, 
                 posts_file: str, 
                 comments_file: str, 
                 output_dir: str, 
                 batch_size: int = 256,
                 chunk_size: int = 1000):
        """
        Initialize the Reddit data processor optimized for GPU processing.
        
        Args:
            posts_file (str): Path to the posts JSON file
            comments_file (str): Path to the comments JSON file
            output_dir (str): Directory for output files
            batch_size (int): Size of batches for NLP processing
            chunk_size (int): Size of chunks for data processing
        """
        self.data_loader = DataLoader(posts_file, comments_file)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # GPU optimized settings
        self.CHUNK_SIZE = chunk_size
        self.BATCH_SIZE = batch_size
        
        # Initialize components
        self.nlp_analyzer = self._initialize_nlp()
        self.post_processor = PostProcessor(self.nlp_analyzer)
        self.comment_processor = CommentProcessor(self.nlp_analyzer)
        self.conversation_processor = ConversationProcessor()
        
        # Initialize state tracking
        self.processed_posts: Optional[List[RedditPost]] = None
        self.processed_comments: Optional[List[RedditComment]] = None
        self.posts_dict: Optional[Dict[str, RedditPost]] = None

    def _initialize_nlp(self) -> NLPAnalyzer:
        """Initialize NLP analyzer with GPU support"""
        if torch.cuda.is_available():
            logger.info(f"Initializing NLP analyzer with GPU support: {torch.cuda.get_device_name(0)}")
        else:
            logger.warning("GPU not available, falling back to CPU")
        
        return NLPAnalyzer(batch_size=self.BATCH_SIZE)

    def process_in_chunks(self, 
                         items: Iterator, 
                         processor_func, 
                         desc: str,
                         chunk_size: Optional[int] = None) -> Generator:
        """
        Process items in chunks optimized for GPU.
        
        Args:
            items: Iterator of items to process
            processor_func: Function to process each item
            desc: Description for progress bar
            chunk_size: Optional override for chunk size
            
        Yields:
            Processed items one at a time
        """
        chunk_size = chunk_size or self.CHUNK_SIZE
        chunk = []
        total_processed = 0
        
        for item in tqdm(items, desc=desc):
            chunk.append(item)
            if len(chunk) >= chunk_size:
                for processed_item in self._process_chunk(chunk, processor_func):
                    yield processed_item
                    total_processed += 1
                    
                    if total_processed % (chunk_size * 5) == 0:
                        logger.info(f"Processed {total_processed} items")
                        self._cleanup_gpu_memory()
                
                chunk = []
        
        # Process remaining items
        if chunk:
            yield from self._process_chunk(chunk, processor_func)

    def _process_chunk(self, chunk: List, processor_func) -> List:
        """
        Process a single chunk of data with error handling.
        
        Args:
            chunk: List of items to process
            processor_func: Function to process each item
            
        Returns:
            List of processed items
        """
        try:
            processed_items = []
            for item in chunk:
                try:
                    processed = processor_func(item)
                    if processed is not None:
                        processed_items.append(processed)
                except Exception as e:
                    logger.error(f"Error processing individual item: {e}")
            return processed_items
        except Exception as e:
            logger.error(f"Error processing chunk: {e}")
            return []

    def _cleanup_gpu_memory(self):
        """Clean up GPU memory"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

    def save_to_file(self, 
                     data: List[Dict], 
                     filename: str, 
                     mode: str = 'w',
                     chunk_size: int = 1000):
        """
        Save processed data to file with chunking for large datasets.
        
        Args:
            data: List of data to save
            filename: Output filename
            mode: Write mode ('w' for write, 'a' for append)
            chunk_size: Size of chunks for writing
        """
        file_path = self.output_dir / filename
        
        if mode == 'w' and file_path.exists():
            file_path.unlink()
        
        try:
            if filename.endswith('.csv'):
                self._save_csv(data, file_path, mode, chunk_size)
            elif filename.endswith('.json'):
                self._save_json(data, file_path, mode, chunk_size)
            else:
                raise ValueError(f"Unsupported file format: {filename}")
                
        except Exception as e:
            logger.error(f"Error saving to {filename}: {e}")
            raise

    def _save_csv(self, 
                  data: List[Dict], 
                  file_path: Path, 
                  mode: str, 
                  chunk_size: int):
        """Save data to CSV file"""
        for i in range(0, len(data), chunk_size):
            chunk = data[i:i + chunk_size]
            df = pd.DataFrame(chunk)
            df.to_csv(
                file_path, 
                mode=mode, 
                header=(i == 0 or mode == 'w'), 
                index=False
            )

    def _save_json(self, 
                   data: List[Dict], 
                   file_path: Path, 
                   mode: str, 
                   chunk_size: int):
        """Save data to JSON file"""
        if mode == 'w':
            with open(file_path, 'w') as f:
                json.dump(data, f, indent=2)
        else:
            for i in range(0, len(data), chunk_size):
                chunk = data[i:i + chunk_size]
                with open(file_path, 'a') as f:
                    if i == 0:
                        f.write('[\n')
                    json.dump(chunk, f, indent=2)
                    if i + chunk_size < len(data):
                        f.write(',\n')
                    else:
                        f.write('\n]')

    def _process_posts(self, raw_posts: List[Dict]) -> List[RedditPost]:
        """Process all posts"""
        logger.info("Processing posts...")
        return list(self.process_in_chunks(
            raw_posts,
            self.post_processor.process_post,
            "Processing posts"
        ))

    def _process_comments(self, raw_comments: List[Dict]) -> List[RedditComment]:
        """Process all comments"""
        logger.info("Processing comments...")
        return list(self.process_in_chunks(
            raw_comments,
            self.comment_processor.process_comment,
            "Processing comments"
        ))

    def _create_conversation_pairs(self) -> List[Dict]:
        """Create conversation pairs from processed posts and comments"""
        logger.info("Creating conversation pairs...")
        self.posts_dict = self.conversation_processor.build_conversation_tree(
            self.processed_posts,
            self.processed_comments
        )
        return self.conversation_processor.create_conversation_pairs(self.posts_dict)

    def _split_and_save_data(self, conversation_pairs: List[Dict]):
        """Split data into train/test sets and save"""
        logger.info("Splitting and saving data...")
        train_pairs, test_pairs = train_test_split(
            conversation_pairs,
            test_size=0.2,
            random_state=42
        )
        
        # Save training data
        self.save_to_file(train_pairs, 'train_conversations.csv')
        
        # Save test data
        self.save_to_file(test_pairs, 'test_conversations.csv')
        
        # Save processed posts
        self.save_to_file(
            [asdict(post) for post in self.processed_posts],
            'processed_posts.json'
        )

    def process_data(self):
        """
        Main method to process the Reddit data with GPU optimization.
        
        This method orchestrates the entire processing pipeline:
        1. Loads raw data
        2. Processes posts and comments
        3. Creates conversation pairs
        4. Splits into train/test sets
        5. Saves all processed data
        """
        try:
            logger.info("Starting data processing with GPU optimization...")
            
            # Load raw data
            raw_posts, raw_comments = self.data_loader.load_data()
            
            # Process posts
            self.processed_posts = self._process_posts(raw_posts)
            
            # Process comments
            self.processed_comments = self._process_comments(raw_comments)
            self._extract_topics()
            # Create conversation pairs
            conversation_pairs = self._create_conversation_pairs()
            
            # Split and save data
            self._split_and_save_data(conversation_pairs)
            
            
            
            logger.info("Processing completed successfully!")
            
        except Exception as e:
            logger.error(f"Error processing data: {e}")
            raise
        finally:
            self._cleanup_gpu_memory()

    def get_processing_stats(self) -> Dict[str, Any]:
        """Get statistics about the processed data"""
        stats = {
            "total_posts": len(self.processed_posts) if self.processed_posts else 0,
            "total_comments": len(self.processed_comments) if self.processed_comments else 0,
            "gpu_available": torch.cuda.is_available(),
            "gpu_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
            "batch_size": self.BATCH_SIZE,
            "chunk_size": self.CHUNK_SIZE
        }
        
        if self.posts_dict:
            stats["posts_with_comments"] = sum(
                1 for post in self.posts_dict.values() if post.comments
            )
            
        return stats

    def __enter__(self):
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        self._cleanup_gpu_memory()

================
File: src/requirements.txt
================
# Core data processing
pandas>=1.5.0
numpy>=1.21.0

# Natural Language Processing
transformers>=4.30.0
torch>=2.0.0  # Required for transformers
scikit-learn>=1.0.0  # For train_test_split

# Text processing
emoji>=2.2.0
regex>=2023.5.5  # For advanced text processing

# Progress tracking and formatting
tqdm>=4.65.0
rich>=13.0.0  # For better console output

# Type checking
typing-extensions>=4.5.0  # For advanced type hints

# Date handli
python-dateutil>=2.8.2

ujson>=5.7.0  # Faster JSON processing
numba>=0.57.0  # For numerical computations

scikit-learn>=1.0.0
scipy>=1.7.0

================
File: main.py
================
import logging
from src.data_processor import GPUOptimizedProcessor
import torch

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def main():
    """Main function with GPU optimization"""
    try:
        # Configure paths
        posts_file = "data/gettingbigger_submissions.json"
        comments_file = "data/gettingbigger_comments.json"
        output_dir = "processed_data"
        
        # Log GPU availability
        logger.info(f"CUDA available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            logger.info(f"GPU device: {torch.cuda.get_device_name(0)}")
            logger.info(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        
        # Initialize and run the GPU-optimized processor
        processor = GPUOptimizedProcessor(posts_file, comments_file, output_dir)
        processor.process_data()
            
    except Exception as e:
        logger.error(f"Error in main: {e}")
        raise

if __name__ == "__main__":
    main()

================
File: package.json
================
{
  "dependencies": {
    "malloc": "^1.1.0"
  }
}

This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-11-06T20:18:53.029Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
src/
  data/
    data_loader.py
  models/
    data_classes.py
  nlp/
    analyzer.py
    language_analyzer.py
    topic_modeling.py
  processors/
    comment_processor.py
    conversation_processor.py
    post_processor.py
  utils/
    text_processor.py
  data_processor.py
  requirements.txt
executive_summary.py
main.py
package.json

================================================================
Repository Files
================================================================

================
File: src/data/data_loader.py
================
import json
import logging
from typing import Tuple, List, Dict, Any
from pathlib import Path

logger = logging.getLogger(__name__)

class DataLoader:
    def __init__(self, posts_file: str, comments_file: str):
        self.posts_file = Path(posts_file)
        self.comments_file = Path(comments_file)

    def load_data(self) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """Load posts and comments from JSON files in chunks"""
        try:
            logger.info(f"Loading posts from {self.posts_file}")
            posts = []
            with open(self.posts_file, 'r') as f:
                for line in f:
                    if line.strip():
                        try:
                            posts.append(json.loads(line.strip()))
                        except json.JSONDecodeError:
                            logger.warning(f"Skipped invalid JSON line in posts file")
                    if len(posts) % 1000 == 0:
                        logger.info(f"Loaded {len(posts)} posts...")
                        
            logger.info(f"Loading comments from {self.comments_file}")
            comments = []
            with open(self.comments_file, 'r') as f:
                for line in f:
                    if line.strip():
                        try:
                            comments.append(json.loads(line.strip()))
                        except json.JSONDecodeError:
                            logger.warning(f"Skipped invalid JSON line in comments file")
                    if len(comments) % 5000 == 0:
                        logger.info(f"Loaded {len(comments)} comments...")
                        
            logger.info(f"Loaded {len(posts)} posts and {len(comments)} comments")
            return posts, comments
        except Exception as e:
            logger.error(f"Error loading data: {e}")
            raise

================
File: src/models/data_classes.py
================
from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Optional

@dataclass
class RedditComment:
    """Data class for storing normalized Reddit comment data"""
    comment_id: str
    post_id: str  # The ID of the parent post (link_id)
    parent_id: str  # Could be post_id or another comment_id
    content: str
    author: str
    timestamp: datetime
    score: int
    edited: bool
    intent: str = None
    sentiment: float = None
     # New topic-related fields
    topics: List[dict[str, float]] = field(default_factory=list)  # List of {topic_id: probability} mappings
    dominant_topic: Optional[int] = None  # ID of the topic with highest probability
    topic_probabilities: dict[int, float] = field(default_factory=dict)  # Full topic distribution


@dataclass
class RedditPost:
    """Data class for storing normalized Reddit post data"""
    post_id: str
    title: str
    content: str
    author: str
    timestamp: datetime
    score: int
    num_comments: int
    upvote_ratio: float
    over_18: bool
    edited: bool
    comments: List[RedditComment] = field(default_factory=list)
    intent: str = None
    sentiment: float = None
     # New topic-related fields
    topics: List[dict[str, float]] = field(default_factory=list)  # List of {topic_id: probability} mappings
    title_topics: List[dict[str, float]] = field(default_factory=list)  # Topic distribution for title
    content_topics: List[dict[str, float]] = field(default_factory=list)  # Topic distribution for content
    dominant_topic: Optional[int] = None  # ID of the topic with highest probability
    topic_probabilities: dict[int, float] = field(default_factory=dict)  # Full topic distribution

================
File: src/nlp/analyzer.py
================
from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import logging
from typing import List, Any

logger = logging.getLogger(__name__)

class NLPAnalyzer:
    def __init__(self, batch_size: int = 64, use_gpu: bool = True):
        self.batch_size = batch_size
        self.device = torch.device("cuda" if torch.cuda.is_available() and use_gpu else "cpu")
        
        logger.info(f"Initializing NLP models on {self.device}")
        
        # Initialize tokenizer for length checking
        self.tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
        self.max_length = 512  # Maximum sequence length for the model
        
        self.sentiment_analyzer = pipeline(
            "sentiment-analysis",
            model="distilbert-base-uncased-finetuned-sst-2-english",
            device=0 if torch.cuda.is_available() and use_gpu else -1,
            batch_size=self.batch_size
        )
        
        self.intent_classifier = pipeline(
            "zero-shot-classification",
            model="facebook/bart-large-mnli",
            device=0 if torch.cuda.is_available() and use_gpu else -1,
            batch_size=self.batch_size
        )

    def _truncate_text(self, text: str) -> str:
        """Truncate text to fit within model's maximum sequence length"""
        tokens = self.tokenizer.encode(text, add_special_tokens=True)
        if len(tokens) > self.max_length:
            logger.debug(f"Truncating text from {len(tokens)} tokens to {self.max_length} tokens")
            truncated_tokens = tokens[:self.max_length - 1] + [tokens[-1]]  # Keep [SEP] token
            return self.tokenizer.decode(truncated_tokens, skip_special_tokens=True)
        return text

    def detect_intent(self, text: str) -> str:
        """Detect the intent of the text using zero-shot classification"""
        if not text.strip():
            return "unknown"
            
        try:
            truncated_text = self._truncate_text(text)
            result = self.intent_classifier(
                truncated_text,
                candidate_labels=["question", "opinion", "answer", "discussion"],
                hypothesis_template="This text is expressing a {}."
            )
            return result['labels'][0]
        except Exception as e:
            logger.warning(f"Error detecting intent: {e}")
            return "unknown"
    
    def analyze_sentiment(self, text: str) -> float:
        """Analyze the sentiment of the text"""
        if not text.strip():
            return 0.0
            
        try:
            truncated_text = self._truncate_text(text)
            result = self.sentiment_analyzer(truncated_text)
            score = result[0]['score']
            return score if result[0]['label'] == 'POSITIVE' else -score
        except Exception as e:
            logger.warning(f"Error analyzing sentiment: {e}")
            return 0.0

    def process_batch(self, texts: List[str], processor_fn) -> List[Any]:
        """Process a batch of texts using the specified processor function"""
        results = []
        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]
            batch_results = [processor_fn(text) for text in batch]
            results.extend(batch_results)
        return results

================
File: src/nlp/language_analyzer.py
================
# src/nlp/language_analyzer.py

from collections import Counter, defaultdict
import json
import logging
from typing import List, Dict, Any, Set, Tuple
import re
from nltk.util import ngrams
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
import spacy
import pandas as pd
from tqdm import tqdm
import nltk


logger = logging.getLogger(__name__)

class LanguageStyleAnalyzer:
    """
    Analyzes and extracts linguistic patterns, common phrases, and unique language features
    from subreddit content to help chatbots mimic community-specific communication styles.
    """
    
    def __init__(self, min_phrase_freq: int = 3, max_ngram_size: int = 3):
        """
        Initialize the language style analyzer.
        
        Args:
            min_phrase_freq (int): Minimum frequency for phrases to be considered common
            max_ngram_size (int): Maximum size of n-grams to analyze
        """
        self.min_phrase_freq = min_phrase_freq
        self.max_ngram_size = max_ngram_size
        
        # Initialize spaCy for better text processing
        try:
            self.nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])
        except OSError:
            logger.warning("Downloading spaCy model...")
            spacy.cli.download('en_core_web_sm')
            self.nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])
        
        # Get stop words but keep some important ones
        self.stop_words = set(stopwords.words('english')) - {
            'not', 'no', 'never', 'can', 'should', 'must', 'need',
            'would', 'could', 'might', 'may', 'will'
        }
        
        # Initialize storage for analysis results
        self.common_phrases = defaultdict(Counter)
        self.slang_terms = Counter()
        self.sentence_patterns = defaultdict(Counter)
        self.subreddit_specific_terms = Counter()
    
    def _preprocess_text(self, text: str) -> str:
        """
        Preprocess text for analysis.
        
        Args:
            text (str): Raw text content
            
        Returns:
            str: Cleaned and preprocessed text
        """
        # Convert to lowercase and remove URLs
        text = re.sub(r'http\S+|www\S+', '', text.lower())
        
        # Remove special characters but keep apostrophes for contractions
        text = re.sub(r'[^a-z0-9\s\']+', ' ', text)
        
        # Normalize whitespace
        text = ' '.join(text.split())
        
        return text
    
    def _extract_ngrams(self, text: str) -> Dict[int, Counter]:
        """
        Extract n-grams from text.
        
        Args:
            text (str): Preprocessed text
            
        Returns:
            Dict[int, Counter]: Dictionary of n-gram sizes to their frequency counts
        """
        tokens = word_tokenize(text)
        ngram_counts = {}
        
        for n in range(2, self.max_ngram_size + 1):
            text_ngrams = ngrams(tokens, n)
            ngram_counts[n] = Counter(' '.join(gram) for gram in text_ngrams)
        
        return ngram_counts
    
    def _identify_slang(self, text: str, common_words: Set[str]) -> List[str]:
        """
        Identify potential slang terms and informal language.
        
        Args:
            text (str): Preprocessed text
            common_words (Set[str]): Set of common English words
            
        Returns:
            List[str]: List of identified slang terms
        """
        # Tokenize and analyze with spaCy
        doc = self.nlp(text)
        
        slang_candidates = []
        for token in doc:
            word = token.text.lower()
            # Consider as slang if:
            # 1. Not in common words
            # 2. Not a stop word
            # 3. Not purely numeric
            # 4. Length > 2
            if (word not in common_words and 
                word not in self.stop_words and 
                not word.isnumeric() and 
                len(word) > 2):
                slang_candidates.append(word)
        
        return slang_candidates
    
    def _analyze_sentence_patterns(self, text: str) -> List[str]:
        """
        Extract common sentence structures and patterns.
        
        Args:
            text (str): Raw text content
            
        Returns:
            List[str]: List of sentence pattern templates
        """
        sentences = sent_tokenize(text)
        patterns = []
        
        for sentence in sentences:
            doc = self.nlp(sentence)
            # Create pattern template using POS tags
            pattern = ' '.join([token.pos_ for token in doc])
            patterns.append(pattern)
        
        return patterns
    
    def analyze_content(self, texts: List[str], reference_texts: List[str] = None):
        """
        Analyze a collection of texts to extract language patterns.
        
        Args:
            texts (List[str]): List of texts from the subreddit
            reference_texts (List[str], optional): General English texts for comparison
        """
        logger.info("Starting content analysis...")
        
        # Load common English words for comparison
        common_words = set(word.lower() for word in nltk.corpus.words.words())
        
        for text in tqdm(texts, desc="Analyzing texts"):
            if not text or not isinstance(text, str):
                continue
                
            processed_text = self._preprocess_text(text)
            
            # Extract and count n-grams
            ngram_counts = self._extract_ngrams(processed_text)
            for n, counts in ngram_counts.items():
                self.common_phrases[n].update(counts)
            
            # Identify slang and informal language
            slang = self._identify_slang(processed_text, common_words)
            self.slang_terms.update(slang)
            
            # Analyze sentence patterns
            patterns = self._analyze_sentence_patterns(text)
            self.sentence_patterns['templates'].update(patterns)
        
        # Filter and process results
        self._process_results(reference_texts)
        
    def _process_results(self, reference_texts: List[str] = None):
        """
        Process and filter analysis results.
        
        Args:
            reference_texts (List[str], optional): Reference texts for comparison
        """
        # Filter phrases by frequency
        for n in self.common_phrases:
            self.common_phrases[n] = Counter({
                phrase: count for phrase, count in self.common_phrases[n].items()
                if count >= self.min_phrase_freq
            })
        
        # If reference texts provided, identify subreddit-specific terms
        if reference_texts:
            reference_terms = Counter()
            for text in reference_texts:
                processed = self._preprocess_text(text)
                tokens = word_tokenize(processed)
                reference_terms.update(tokens)
            
            # Compare frequencies to identify unique terms
            for term, count in self.slang_terms.items():
                if term not in reference_terms or \
                   (count / sum(self.slang_terms.values()) > 
                    reference_terms[term] / sum(reference_terms.values()) * 2):
                    self.subreddit_specific_terms[term] = count
    
    def save_results(self, output_path: str):
        """
        Save analysis results to a JSON file.
        
        Args:
            output_path (str): Path to save the results
        """
        results = {
            'common_phrases': {
                f'{n}_grams': dict(self.common_phrases[n].most_common(50))
                for n in self.common_phrases
            },
            'slang_terms': dict(self.slang_terms.most_common(100)),
            'sentence_patterns': {
                'common_templates': dict(
                    self.sentence_patterns['templates'].most_common(50)
                )
            },
            'subreddit_specific_terms': dict(
                self.subreddit_specific_terms.most_common(100)
            )
        }
        
        # Add metadata
        results['metadata'] = {
            'total_phrases_analyzed': sum(
                len(counter) for counter in self.common_phrases.values()
            ),
            'total_slang_terms': len(self.slang_terms),
            'total_sentence_patterns': len(self.sentence_patterns['templates']),
            'analysis_parameters': {
                'min_phrase_freq': self.min_phrase_freq,
                'max_ngram_size': self.max_ngram_size
            }
        }
        
        # Save to file
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
            
        logger.info(f"Analysis results saved to {output_path}")
        
    def get_summary_statistics(self) -> Dict[str, Any]:
        """
        Get summary statistics of the analysis.
        
        Returns:
            Dict[str, Any]: Summary statistics
        """
        return {
            'total_unique_phrases': {
                f'{n}_grams': len(counter)
                for n, counter in self.common_phrases.items()
            },
            'total_slang_terms': len(self.slang_terms),
            'total_sentence_patterns': len(self.sentence_patterns['templates']),
            'top_phrases': {
                f'{n}_grams': dict(counter.most_common(10))
                for n, counter in self.common_phrases.items()
            },
            'top_slang': dict(self.slang_terms.most_common(10)),
            'common_patterns': dict(
                self.sentence_patterns['templates'].most_common(10)
            )
        }

================
File: src/nlp/topic_modeling.py
================
# src/nlp/topic_modeling.py

import logging
from typing import List, Dict, Any, Tuple
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
from sklearn.preprocessing import normalize
import pandas as pd
from tqdm import tqdm

logger = logging.getLogger(__name__)

class TopicModeler:
    def __init__(self, 
                 method: str = 'lda',
                 n_topics: int = 10,
                 max_features: int = 10000,
                 batch_size: int = 128,
                 n_jobs: int = -1):
        """
        Initialize the topic modeling system.
        
        Args:
            method (str): Topic modeling method ('lda' or 'nmf')
            n_topics (int): Number of topics to extract
            max_features (int): Maximum number of features for vocabulary
            batch_size (int): Batch size for processing
            n_jobs (int): Number of jobs for parallel processing
        """
        self.method = method.lower()
        self.n_topics = n_topics
        self.max_features = max_features
        self.batch_size = batch_size
        self.n_jobs = n_jobs
        
        # Initialize vectorizers
        self.count_vectorizer = CountVectorizer(
            max_features=max_features,
            stop_words='english',
            max_df=0.95,  # Ignore terms that appear in >95% of docs
            min_df=2      # Ignore terms that appear in <2 documents
        )
        
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=max_features,
            stop_words='english',
            max_df=0.95,
            min_df=2
        )
        
        # Initialize topic model
        if self.method == 'lda':
            self.model = LatentDirichletAllocation(
                n_components=n_topics,
                max_iter=20,
                learning_method='online',
                batch_size=batch_size,
                n_jobs=n_jobs,
                random_state=42
            )
        elif self.method == 'nmf':
            self.model = NMF(
                n_components=n_topics,
                init='nndsvd',
                max_iter=200,
                random_state=42
            )
        else:
            raise ValueError(f"Unsupported method: {method}")
            
        self.feature_names = None
        self.document_topics = None
        self.topic_terms = None
        
    def preprocess_texts(self, texts: List[str]) -> np.ndarray:
        """
        Preprocess and vectorize the input texts.
        
        Args:
            texts: List of text documents
            
        Returns:
            Document-term matrix
        """
        logger.info(f"Preprocessing {len(texts)} documents...")
        
        try:
            if self.method == 'lda':
                # Use count vectorization for LDA
                dtm = self.count_vectorizer.fit_transform(texts)
                self.feature_names = self.count_vectorizer.get_feature_names_out()
            else:
                # Use TF-IDF for NMF
                dtm = self.tfidf_vectorizer.fit_transform(texts)
                self.feature_names = self.tfidf_vectorizer.get_feature_names_out()
                
            logger.info(f"Vocabulary size: {len(self.feature_names)}")
            return dtm
            
        except Exception as e:
            logger.error(f"Error in preprocessing: {e}")
            raise
            
    def fit_transform(self, texts: List[str]) -> np.ndarray:
        """
        Fit the topic model and transform documents.
        
        Args:
            texts: List of text documents
            
        Returns:
            Document-topic matrix
        """
        try:
            # Preprocess texts
            dtm = self.preprocess_texts(texts)
            
            logger.info("Fitting topic model...")
            self.document_topics = self.model.fit_transform(dtm)
            
            # Get topic-term matrix
            self.topic_terms = self.model.components_
            
            return self.document_topics
            
        except Exception as e:
            logger.error(f"Error in fit_transform: {e}")
            raise
            
    def get_topic_terms(self, n_terms: int = 10) -> List[List[str]]:
        """
        Get the top terms for each topic.
        
        Args:
            n_terms: Number of terms to return per topic
            
        Returns:
            List of top terms for each topic
        """
        topics = []
        for topic_idx in range(self.n_topics):
            top_term_indices = self.topic_terms[topic_idx].argsort()[:-n_terms-1:-1]
            topics.append([
                self.feature_names[i] for i in top_term_indices
            ])
        return topics
        
    def get_document_topics(self, 
                          threshold: float = 0.1) -> List[List[Tuple[int, float]]]:
        """
        Get topic assignments for each document.
        
        Args:
            threshold: Minimum probability threshold for topic assignment
            
        Returns:
            List of (topic_id, probability) tuples for each document
        """
        doc_topics = []
        for doc_topic_dist in self.document_topics:
            # Normalize probabilities
            probs = normalize(doc_topic_dist.reshape(1, -1))[0]
            
            # Get topics above threshold
            topic_probs = [
                (topic_idx, prob) 
                for topic_idx, prob in enumerate(probs)
                if prob > threshold
            ]
            
            # Sort by probability
            topic_probs.sort(key=lambda x: x[1], reverse=True)
            doc_topics.append(topic_probs)
            
        return doc_topics
        
    def get_topic_summary(self, n_terms: int = 10) -> List[Dict[str, Any]]:
        """
        Get a summary of all topics with their top terms.
        
        Args:
            n_terms: Number of terms to include per topic
            
        Returns:
            List of topic summaries with terms and term weights
        """
        summaries = []
        for topic_idx in range(self.n_topics):
            # Get top terms and their weights
            term_weights = self.topic_terms[topic_idx]
            top_term_indices = term_weights.argsort()[:-n_terms-1:-1]
            
            terms = []
            for idx in top_term_indices:
                terms.append({
                    'term': self.feature_names[idx],
                    'weight': float(term_weights[idx])
                })
            
            summaries.append({
                'topic_id': topic_idx,
                'terms': terms
            })
            
        return summaries
        
    def save_topic_model(self, output_dir: str):
        """
        Save topic model results to files.
        
        Args:
            output_dir: Directory to save files
        """
        try:
            # Save topic terms
            topic_terms = self.get_topic_terms()
            pd.DataFrame(topic_terms).to_csv(
                f"{output_dir}/topic_terms.csv",
                index=True,
                header=[f"term_{i}" for i in range(len(topic_terms[0]))]
            )
            
            # Save topic summaries
            summaries = self.get_topic_summary()
            pd.DataFrame(summaries).to_json(
                f"{output_dir}/topic_summaries.json",
                orient='records',
                indent=2
            )
            
            logger.info(f"Saved topic model results to {output_dir}")
            
        except Exception as e:
            logger.error(f"Error saving topic model: {e}")
            raise

================
File: src/processors/comment_processor.py
================
from datetime import datetime
from typing import Dict, Any
import logging
from ..models.data_classes import RedditComment
from ..utils.text_processor import TextProcessor
from ..nlp.analyzer import NLPAnalyzer

logger = logging.getLogger(__name__)

class CommentProcessor:
    def __init__(self, nlp_analyzer: NLPAnalyzer):
        self.nlp_analyzer = nlp_analyzer

    def process_comment(self, comment_data: Dict[str, Any]) -> RedditComment:
        """Process a single Reddit comment"""
        try:
            comment_id = TextProcessor.parse_reddit_id(comment_data.get('name', ''))
            post_id = TextProcessor.parse_reddit_id(comment_data.get('link_id', ''))
            parent_id = TextProcessor.parse_reddit_id(comment_data.get('parent_id', ''))
            
            # Clean the content
            cleaned_content = TextProcessor.clean_text(comment_data.get('body', ''))
            
            comment = RedditComment(
                comment_id=comment_id,
                post_id=post_id,
                parent_id=parent_id,
                content=cleaned_content,
                author=comment_data.get('author', '[deleted]'),
                timestamp=datetime.fromtimestamp(comment_data.get('created_utc', 0)),
                score=comment_data.get('score', 0),
                edited=bool(comment_data.get('edited', False))
            )
            
            # Add intent and sentiment analysis
            if cleaned_content:
                comment.intent = self.nlp_analyzer.detect_intent(cleaned_content)
                comment.sentiment = self.nlp_analyzer.analyze_sentiment(cleaned_content)
                
            return comment
        except Exception as e:
            logger.error(f"Error processing comment {comment_data.get('name', 'unknown')}: {e}")
            raise

================
File: src/processors/conversation_processor.py
================
from typing import Dict, List, Any
from ..models.data_classes import RedditPost, RedditComment

class ConversationProcessor:
    @staticmethod
    def build_conversation_tree(posts: List[RedditPost], comments: List[RedditComment]) -> Dict[str, RedditPost]:
        """Build conversation trees by linking comments to their posts and parent comments"""
        # Create a dictionary of posts for easy lookup
        posts_dict = {post.post_id: post for post in posts}
        
        # Create a dictionary of comments for easy lookup
        comments_dict = {comment.comment_id: comment for comment in comments}
        
        # Link comments to their parents
        for comment in comments:
            if comment.post_id in posts_dict:
                # If parent is a post, add to post's comments
                if comment.parent_id == comment.post_id:
                    posts_dict[comment.post_id].comments.append(comment)
                # If parent is another comment, it's a reply
                elif comment.parent_id in comments_dict:
                    parent_comment = comments_dict[comment.parent_id]
                    if not hasattr(parent_comment, 'replies'):
                        parent_comment.replies = []
                    parent_comment.replies.append(comment)
        
        return posts_dict

    @staticmethod
    def create_conversation_pairs(posts_dict: Dict[str, RedditPost]) -> List[Dict[str, Any]]:
        """Create conversation pairs from posts and comments"""
        conversation_pairs = []
        
        for post in posts_dict.values():
            # Create pairs between post and direct comments
            for comment in post.comments:
                pair = {
                    'post_id': post.post_id,
                    'context': post.content,
                    'response': comment.content,
                    'context_intent': post.intent,
                    'response_intent': comment.intent,
                    'context_sentiment': post.sentiment,
                    'response_sentiment': comment.sentiment,
                    'context_author': post.author,
                    'response_author': comment.author,
                    'score': comment.score,
                    'timestamp': comment.timestamp.isoformat()
                }
                conversation_pairs.append(pair)
                
                # If comment has replies, create pairs between comment and replies
                if hasattr(comment, 'replies'):
                    for reply in comment.replies:
                        pair = {
                            'post_id': post.post_id,
                            'context': comment.content,
                            'response': reply.content,
                            'context_intent': comment.intent,
                            'response_intent': reply.intent,
                            'context_sentiment': comment.sentiment,
                            'response_sentiment': reply.sentiment,
                            'context_author': comment.author,
                            'response_author': reply.author,
                            'score': reply.score,
                            'timestamp': reply.timestamp.isoformat()
                        }
                        conversation_pairs.append(pair)
        
        return conversation_pairs

================
File: src/processors/post_processor.py
================
from datetime import datetime
from typing import Dict, Any
import logging
from ..models.data_classes import RedditPost
from ..utils.text_processor import TextProcessor
from ..nlp.analyzer import NLPAnalyzer

logger = logging.getLogger(__name__)

class PostProcessor:
    def __init__(self, nlp_analyzer: NLPAnalyzer):
        self.nlp_analyzer = nlp_analyzer

    def process_post(self, post_data: Dict[str, Any]) -> RedditPost:
        """Process a single Reddit post"""
        try:
            post_id = TextProcessor.parse_reddit_id(post_data.get('name', ''))
            
            # Clean the content
            cleaned_content = TextProcessor.clean_text(post_data.get('selftext', ''))
            cleaned_title = TextProcessor.clean_text(post_data.get('title', ''))
            
            post = RedditPost(
                post_id=post_id,
                title=cleaned_title,
                content=cleaned_content,
                author=post_data.get('author', '[deleted]'),
                timestamp=datetime.fromtimestamp(post_data.get('created_utc', 0)),
                score=post_data.get('score', 0),
                num_comments=post_data.get('num_comments', 0),
                upvote_ratio=post_data.get('upvote_ratio', 0.0),
                over_18=post_data.get('over_18', False),
                edited=bool(post_data.get('edited', False)),
                comments=[]
            )
            
            # Add intent and sentiment analysis
            if cleaned_content:
                post.intent = self.nlp_analyzer.detect_intent(cleaned_content)
                post.sentiment = self.nlp_analyzer.analyze_sentiment(cleaned_content)
                
            return post
        except Exception as e:
            logger.error(f"Error processing post {post_data.get('name', 'unknown')}: {e}")
            raise

================
File: src/utils/text_processor.py
================
import re
import emoji
from typing import List, Any
import logging

logger = logging.getLogger(__name__)

class TextProcessor:
    @staticmethod
    def clean_text(text: str) -> str:
        """Clean and normalize text content"""
        if not isinstance(text, str):
            return ""
            
        # Remove URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        
        # Remove emoji but keep the text representation
        text = emoji.demojize(text)
        
        # Remove special characters but keep basic punctuation
        text = re.sub(r'[^\w\s.,!?]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()

    @staticmethod
    def parse_reddit_id(full_id: str) -> str:
        """Extract the base ID from Reddit's fullname format"""
        if full_id and '_' in full_id:
            return full_id.split('_')[1]
        return full_id

================
File: src/data_processor.py
================
# src/data_processor.py

import logging
import json
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any, Generator, Iterator, Optional
from sklearn.model_selection import train_test_split
from dataclasses import asdict, dataclass, field
import nltk


import torch
from tqdm import tqdm
import gc
from .nlp.topic_modeling import TopicModeler
import psutil



from .data.data_loader import DataLoader
from .nlp.analyzer import NLPAnalyzer
from .processors.post_processor import PostProcessor
from .processors.comment_processor import CommentProcessor
from .processors.conversation_processor import ConversationProcessor
from .models.data_classes import RedditPost, RedditComment
from .nlp.language_analyzer import LanguageStyleAnalyzer


import logging
import json
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any, Generator, Iterator, Optional
from sklearn.model_selection import train_test_split
from dataclasses import asdict
import torch
from tqdm import tqdm
import gc
from datetime import datetime
import time
from .nlp.topic_modeling import TopicModeler
from .nlp.analyzer import NLPAnalyzer
from .data.data_loader import DataLoader
from .processors.post_processor import PostProcessor
from .processors.comment_processor import CommentProcessor
from .processors.conversation_processor import ConversationProcessor
from .models.data_classes import RedditPost, RedditComment
import sys
from rich.progress import (
    Progress,
    TimeElapsedColumn,
    TimeRemainingColumn,
    SpinnerColumn,
    BarColumn,
    TextColumn
)
from rich.console import Console
from rich.logging import RichHandler
from rich.table import Table

logger = logging.getLogger(__name__)



def setup_logging(output_dir: Path) -> logging.Logger:
    """Configure enhanced logging with rich handler and file output"""
    log_file = output_dir / f"processing_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s | %(levelname)s | %(message)s",
        handlers=[
            RichHandler(rich_tracebacks=True, console=console),
            logging.FileHandler(log_file)
        ]
    )
    
    return logging.getLogger(__name__)

class PerformanceMetrics:
    cpu_percent: float = 0.0
    memory_percent: float = 0.0
    gpu_utilization: Optional[float] = None
    gpu_memory_used: Optional[float] = None
    timestamp: datetime = field(default_factory=datetime.now)

class ProcessingStats:
    """Track processing statistics, timing, and system performance"""
    def __init__(self):
        self.start_time = time.time()
        self.operation_times = {}
        self.operation_counts = {}
        self.errors = []
        self.warnings = []
        self.performance_history: List[PerformanceMetrics] = []
        self.monitoring_interval = 1.0  # seconds
        self.last_monitored = time.time()
        
    def _get_gpu_metrics(self) -> tuple[Optional[float], Optional[float]]:
        """Get GPU utilization and memory usage if available"""
        if torch.cuda.is_available():
            try:
                gpu_util = torch.cuda.utilization()
                gpu_mem = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() * 100
                return gpu_util, gpu_mem
            except:
                return None, None
        return None, None

    #def update_performance_metrics(self):
        """Update performance metrics if monitoring interval has elapsed."""
        current_time = time.time()
        if current_time - self.last_monitored >= self.monitoring_interval:
            gpu_util, gpu_mem = self._get_gpu_metrics()
            metrics = PerformanceMetrics(
                cpu_percent=psutil.cpu_percent(),
                memory_percent=psutil.virtual_memory().percent,
                gpu_utilization=gpu_util,
                gpu_memory_used=gpu_mem
            )
            self.performance_history.append(metrics)
            self.last_monitored = current_time
        
    def start_operation(self, operation_name: str):
        """Start timing an operation and record initial performance metrics"""
        self.operation_times[operation_name] = {
            'start': time.time(),
            'start_metrics': PerformanceMetrics(
                cpu_percent=psutil.cpu_percent(),
                memory_percent=psutil.virtual_memory().percent,
                *self._get_gpu_metrics()
            )
        }
        
    def end_operation(self, operation_name: str, success: bool = True):
        """End timing an operation and record final performance metrics"""
        if operation_name in self.operation_times:
            end_time = time.time()
            duration = end_time - self.operation_times[operation_name]['start']
            self.operation_times[operation_name].update({
                'duration': duration,
                'success': success,
                'end_metrics': PerformanceMetrics(
                    cpu_percent=psutil.cpu_percent(),
                    memory_percent=psutil.virtual_memory().percent,
                    *self._get_gpu_metrics()
                )
            })
            
    def get_performance_summary(self) -> Dict:
        """Generate summary of performance metrics"""
        if not self.performance_history:
            return {}
            
        cpu_percentages = [m.cpu_percent for m in self.performance_history]
        memory_percentages = [m.memory_percent for m in self.performance_history]
        gpu_utils = [m.gpu_utilization for m in self.performance_history if m.gpu_utilization is not None]
        gpu_mems = [m.gpu_memory_used for m in self.performance_history if m.gpu_memory_used is not None]
        
        return {
            'cpu': {
                'avg': sum(cpu_percentages) / len(cpu_percentages),
                'max': max(cpu_percentages),
                'min': min(cpu_percentages)
            },
            'memory': {
                'avg': sum(memory_percentages) / len(memory_percentages),
                'max': max(memory_percentages),
                'min': min(memory_percentages)
            },
            'gpu': {
                'avg_utilization': sum(gpu_utils) / len(gpu_utils) if gpu_utils else None,
                'max_utilization': max(gpu_utils) if gpu_utils else None,
                'avg_memory': sum(gpu_mems) / len(gpu_mems) if gpu_mems else None,
                'max_memory': max(gpu_mems) if gpu_mems else None
            }
        }

    def generate_report(self) -> Dict:
        """Generate comprehensive processing report including performance metrics"""
        total_duration = time.time() - self.start_time
        
        return {
            'total_duration': total_duration,
            'operations': self.operation_times,
            'error_count': len(self.errors),
            'warning_count': len(self.warnings),
            'errors': self.errors,
            'warnings': self.warnings,
            'performance': self.get_performance_summary()
        }

class ProcessingStats:
    """Track processing statistics and timing"""
    def __init__(self):
        self.start_time = time.time()
        self.operation_times = {}
        self.operation_counts = {}
        self.errors = []
        self.warnings = []
        
    def start_operation(self, operation_name: str):
        """Start timing an operation"""
        self.operation_times[operation_name] = {'start': time.time()}
        
    def end_operation(self, operation_name: str, success: bool = True):
        """End timing an operation and record statistics"""
        if operation_name in self.operation_times:
            end_time = time.time()
            duration = end_time - self.operation_times[operation_name]['start']
            self.operation_times[operation_name]['duration'] = duration
            self.operation_times[operation_name]['success'] = success
            
    def add_error(self, operation: str, error: str):
        """Record an error"""
        self.errors.append({
            'timestamp': datetime.now(),
            'operation': operation,
            'error': error
        })
        
    def add_warning(self, operation: str, warning: str):
        """Record a warning"""
        self.warnings.append({
            'timestamp': datetime.now(),
            'operation': operation,
            'warning': warning
        })
        
    def generate_report(self) -> Dict:
        """Generate comprehensive processing report"""
        total_duration = time.time() - self.start_time
        
        return {
            'total_duration': total_duration,
            'operations': self.operation_times,
            'error_count': len(self.errors),
            'warning_count': len(self.warnings),
            'errors': self.errors,
            'warnings': self.warnings
        }


class GPUOptimizedProcessor:
    def __init__(self, 
                 posts_file: str, 
                 comments_file: str, 
                 output_dir: str, 
                 batch_size: int = 32,
                 chunk_size: int = 1000):
        """
        Initialize the Reddit data processor optimized for GPU processing.
        
        Args:
            posts_file (str): Path to the posts JSON file
            comments_file (str): Path to the comments JSON file
            output_dir (str): Directory for output files
            batch_size (int): Size of batches for NLP processing
            chunk_size (int): Size of chunks for data processing
        """
        self.data_loader = DataLoader(posts_file, comments_file)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # GPU optimized settings
        self.CHUNK_SIZE = chunk_size
        self.BATCH_SIZE = batch_size
        
        # Initialize components
        self.nlp_analyzer = self._initialize_nlp()
        self.post_processor = PostProcessor(self.nlp_analyzer)
        self.comment_processor = CommentProcessor(self.nlp_analyzer)
        self.conversation_processor = ConversationProcessor()
        
        # Initialize state tracking
        self.processed_posts: Optional[List[RedditPost]] = None
        self.processed_comments: Optional[List[RedditComment]] = None
        self.posts_dict: Optional[Dict[str, RedditPost]] = None

    def _initialize_nlp(self) -> NLPAnalyzer:
        """Initialize NLP analyzer with GPU support"""
        if torch.cuda.is_available():
            logger.info(f"Initializing NLP analyzer with GPU support: {torch.cuda.get_device_name(0)}")
        else:
            logger.warning("GPU not available, falling back to CPU")
        
        return NLPAnalyzer(batch_size=self.BATCH_SIZE)

    def process_in_chunks(self, 
                         items: Iterator, 
                         processor_func, 
                         desc: str,
                         chunk_size: Optional[int] = None) -> Generator:
        """
        Process items in chunks optimized for GPU.
        
        Args:
            items: Iterator of items to process
            processor_func: Function to process each item
            desc: Description for progress bar
            chunk_size: Optional override for chunk size
            
        Yields:
            Processed items one at a time
        """
        chunk_size = chunk_size or self.CHUNK_SIZE
        chunk = []
        total_processed = 0
        
        for item in tqdm(items, desc=desc):
            chunk.append(item)
            if len(chunk) >= chunk_size:
                for processed_item in self._process_chunk(chunk, processor_func):
                    yield processed_item
                    total_processed += 1
                    
                    if total_processed % (chunk_size * 5) == 0:
                        logger.info(f"Processed {total_processed} items")
                        self._cleanup_gpu_memory()
                
                chunk = []
        
        # Process remaining items
        if chunk:
            yield from self._process_chunk(chunk, processor_func)

    def _process_chunk(self, chunk: List, processor_func) -> List:
        """
        Process a single chunk of data with error handling.
        
        Args:
            chunk: List of items to process
            processor_func: Function to process each item
            
        Returns:
            List of processed items
        """
        try:
            processed_items = []
            for item in chunk:
                try:
                    processed = processor_func(item)
                    if processed is not None:
                        processed_items.append(processed)
                except Exception as e:
                    logger.error(f"Error processing individual item: {e}")
            return processed_items
        except Exception as e:
            logger.error(f"Error processing chunk: {e}")
            return []

    def _cleanup_gpu_memory(self):
        """Clean up GPU memory"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

    def save_to_file(self, 
                     data: List[Dict], 
                     filename: str, 
                     mode: str = 'w',
                     chunk_size: int = 1000):
        """
        Save processed data to file with chunking for large datasets.
        
        Args:
            data: List of data to save
            filename: Output filename
            mode: Write mode ('w' for write, 'a' for append)
            chunk_size: Size of chunks for writing
        """
        file_path = self.output_dir / filename
        
        if mode == 'w' and file_path.exists():
            file_path.unlink()
        
        try:
            if filename.endswith('.csv'):
                self._save_csv(data, file_path, mode, chunk_size)
            elif filename.endswith('.json'):
                self._save_json(data, file_path, mode, chunk_size)
            else:
                raise ValueError(f"Unsupported file format: {filename}")
                
        except Exception as e:
            logger.error(f"Error saving to {filename}: {e}")
            raise

    def _save_csv(self, 
                  data: List[Dict], 
                  file_path: Path, 
                  mode: str, 
                  chunk_size: int):
        """Save data to CSV file"""
        for i in range(0, len(data), chunk_size):
            chunk = data[i:i + chunk_size]
            df = pd.DataFrame(chunk)
            df.to_csv(
                file_path, 
                mode=mode, 
                header=(i == 0 or mode == 'w'), 
                index=False
            )

    def _save_json(self, 
                   data: List[Dict], 
                   file_path: Path, 
                   mode: str, 
                   chunk_size: int):
        """Save data to JSON file"""
        if mode == 'w':
            with open(file_path, 'w') as f:
                json.dump(data, f, indent=2)
        else:
            for i in range(0, len(data), chunk_size):
                chunk = data[i:i + chunk_size]
                with open(file_path, 'a') as f:
                    if i == 0:
                        f.write('[\n')
                    json.dump(chunk, f, indent=2)
                    if i + chunk_size < len(data):
                        f.write(',\n')
                    else:
                        f.write('\n]')

    def _process_posts(self, raw_posts: List[Dict]) -> List[RedditPost]:
        """Process all posts"""
        logger.info("Processing posts...")
        return list(self.process_in_chunks(
            raw_posts,
            self.post_processor.process_post,
            "Processing posts"
        ))

    def _process_comments(self, raw_comments: List[Dict]) -> List[RedditComment]:
        """Process all comments"""
        logger.info("Processing comments...")
        return list(self.process_in_chunks(
            raw_comments,
            self.comment_processor.process_comment,
            "Processing comments"
        ))

    def _create_conversation_pairs(self) -> List[Dict]:
        """Create conversation pairs from processed posts and comments"""
        logger.info("Creating conversation pairs...")
        self.posts_dict = self.conversation_processor.build_conversation_tree(
            self.processed_posts,
            self.processed_comments
        )
        return self.conversation_processor.create_conversation_pairs(self.posts_dict)
    

    def _split_and_save_data(self, conversation_pairs):
        """Split data into train/test sets and save if non-empty."""
        if not conversation_pairs:
            logger.warning("No conversation pairs available. Skipping train/test split and save.")
            return

        try:
            # Perform train-test split
            train_pairs, test_pairs = train_test_split(
                conversation_pairs,
                test_size=0.2,
                random_state=42
            )

            # Save training data
            train_path = self.output_dir / 'train_conversations.csv'
            pd.DataFrame(train_pairs).to_csv(train_path, index=False)
            logger.info(f"Training data saved to {train_path}")

            # Save test data
            test_path = self.output_dir / 'test_conversations.csv'
            pd.DataFrame(test_pairs).to_csv(test_path, index=False)
            logger.info(f"Test data saved to {test_path}")

        except ValueError as e:
            logger.error(f"Error splitting conversation pairs: {e}")

    def _analyze_language_style(self):
        """Analyze language style and common phrases"""
        logger.info("Analyzing language style and common phrases...")
        
        # Initialize language analyzer
        language_analyzer = LanguageStyleAnalyzer(
            min_phrase_freq=3,
            max_ngram_size=3
        )
        
        # Combine post and comment texts
        texts = []
        
        # Add post texts
        for post in self.processed_posts:
            if post.title:
                texts.append(post.title)
            if post.content:
                texts.append(post.content)
                
        # Add comment texts
        for comment in self.processed_comments:
            if comment.content:
                texts.append(comment.content)
        
        # Analyze content
        language_analyzer.analyze_content(texts)
        
        # Save results
        language_analyzer.save_results(
            self.output_dir / 'language_analysis.json'
        )
        
        # Log summary statistics
        stats = language_analyzer.get_summary_statistics()
        logger.info("Language analysis summary:")
        logger.info(f"Total unique phrases: {stats['total_unique_phrases']}")
        logger.info(f"Total slang terms: {stats['total_slang_terms']}")
        logger.info(f"Total sentence patterns: {stats['total_sentence_patterns']}")


    def process_data(self):
        try:
            stats = ProcessingStats()
            logger.info("Starting data processing with GPU optimization...")
            
            # Load raw data
            raw_posts, raw_comments = self.data_loader.load_data()
            
            # Process posts
            self.processed_posts = self._process_posts(raw_posts)
            
            # Process comments
            self.processed_comments = self._process_comments(raw_comments)
            
            # Analyze language style
            self._analyze_language_style()
            
            # Extract topics
            self._extract_topics()
            
            # Create conversation pairs
            conversation_pairs = self._create_conversation_pairs()
            
            # Split and save data
            self._split_and_save_data(conversation_pairs)
            
            # Update performance metrics
            #stats.update_performance_metrics()
            
            logger.info("Processing completed successfully!")
            logger.info(f"Number of conversation pairs: {len(conversation_pairs)}")
        
        except Exception as e:
            logger.error(f"Error processing data: {e}")
            raise
        
        finally:
            self._cleanup_gpu_memory()
            
    def _extract_topics(self):
        """Extracts topics from the processed posts and comments."""
        logger.info("Starting topic extraction...")
        
        # Instantiate the topic modeler with LDA (or NMF) parameters
        topic_modeler = TopicModeler(
            method='lda',         # You can switch to 'nmf' if desired
            n_topics=10,          # Set the number of topics you want
            max_features=10000    # Set max features based on your text data
        )
        
        # Collect text data for topic extraction (posts + comments)
        texts = []
        for post in self.processed_posts:
            if post.content:
                texts.append(post.content)
            if post.title:
                texts.append(post.title)
        for comment in self.processed_comments:
            if comment.content:
                texts.append(comment.content)

        # Step 1: Preprocess and transform texts for topic modeling
        try:
            document_topic_matrix = topic_modeler.fit_transform(texts)
        except Exception as e:
            logger.error(f"Error during topic modeling: {e}")
            return
        
        # Step 2: Retrieve and structure the topics with top terms
        try:
            topic_terms = topic_modeler.get_topic_terms(n_terms=10)  # Get top terms for each topic
            topic_summary = topic_modeler.get_topic_summary(n_terms=10)
            
            # Log summary for review
            for idx, topic in enumerate(topic_summary):
                logger.info(f"Topic {idx + 1}: {', '.join([term['term'] for term in topic['terms']])}")

            # Save topic summaries to a JSON file
            summary_path = self.output_dir / "topic_summary.json"
            with open(summary_path, "w") as f:
                json.dump(topic_summary, f, indent=2)
            logger.info(f"Topic summaries saved to {summary_path}")

            # Save document-topic matrix to CSV
            dtm_path = self.output_dir / "document_topic_matrix.csv"
            pd.DataFrame(document_topic_matrix).to_csv(dtm_path, index=False)
            logger.info(f"Document-topic matrix saved to {dtm_path}")

        except Exception as e:
            logger.error(f"Error saving topic results: {e}")

        logger.info("Topic extraction completed successfully.")



    def get_processing_stats(self) -> Dict[str, Any]:
        """Get statistics about the processed data"""
        stats = {
            "total_posts": len(self.processed_posts) if self.processed_posts else 0,
            "total_comments": len(self.processed_comments) if self.processed_comments else 0,
            "gpu_available": torch.cuda.is_available(),
            "gpu_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
            "batch_size": self.BATCH_SIZE,
            "chunk_size": self.CHUNK_SIZE
        }
        
        if self.posts_dict:
            stats["posts_with_comments"] = sum(
                1 for post in self.posts_dict.values() if post.comments
            )
            
        return stats

    def __enter__(self):
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        self._cleanup_gpu_memory()

================
File: src/requirements.txt
================
# Core data processing
pandas>=1.5.0
numpy>=1.21.0

# Natural Language Processing
transformers>=4.30.0
torch>=2.0.0  # Required for transformers
scikit-learn>=1.0.0  # For train_test_split

# Text processing
emoji>=2.2.0
regex>=2023.5.5  # For advanced text processing

# Progress tracking and formatting
tqdm>=4.65.0
rich>=13.0.0  # For better console output

# Type checking
typing-extensions>=4.5.0  # For advanced type hints

# Date handli
python-dateutil>=2.8.2

ujson>=5.7.0  # Faster JSON processing
numba>=0.57.0  # For numerical computations

scikit-learn>=1.0.0
scipy>=1.7.0
nltk 
spacy

================
File: executive_summary.py
================
import json
import pandas as pd
import logging
from pathlib import Path
from rich.console import Console
from rich.table import Table
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize logging and console output
logger = logging.getLogger(__name__)
console = Console()

class ExecutiveSummaryGenerator:
    def __init__(self, output_dir: str):
        self.output_dir = Path(output_dir)
        self.topic_summary_file = self.output_dir / "topic_summary.json"
        self.language_analysis_file = self.output_dir / "language_analysis.json"
        self.document_topic_matrix_file = self.output_dir / "document_topic_matrix.csv"

        # Load the data
        self.topic_summary = self._load_json(self.topic_summary_file)
        self.language_analysis = self._load_json(self.language_analysis_file)
        self.document_topic_matrix = pd.read_csv(self.document_topic_matrix_file)

    def _load_json(self, file_path: Path):
        try:
            with open(file_path, 'r') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error loading JSON from {file_path}: {e}")
            return {}

    def generate_summary(self):
        """Generate and save an executive summary report."""
        console.print("[bold cyan]Executive Summary Report[/bold cyan]")
        
        # 1. Top 5 Topics with Key Terms
        self._generate_top_topics_summary()

        # 2. Sentiment Overview
        self._generate_sentiment_overview()

        # 3. Language Style Insights
        self._generate_language_style_insights()

        # 4. Save summary to text file
        self._save_summary_to_file()

    def _generate_top_topics_summary(self):
        """Generate a summary of the top 5 topics and display key terms."""
        console.print("\n[bold green]Top 5 Topics[/bold green]")
        table = Table(show_header=True, header_style="bold magenta")
        table.add_column("Topic ID", style="dim", width=12)
        table.add_column("Key Terms", width=40)
        
        # Get top 5 topics based on weight
        for topic in self.topic_summary[:5]:
            topic_id = str(topic['topic_id'])
            terms = ', '.join([term['term'] for term in topic['terms']])
            table.add_row(topic_id, terms)
        
        console.print(table)

    def _generate_sentiment_overview(self):
        """Generate a sentiment overview from the document-topic matrix."""
        console.print("\n[bold green]Sentiment Overview by Topic[/bold green]")

        # Example sentiment extraction (you'll need to calculate sentiment per topic based on document scores)
        # Assuming `self.document_topic_matrix` contains a column "sentiment" with positive/negative/neutral scores
        sentiments = self.document_topic_matrix.groupby('topic_id')['sentiment'].mean().reset_index()
        sentiments.columns = ['Topic ID', 'Average Sentiment']

        # Plotting sentiment overview
        plt.figure(figsize=(10, 6))
        sns.barplot(x='Topic ID', y='Average Sentiment', data=sentiments)
        plt.title('Average Sentiment by Topic')
        plt.ylabel('Average Sentiment')
        plt.xlabel('Topic ID')

        # Save and show plot
        sentiment_plot_path = self.output_dir / "sentiment_overview.png"
        plt.savefig(sentiment_plot_path)
        console.print(f"\nSentiment overview saved to {sentiment_plot_path}")
        plt.show()

    def _generate_language_style_insights(self):
        """Generate insights from the language analysis."""
        console.print("\n[bold green]Language Style Insights[/bold green]")
        slang_terms = self.language_analysis.get('slang_terms', {})
        common_phrases = self.language_analysis.get('common_phrases', {}).get('2_grams', {})

        # Display common phrases and slang terms
        console.print(f"[bold]Top 5 Common 2-Gram Phrases[/bold]:")
        for phrase, freq in list(common_phrases.items())[:5]:
            console.print(f"- {phrase}: {freq} times")

        console.print(f"\n[bold]Top 5 Slang Terms[/bold]:")
        for term, freq in list(slang_terms.items())[:5]:
            console.print(f"- {term}: {freq} times")

    def _save_summary_to_file(self):
        """Save the executive summary to a text file."""
        summary_file = self.output_dir / "executive_summary.txt"
        with open(summary_file, 'w') as f:
            f.write("Executive Summary Report\n")
            f.write("=======================\n\n")
            f.write("1. Top 5 Topics\n")
            for topic in self.topic_summary[:5]:
                terms = ', '.join([term['term'] for term in topic['terms']])
                f.write(f"Topic {topic['topic_id']}: {terms}\n")
            
            f.write("\n2. Sentiment Overview\n")
            f.write("See the sentiment_overview.png plot for the average sentiment per topic.\n")

            f.write("\n3. Language Style Insights\n")
            slang_terms = self.language_analysis.get('slang_terms', {})
            common_phrases = self.language_analysis.get('common_phrases', {}).get('2_grams', {})
            f.write("Top 5 Common 2-Gram Phrases:\n")
            for phrase, freq in list(common_phrases.items())[:5]:
                f.write(f"- {phrase}: {freq} times\n")
            f.write("\nTop 5 Slang Terms:\n")
            for term, freq in list(slang_terms.items())[:5]:
                f.write(f"- {term}: {freq} times\n")

        console.print(f"\n[bold green]Executive summary saved to {summary_file}[/bold green]")

if __name__ == "__main__":
    output_dir = "processed_data"
    summary_generator = ExecutiveSummaryGenerator(output_dir)
    summary_generator.generate_summary()

================
File: main.py
================
import logging
from src.data_processor import GPUOptimizedProcessor
import torch
import nltk
nltk.download('stopwords')
nltk.download('words')
nltk.download('punkt_tab')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def main():
    """Main function with GPU optimization"""
    try:
        # Configure paths
        posts_file = "data/gettingbigger_submissions.json"
        comments_file = "data/gettingbigger_comments.json"
        output_dir = "processed_data"
        
        # Log GPU availability
        logger.info(f"CUDA available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            logger.info(f"GPU device: {torch.cuda.get_device_name(0)}")
            logger.info(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        
        # Initialize and run the GPU-optimized processor
        processor = GPUOptimizedProcessor(posts_file, comments_file, output_dir)
        processor.process_data()
            
    except Exception as e:
        logger.error(f"Error in main: {e}")
        raise

if __name__ == "__main__":
    main()

================
File: package.json
================
{
  "dependencies": {
    "malloc": "^1.1.0"
  }
}

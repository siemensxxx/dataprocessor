This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-11-13T08:07:01.669Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
data/
  gettingbigger_comments.json
  gettingbigger_submissions.json
src/
  data/
    data_loader.py
  models/
    data_classes.py
  nlp/
    analyzer.py
  processors/
    comment_processor.py
    conversation_processor.py
    post_processor.py
  utils/
    text_processor.py
  data_processor.py
  requirements.txt
.repomixignore
main.py
package.json
repomix.config.json

================================================================
Repository Files
================================================================

================
File: data/gettingbigger_comments.json
================
{"all_awardings": [], "archived": false, "associated_award": null, "author": "iamzangrief", "author_created_utc": 1557355731, "author_flair_background_color": null, "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": null, "author_flair_text": null, "author_flair_text_color": null, "author_flair_type": "text", "author_fullname": "t2_3hhsow96", "author_patreon_flair": false, "author_premium": false, "body": "For real, I don't really reply or post anymore but I still see these posts crop up or depending on the day flood my homepage thanks to them cross posting to AJFY, it looks kinda pathetic imo, but hey you-do-you. I at least hope they're pushing just as hard on other social media sites or their own website. By the way if you don't have a website, consider clicking my link to squarespace - jk, I don't have one.", "can_gild": true, "collapsed": false, "collapsed_because_crowd_control": null, "collapsed_reason": null, "collapsed_reason_code": null, "comment_type": null, "controversiality": 0, "created_utc": 1672602995, "distinguished": null, "edited": false, "gilded": 0, "gildings": {}, "id": "j2jeca9", "is_submitter": false, "link_id": "t3_100q9kw", "locked": false, "name": "t1_j2jeca9", "no_follow": true, "parent_id": "t3_100q9kw", "permalink": "/r/gettingbigger/comments/100q9kw/be_wary_of_affiliates/j2jeca9/", "retrieved_on": 1676213564, "score": 2, "score_hidden": false, "send_replies": true, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_type": "public", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "unrepliable_reason": null, "ups": 2}
{"all_awardings": [], "archived": false, "associated_award": null, "author": "gbuser2022", "author_created_utc": 1646940434, "author_flair_background_color": "#d3d6da", "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": "a8a3dff8-c1e9-11ec-a01e-f27f07d63108", "author_flair_text": "[Started 12/15/2021] [B: 5.5/5] [C: 6.1/5.5] [G: 8/6]", "author_flair_text_color": "dark", "author_flair_type": "text", "author_fullname": "t2_kk2lcqfa", "author_patreon_flair": false, "author_premium": false, "body": "There\u2019s this prick who keeps posting his routine (which is convoluted and takes like 4 hours) as a comment under any post which could in some way be seen as related to PE. Of course, the routine is full of self-promotion and affiliate links. His name is [Something]_Hangz", "can_gild": true, "collapsed": false, "collapsed_because_crowd_control": null, "collapsed_reason": null, "collapsed_reason_code": null, "comment_type": null, "controversiality": 0, "created_utc": 1672603858, "distinguished": null, "edited": false, "gilded": 0, "gildings": {}, "id": "j2jgkwn", "is_submitter": false, "link_id": "t3_100q9kw", "locked": false, "name": "t1_j2jgkwn", "no_follow": false, "parent_id": "t3_100q9kw", "permalink": "/r/gettingbigger/comments/100q9kw/be_wary_of_affiliates/j2jgkwn/", "retrieved_on": 1676213506, "score": 11, "score_hidden": false, "send_replies": true, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_type": "public", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "unrepliable_reason": null, "ups": 11}
{"all_awardings": [], "archived": false, "associated_award": null, "author": "EmceeSpike", "author_created_utc": 1400646146, "author_flair_background_color": "#d3d6da", "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": "a8a3dff8-c1e9-11ec-a01e-f27f07d63108", "author_flair_text": "7 YEAR VET", "author_flair_text_color": "dark", "author_flair_type": "text", "author_fullname": "t2_gnj40", "author_patreon_flair": false, "author_premium": false, "body": "/u/Hunter_Hangz ?\n\nEdit: He blocked me lol", "can_gild": true, "collapsed": false, "collapsed_because_crowd_control": null, "collapsed_reason": null, "collapsed_reason_code": null, "comment_type": null, "controversiality": 0, "created_utc": 1672606352, "distinguished": null, "edited": 1672647257, "gilded": 0, "gildings": {}, "id": "j2jn4hp", "is_submitter": false, "link_id": "t3_100q9kw", "locked": false, "name": "t1_j2jn4hp", "no_follow": false, "parent_id": "t1_j2jgkwn", "permalink": "/r/gettingbigger/comments/100q9kw/be_wary_of_affiliates/j2jn4hp/", "retrieved_on": 1676213336, "score": 11, "score_hidden": false, "send_replies": true, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_type": "public", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "unrepliable_reason": null, "ups": 11}
{"all_awardings": [], "archived": false, "associated_award": null, "author": "Stillwantmore2", "author_created_utc": 1534766422, "author_flair_background_color": "#005ba1", "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": "046e65ea-38f4-11eb-acc9-0eeefa290e51", "author_flair_text": "Owner Malehanger", "author_flair_text_color": "light", "author_flair_type": "text", "author_fullname": "t2_20ux66cd", "author_patreon_flair": false, "author_premium": false, "body": "Thanks BD. Totally agree.", "can_gild": true, "collapsed": false, "collapsed_because_crowd_control": null, "collapsed_reason": null, "collapsed_reason_code": null, "comment_type": null, "controversiality": 0, "created_utc": 1672607289, "distinguished": null, "edited": false, "gilded": 0, "gildings": {}, "id": "j2jpjij", "is_submitter": false, "link_id": "t3_100q9kw", "locked": false, "name": "t1_j2jpjij", "no_follow": false, "parent_id": "t3_100q9kw", "permalink": "/r/gettingbigger/comments/100q9kw/be_wary_of_affiliates/j2jpjij/", "retrieved_on": 1676213274, "score": 1, "score_hidden": false, "send_replies": true, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_type": "public", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "unrepliable_reason": null, "ups": 1}
{"all_awardings": [], "archived": false, "associated_award": null, "author": "Hunter_Hangz", "author_created_utc": 1672145226, "author_flair_background_color": "#005ba1", "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": "046e65ea-38f4-11eb-acc9-0eeefa290e51", "author_flair_text": " 8==D -> 8===D -> 8====D", "author_flair_text_color": "light", "author_flair_type": "text", "author_fullname": "t2_v8kw2mxj", "author_patreon_flair": false, "author_premium": false, "body": "[My Routine](https://www.reddit.com/user/Hunter_Hangz/comments/zwxdk1/the_hangz_method_for_pe/?utm_source=share&utm_medium=web2x&context=3) This is what everyone is fussing about.\n\nJust like my routine says. Any devices will work, those are just the ones I found success with. I use other products that the ones I am affiliated with, and I purchased them before I was ever affiliated with them with my own money. \n\nI want to apologize about spamming my routine last night. I was drunk and I thought I could solve everyone's problem. \n\nAgain, any devices will work! Aliexpress, Ebay whatever. These are just the products I found success with.", "can_gild": true, "collapsed": true, "collapsed_because_crowd_control": null, "collapsed_reason": "comment score below threshold", "collapsed_reason_code": "LOW_SCORE", "comment_type": null, "controversiality": 0, "created_utc": 1672607340, "distinguished": null, "edited": false, "gilded": 0, "gildings": {}, "id": "j2jpo8v", "is_submitter": false, "link_id": "t3_100q9kw", "locked": false, "name": "t1_j2jpo8v", "no_follow": false, "parent_id": "t3_100q9kw", "permalink": "/r/gettingbigger/comments/100q9kw/be_wary_of_affiliates/j2jpo8v/", "retrieved_on": 1676213270, "score": -16, "score_hidden": false, "send_replies": true, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_type": "public", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "unrepliable_reason": null, "ups": -16}
{"all_awardings": [], "archived": false, "associated_award": null, "author": "tobeginpe", "author_created_utc": 1634845749, "author_flair_background_color": null, "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": null, "author_flair_text": null, "author_flair_text_color": null, "author_flair_type": "text", "author_fullname": "t2_fqcq6l59", "author_patreon_flair": false, "author_premium": false, "body": "Formerly known as u/ChemSexCouple before he went apeshit and ruined his reputation.  I can see now he's attempting to be more mature but not for the right reasons.  Hes already taking the easy way out via deception by trying to change his name.\n\nOn top of that both of his accounts are brand new and per his username it appears chemically enhanced PE, which is now not disclosed in any comments I've seen since he \"rebranded\".\n\nChem PE is fine but don't act like your routine is the source of your gains.  Reminds you of someone? *cough Liver King cough cough*", "can_gild": true, "collapsed": false, "collapsed_because_crowd_control": null, "collapsed_reason": null, "collapsed_reason_code": null, "comment_type": null, "controversiality": 0, "created_utc": 1672609701, "distinguished": null, "edited": false, "gilded": 0, "gildings": {}, "id": "j2jvhow", "is_submitter": false, "link_id": "t3_100q9kw", "locked": false, "name": "t1_j2jvhow", "no_follow": false, "parent_id": "t1_j2jgkwn", "permalink": "/r/gettingbigger/comments/100q9kw/be_wary_of_affiliates/j2jvhow/", "retrieved_on": 1676213120, "score": 16, "score_hidden": false, "send_replies": true, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_type": "public", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "unrepliable_reason": null, "ups": 16}
{"all_awardings": [], "archived": false, "associated_award": null, "author": "[deleted]", "author_created_utc": null, "author_flair_background_color": "", "author_flair_css_class": null, "author_flair_template_id": null, "author_flair_text": null, "author_flair_text_color": "dark", "body": "[deleted]", "can_gild": true, "collapsed": false, "collapsed_because_crowd_control": null, "collapsed_reason": null, "collapsed_reason_code": null, "comment_type": null, "controversiality": 0, "created_utc": 1672610012, "distinguished": null, "edited": false, "gilded": 0, "gildings": {}, "id": "j2jw8so", "is_submitter": false, "link_id": "t3_100q9kw", "locked": false, "name": "t1_j2jw8so", "no_follow": true, "parent_id": "t1_j2jpo8v", "permalink": "/r/gettingbigger/comments/100q9kw/be_wary_of_affiliates/j2jw8so/", "retrieved_on": 1676213101, "score": 3, "score_hidden": false, "send_replies": true, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_type": "public", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "unrepliable_reason": null, "ups": 3}
{"all_awardings": [], "archived": false, "associated_award": null, "author": "fotw75", "author_created_utc": 1590521791, "author_flair_background_color": "#ffd635", "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": "304d65a8-38f4-11eb-a051-0ecb59bb3fe1", "author_flair_text": "(B: 5.75Lx4.25G) (C: 6.75Lx4.75G) (G: 7.5Lx5.25G)", "author_flair_text_color": "dark", "author_flair_type": "text", "author_fullname": "t2_5ip4qf2h", "author_patreon_flair": false, "author_premium": false, "body": "His shit is exactly what I come to this sub to avoid. It was worse as ChemSexCouple but still gets ridiculous under the new name. To me - it should have been \"ban-worthy\" weeks ago but I have to respect that it's being allowed for whatever reason.\n\nI seriously hope he learns to chill if he' stays because the constant affiliate shit is the opposite of what this place is supposed to be.", "can_gild": true, "collapsed": false, "collapsed_because_crowd_control": null, "collapsed_reason": null, "collapsed_reason_code": null, "comment_type": null, "controversiality": 0, "created_utc": 1672610109, "distinguished": null, "edited": false, "gilded": 0, "gildings": {}, "id": "j2jwhay", "is_submitter": false, "link_id": "t3_100q9kw", "locked": false, "name": "t1_j2jwhay", "no_follow": false, "parent_id": "t1_j2jvhow", "permalink": "/r/gettingbigger/comments/100q9kw/be_wary_of_affiliates/j2jwhay/", "retrieved_on": 1676213095, "score": 8, "score_hidden": false, "send_replies": true, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_type": "public", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "unrepliable_reason": null, "ups": 8}
{"all_awardings": [], "archived": false, "associated_award": null, "author": "tobeginpe", "author_created_utc": 1634845749, "author_flair_background_color": null, "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": null, "author_flair_text": null, "author_flair_text_color": null, "author_flair_type": "text", "author_fullname": "t2_fqcq6l59", "author_patreon_flair": false, "author_premium": false, "body": "Dude made a post saying he was going to change, then he changed his name and affiliate codes which identified him by that name. Lol.\n\nAt least he's not going around trolling people again...yet.", "can_gild": true, "collapsed": false, "collapsed_because_crowd_control": null, "collapsed_reason": null, "collapsed_reason_code": null, "comment_type": null, "controversiality": 0, "created_utc": 1672611563, "distinguished": null, "edited": false, "gilded": 0, "gildings": {}, "id": "j2k00s6", "is_submitter": false, "link_id": "t3_100q9kw", "locked": false, "name": "t1_j2k00s6", "no_follow": false, "parent_id": "t1_j2jwhay", "permalink": "/r/gettingbigger/comments/100q9kw/be_wary_of_affiliates/j2k00s6/", "retrieved_on": 1676213005, "score": 7, "score_hidden": false, "send_replies": true, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_type": "public", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "unrepliable_reason": null, "ups": 7}
{"all_awardings": [], "archived": false, "associated_award": null, "author": "bd19962015", "author_created_utc": 1546227149, "author_flair_background_color": "#373c3f", "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": "29fba9d6-d2a3-11ec-85ad-ae629c392782", "author_flair_text": "BD - idk somewhere between 4 and 12", "author_flair_text_color": "dark", "author_flair_type": "text", "author_fullname": "t2_2w5g9689", "author_patreon_flair": false, "author_premium": false, "body": "Don't tip him off lol", "can_gild": true, "collapsed": false, "collapsed_because_crowd_control": null, "collapsed_reason": null, "collapsed_reason_code": null, "comment_type": null, "controversiality": 0, "created_utc": 1672612135, "distinguished": null, "edited": false, "gilded": 0, "gildings": {}, "id": "j2k1ddg", "is_submitter": true, "link_id": "t3_100q9kw", "locked": false, "name": "t1_j2k1ddg", "no_follow": true, "parent_id": "t1_j2jw8so", "permalink": "/r/gettingbigger/comments/100q9kw/be_wary_of_affiliates/j2k1ddg/", "retrieved_on": 1676212969, "score": 1, "score_hidden": false, "send_replies": true, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_type": "public", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "unrepliable_reason": null, "ups": 1}
{"all_awardings": [], "archived": false, "associated_award": null, "author": "fotw75", "author_created_utc": 1590521791, "author_flair_background_color": "#ffd635", "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": "304d65a8-38f4-11eb-a051-0ecb59bb3fe1", "author_flair_text": "(B: 5.75Lx4.25G) (C: 6.75Lx4.75G) (G: 7.5Lx5.25G)", "author_flair_text_color": "dark", "author_flair_type": "text", "author_fullname": "t2_5ip4qf2h", "author_patreon_flair": false, "author_premium": false, "body": "I fully understand BD's point about trying to not be hypocritical about monetization of PE.\n\nBut back when he was CSC... he was trying to say it wasn't fair to target him by comparing himself to actual veterans who've put decades into what they do.\n\nThere is a MASSIVE DIFFERENCE between a guy cobbling together a \"PE Workout Plan\" based on bits and pieces of other people's established routines - and - actually BEING the guys who came up with those routines through hours of literal sweat, blood, and injury. CSC / HH's has not sat woodshedding for hours, days, weeks, and months, like guys like Ben with Malehanger, or M9, or the Total Man guy (regardless of current reviews).\n\nThose guys have dedicated years if not decades to making a safe path for all of us WHILE monetizing their hard work. BD is doing the same. *This* guy has tried to develop \"a character\".... 2 now actually, and tries to PM people a routine built on the backs of other people's hard work, then tacks on affiliate links based on products that he didn't create.\n\nHe's literally a drop shipper with a copy paste \"workout routine\".\n\nHunterHangz... if you're reading this - I'm not here to attack you, dude. I get it. I once drop shipped Doc Johnson Vibrating Pearl VIBRATORS back in the day and my tactics were the same. I'm peacefully trying to get you to understand what you are doing here... even the more \"chilled out version\" you're presenting under your new account - is still spam. \n\nPosting comments on this board and following them up with \"Hit me up!\", \"Message me bro\", and linking \"Hunterz Routine Here\" all over this sub... **whether it is allowed or not**\" is still spammy AF.\n\nYou can do what you did under your old username, which was call us \"haters\", tell us \"Peace Bro!\" all you want. But everyone recognizes what you're doing. You might part a few from their money but I've worked in internet sales since the internet was a thing dude. You're not going to get your return on time investment doing what you're doing. Affiliates only make the people above them richer.\n\nJust stop and contribute man. Please.", "can_gild": true, "collapsed": false, "collapsed_because_crowd_control": null, "collapsed_reason": null, "collapsed_reason_code": null, "comment_type": null, "controversiality": 0, "created_utc": 1672612492, "distinguished": null, "edited": false, "gilded": 0, "gildings": {}, "id": "j2k28fe", "is_submitter": false, "link_id": "t3_100q9kw", "locked": false, "name": "t1_j2k28fe", "no_follow": false, "parent_id": "t1_j2k00s6", "permalink": "/r/gettingbigger/comments/100q9kw/be_wary_of_affiliates/j2k28fe/", "retrieved_on": 1676212947, "score": 9, "score_hidden": false, "send_replies": true, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_type": "public", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "unrepliable_reason": null, "ups": 9}
{"all_awardings": [], "archived": false, "associated_award": null, "author": "Hunter_Hangz", "author_created_utc": 1672145226, "author_flair_background_color": "#005ba1", "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": "046e65ea-38f4-11eb-acc9-0eeefa290e51", "author_flair_text": " 8==D -> 8===D -> 8====D", "author_flair_text_color": "light", "author_flair_type": "text", "author_fullname": "t2_v8kw2mxj", "author_patreon_flair": false, "author_premium": false, "body": "Lol this is hilarious. I just want to sit with you guys at the cool table.", "can_gild": true, "collapsed": true, "collapsed_because_crowd_control": null, "collapsed_reason": "comment score below threshold", "collapsed_reason_code": "LOW_SCORE", "comment_type": null, "controversiality": 0, "created_utc": 1672613856, "distinguished": null, "edited": false, "gilded": 0, "gildings": {}, "id": "j2k5k7b", "is_submitter": false, "link_id": "t3_100q9kw", "locked": false, "name": "t1_j2k5k7b", "no_follow": true, "parent_id": "t3_100q9kw", "permalink": "/r/gettingbigger/comments/100q9kw/be_wary_of_affiliates/j2k5k7b/", "retrieved_on": 1676212860, "score": -13, "score_hidden": false, "send_replies": true, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_type": "public", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "unrepliable_reason": null, "ups": -13}
{"all_awardings": [], "archived": false, "associated_award": null, "author": "bd19962015", "author_created_utc": 1546227149, "author_flair_background_color": "#373c3f", "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": "29fba9d6-d2a3-11ec-85ad-ae629c392782", "author_flair_text": "BD - idk somewhere between 4 and 12", "author_flair_text_color": "dark", "author_flair_type": "text", "author_fullname": "t2_2w5g9689", "author_patreon_flair": false, "author_premium": false, "body": "But you are not cool.", "can_gild": true, "collapsed": false, "collapsed_because_crowd_control": null, "collapsed_reason": null, "collapsed_reason_code": null, "comment_type": null, "controversiality": 0, "created_utc": 1672613967, "distinguished": null, "edited": false, "gilded": 0, "gildings": {}, "id": "j2k5u0w", "is_submitter": true, "link_id": "t3_100q9kw", "locked": false, "name": "t1_j2k5u0w", "no_follow": false, "parent_id": "t1_j2k5k7b", "permalink": "/r/gettingbigger/comments/100q9kw/be_wary_of_affiliates/j2k5u0w/", "retrieved_on": 1676212854, "score": 9, "score_hidden": false, "send_replies": true, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_type": "public", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "unrepliable_reason": null, "ups": 9}
{"all_awardings": [], "archived": false, "associated_award": null, "author": "Hunter_Hangz", "author_created_utc": 1672145226, "author_flair_background_color": "#005ba1", "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": "046e65ea-38f4-11eb-acc9-0eeefa290e51", "author_flair_text": " 8==D -> 8===D -> 8====D", "author_flair_text_color": "light", "author_flair_type": "text", "author_fullname": "t2_v8kw2mxj", "author_patreon_flair": false, "author_premium": false, "body": "But I went ahead and blocked all you guys that seem to have a problem with me, idk if that's gonna keep you from seeing my posts. I'm still new to reddit but hopefully it will somewhat limit my exposure to you guys.", "can_gild": true, "collapsed": true, "collapsed_because_crowd_control": null, "collapsed_reason": "comment score below threshold", "collapsed_reason_code": "LOW_SCORE", "comment_type": null, "controversiality": 0, "created_utc": 1672614077, "distinguished": null, "edited": false, "gilded": 0, "gildings": {}, "id": "j2k63hf", "is_submitter": false, "link_id": "t3_100q9kw", "locked": false, "name": "t1_j2k63hf", "no_follow": true, "parent_id": "t1_j2k5k7b", "permalink": "/r/gettingbigger/comments/100q9kw/be_wary_of_affiliates/j2k63hf/", "retrieved_on": 1676212847, "score": -8, "score_hidden": false, "send_replies": true, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_type": "public", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "unrepliable_reason": null, "ups": -8}
{"all_awardings": [], "archived": false, "associated_award": null, "author": "Hunter_Hangz", "author_created_utc": 1672145226, "author_flair_background_color": "#005ba1", "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": "046e65ea-38f4-11eb-acc9-0eeefa290e51", "author_flair_text": " 8==D -> 8===D -> 8====D", "author_flair_text_color": "light", "author_flair_type": "text", "author_fullname": "t2_v8kw2mxj", "author_patreon_flair": false, "author_premium": false, "body": "Damn I just got outcasted for the second time by a penis enlargement clique on Reddit. I don't know how I'm going to recover or feed my kids ...", "can_gild": true, "collapsed": true, "collapsed_because_crowd_control": null, "collapsed_reason": "comment score below threshold", "collapsed_reason_code": "LOW_SCORE", "comment_type": null, "controversiality": 0, "created_utc": 1672614409, "distinguished": null, "edited": false, "gilded": 0, "gildings": {}, "id": "j2k6w0y", "is_submitter": false, "link_id": "t3_100q9kw", "locked": false, "name": "t1_j2k6w0y", "no_follow": true, "parent_id": "t1_j2k5u0w", "permalink": "/r/gettingbigger/comments/100q9kw/be_wary_of_affiliates/j2k6w0y/", "retrieved_on": 1676212827, "score": -16, "score_hidden": false, "send_replies": true, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_type": "public", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "unrepliable_reason": null, "ups": -16}
{"all_awardings": [], "archived": false, "associated_award": null, "author": "19Expansion2X", "author_created_utc": 1599787511, "author_flair_background_color": null, "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": null, "author_flair_text": null, "author_flair_text_color": null, "author_flair_type": "text", "author_fullname": "t2_81r0llw4", "author_patreon_flair": false, "author_premium": false, "body": "Im an affiliate but I actually care about my reputation. I enjoy experimenting with new devices but if it\u2019s bullshit ima call it bullshit that\u2019s why I could make a post like this\n\nhttps://www.reddit.com/r/gettingbigger/comments/vxktsn/total_man_hanger_looks_pretty_interesting_also_a/?utm_source=share&utm_medium=ios_app&utm_name=iossmf\n\n\nfollowed by a post like this\n\nhttps://www.reddit.com/r/gettingbigger/comments/wjo21s/am_i_the_only_guy_that_hates_the_new_total_man/?utm_source=share&utm_medium=ios_app&utm_name=iossmf\n\nI only became a Total Man affiliate because the OG was so good. I used it religiously And nothing at that price point compared to it. But I\u2019m extremely honest about the new line being complete trash. I\u2019ve made endless comments stating that in hopes of helping guys save some money\nhttps://www.reddit.com/r/gettingbigger/comments/x1oy08/i_finally_found_a_purpose_for_those_leluv_bags/?utm_source=share&utm_medium=ios_app&utm_name=iossmf\n\nI was waiting for this post. you make a great point when you said December we all knew exactly who you were referring to & he knows too that why he\u2019s so mad. I think he assumed the rebrand would work. I tried give him some solid advice to be less annoying but he didn\u2019t take it https://www.reddit.com/r/gettingbigger/comments/zlogze/chemsexcouple_ban/j085pbs/?utm_source=share&utm_medium=ios_app&utm_name=iossmf&context=3", "can_gild": true, "collapsed": false, "collapsed_because_crowd_control": null, "collapsed_reason": null, "collapsed_reason_code": null, "comment_type": null, "controversiality": 1, "created_utc": 1672619862, "distinguished": null, "edited": 1672622313, "gilded": 0, "gildings": {}, "id": "j2kjws6", "is_submitter": false, "link_id": "t3_100q9kw", "locked": false, "name": "t1_j2kjws6", "no_follow": true, "parent_id": "t3_100q9kw", "permalink": "/r/gettingbigger/comments/100q9kw/be_wary_of_affiliates/j2kjws6/", "retrieved_on": 1676212491, "score": 2, "score_hidden": false, "send_replies": true, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_type": "public", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "unrepliable_reason": null, "ups": 2}
{"all_awardings": [], "archived": false, "associated_award": null, "author": "bd19962015", "author_created_utc": 1546227149, "author_flair_background_color": "#373c3f", "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": "29fba9d6-d2a3-11ec-85ad-ae629c392782", "author_flair_text": "BD - idk somewhere between 4 and 12", "author_flair_text_color": "dark", "author_flair_type": "text", "author_fullname": "t2_2w5g9689", "author_patreon_flair": false, "author_premium": false, "body": "Yeah I had a conversation in private with in what he needed to do but he did not take it to heart obviously", "can_gild": true, "collapsed": false, "collapsed_because_crowd_control": null, "collapsed_reason": null, "collapsed_reason_code": null, "comment_type": null, "controversiality": 0, "created_utc": 1672620371, "distinguished": null, "edited": false, "gilded": 0, "gildings": {}, "id": "j2kl230", "is_submitter": true, "link_id": "t3_100q9kw", "locked": false, "name": "t1_j2kl230", "no_follow": false, "parent_id": "t1_j2kjws6", "permalink": "/r/gettingbigger/comments/100q9kw/be_wary_of_affiliates/j2kl230/", "retrieved_on": 1676212461, "score": 8, "score_hidden": false, "send_replies": true, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_type": "public", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "unrepliable_reason": null, "ups": 8}
{"all_awardings": [], "archived": false, "associated_award": null, "author": "19Expansion2X", "author_created_utc": 1599787511, "author_flair_background_color": null, "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": null, "author_flair_text": null, "author_flair_text_color": null, "author_flair_type": "text", "author_fullname": "t2_81r0llw4", "author_patreon_flair": false, "author_premium": false, "body": "\ud83d\ude02\ud83d\ude02\u2620\ufe0f", "can_gild": true, "collapsed": false, "collapsed_because_crowd_control": null, "collapsed_reason": null, "collapsed_reason_code": null, "comment_type": null, "controversiality": 0, "created_utc": 1672621128, "distinguished": null, "edited": false, "gilded": 0, "gildings": {}, "id": "j2kmpyx", "is_submitter": false, "link_id": "t3_100q9kw", "locked": false, "name": "t1_j2kmpyx", "no_follow": false, "parent_id": "t1_j2k00s6", "permalink": "/r/gettingbigger/comments/100q9kw/be_wary_of_affiliates/j2kmpyx/", "retrieved_on": 1676212419, "score": 5, "score_hidden": false, "send_replies": true, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_type": "public", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "unrepliable_reason": null, "ups": 5}
{"all_awardings": [], "archived": false, "associated_award": null, "author": "fotw75", "author_created_utc": 1590521791, "author_flair_background_color": "#ffd635", "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": "304d65a8-38f4-11eb-a051-0ecb59bb3fe1", "author_flair_text": "(B: 5.75Lx4.25G) (C: 6.75Lx4.75G) (G: 7.5Lx5.25G)", "author_flair_text_color": "dark", "author_flair_type": "text", "author_fullname": "t2_5ip4qf2h", "author_patreon_flair": false, "author_premium": false, "body": ">I was waiting for this post. you make a great point when you said December we all knew exactly who you were referring to & he knows too that why he\u2019s so mad.\n\nMe too.\n\nAaaaand it appears he has taken his ball and flounced off in a huff. Or got banned.\n\nEither way - hopefully if he comes back he realizes his way of doing things doesn't work. Being an affiliate is perfectly fine. Coming along, acting like he's some kind of PE Guru with a character name and everything, and doling out copy paste advice on the backs of others' hard work. Nah.", "can_gild": true, "collapsed": false, "collapsed_because_crowd_control": null, "collapsed_reason": null, "collapsed_reason_code": null, "comment_type": null, "controversiality": 0, "created_utc": 1672621130, "distinguished": null, "edited": false, "gilded": 0, "gildings": {}, "id": "j2kmq4x", "is_submitter": false, "link_id": "t3_100q9kw", "locked": false, "name": "t1_j2kmq4x", "no_follow": false, "parent_id": "t1_j2kjws6", "permalink": "/r/gettingbigger/comments/100q9kw/be_wary_of_affiliates/j2kmq4x/", "retrieved_on": 1676212419, "score": 3, "score_hidden": false, "send_replies": true, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_type": "public", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "unrepliable_reason": null, "ups": 3}
{"all_awardings": [], "archived": false, "associated_award": null, "author": "19Expansion2X", "author_created_utc": 1599787511, "author_flair_background_color": null, "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": null, "author_flair_text": null, "author_flair_text_color": null, "author_flair_type": "text", "author_fullname": "t2_81r0llw4", "author_patreon_flair": false, "author_premium": false, "body": "Well apparently he blocked you now not like you should care but you did your best to warn the newbies so your hands are clean. If anybody falls into his trap now it\u2019s not the fault of the sub at all", "can_gild": true, "collapsed": false, "collapsed_because_crowd_control": null, "collapsed_reason": null, "collapsed_reason_code": null, "comment_type": null, "controversiality": 0, "created_utc": 1672621346, "distinguished": null, "edited": false, "gilded": 0, "gildings": {}, "id": "j2kn6zk", "is_submitter": false, "link_id": "t3_100q9kw", "locked": false, "name": "t1_j2kn6zk", "no_follow": false, "parent_id": "t1_j2kl230", "permalink": "/r/gettingbigger/comments/100q9kw/be_wary_of_affiliates/j2kn6zk/", "retrieved_on": 1676212407, "score": 4, "score_hidden": false, "send_replies": true, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_type": "public", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "unrepliable_reason": null, "ups": 4}

================
File: data/gettingbigger_submissions.json
================
{"all_awardings": [], "allow_live_comments": false, "archived": false, "author": "bd19962015", "author_created_utc": 1546227149, "author_flair_background_color": "#373c3f", "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": "29fba9d6-d2a3-11ec-85ad-ae629c392782", "author_flair_text": "BD - idk somewhere between 4 and 12", "author_flair_text_color": "dark", "author_flair_type": "text", "author_fullname": "t2_2w5g9689", "author_patreon_flair": false, "author_premium": false, "awarders": [], "banned_by": null, "can_gild": true, "can_mod_post": false, "category": null, "content_categories": null, "contest_mode": false, "created_utc": 1672598964, "discussion_type": null, "distinguished": null, "domain": "self.gettingbigger", "edited": 1672603272, "gilded": 0, "gildings": {}, "hidden": false, "hide_score": false, "id": "100q9kw", "is_created_from_ads_ui": false, "is_crosspostable": true, "is_meta": false, "is_original_content": false, "is_reddit_media_domain": false, "is_robot_indexable": true, "is_self": true, "is_video": false, "link_flair_background_color": "#1a1a1b", "link_flair_css_class": "", "link_flair_richtext": [], "link_flair_template_id": "7e7d7a68-b9ac-11eb-9174-0e00830b723f", "link_flair_text": "META\u2755", "link_flair_text_color": "light", "link_flair_type": "text", "locked": false, "media": null, "media_embed": {}, "media_only": false, "name": "t3_100q9kw", "no_follow": false, "num_comments": 27, "num_crossposts": 0, "over_18": true, "parent_whitelist_status": null, "permalink": "/r/gettingbigger/comments/100q9kw/be_wary_of_affiliates/", "pinned": false, "pwls": null, "quarantine": false, "removed_by": null, "removed_by_category": null, "retrieved_on": 1676252437, "score": 67, "secure_media": null, "secure_media_embed": {}, "selftext": "Yo, \n\nHappy... Almost making a complete loop around an arbitrary point in orbit around the sun day. \n\n\nThe mods have been notified of numerous affiliate accounts rising up particularly in December \n\n\nIt would be hypocritical of me to remove these accounts entirely as I also make money off of PE... In a different way through coaching and PE adjacent products.  \n\n\nI think we do have a rule against balant product promotion but  the smarter ones use choice wording to get around it technically\n\n\nHowever... I want it to be known that I don't necessarily condone of these practices... Particularly when the product is not as great as the company says. \n\n\nA few years ago it was nothing but bathmate hydromax plaguing PE.... Don't get me wrong. It's good but it's not the holygrail these affiliates made it out to be. \n\n\nNow we have another company... Doing a similar tactic... Totalman \n\n\nTotalman used to have a pretty decent package but as most companies do... It's a race to bottom in cost and race to the top in profits... \n\n\nSo they stoppes making their good extenders and started selling an inferior version that is much easier and cheaper to produce\n\n\nThat is beside the point \n\n\nIn this era of online marketing... It is much more effective to use influencers to build trust and sell a product than it is to make ads... Because they will do a lot more work than an ad buy ever will \n\n\nSo we have a few hustlers trying to make a name for themselves... Build a following and sell heating pads in hopes they can make 10 dollars off of you when you buy from their link. \n\n\nI have seen the same talking points from multiple affiliates this month... To these guys... Try harder... if you want people to actually take you seriously... Don't just copy and paste the TM approved talking point to make a novel reason why you need a heating pad in general.\n\nOne of you said you want to be like me... Do you see me shilling out other products? \n\n\n**This post is to be a warning to you new guys...** \n\n\nIf they paint a flawless picture of a product they are not being genuine...\n \n\nIf you read the same points over and over again about the same product with little downsides they are almost certainly shilling \n\n\nI do make product recommendations. I don't take money from the companies. I usually buy the product with my own cash first... So I can speak freely. \n\n\nI am yet to find a flawless method, I am yet to find something that is 10x better than the alternative... It usually just comes down to preference \n\n\nHeating pads for example .. I have used IR and copper coils.... They work just about the same ... IR is easier to control the temperature however but for the 3x in cost is it really worth it... Probably not. \n\n\n\n\nTLDR: \n\n\nWatch out for affiliates they don't have your best interest at heart.  Make sure you do your own research on the products and check the usernames of commenters to rule out any patterns", "send_replies": true, "spoiler": false, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_subscribers": 57470, "subreddit_type": "public", "suggested_sort": null, "thumbnail": "nsfw", "thumbnail_height": null, "thumbnail_width": null, "title": "Be wary of affiliates", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "upvote_ratio": 0.9599999785423279, "url": "https://www.reddit.com/r/gettingbigger/comments/100q9kw/be_wary_of_affiliates/", "view_count": null, "whitelist_status": null, "wls": null, "ups": 67}
{"all_awardings": [], "allow_live_comments": false, "archived": false, "author": "pastelyro", "author_created_utc": 1659126082, "author_flair_background_color": "#d3d6da", "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": "a8a3dff8-c1e9-11ec-a01e-f27f07d63108", "author_flair_text": "\u200c", "author_flair_text_color": "dark", "author_flair_type": "text", "author_fullname": "t2_qn5p7eyf", "author_patreon_flair": false, "author_premium": false, "awarders": [], "banned_by": null, "can_gild": true, "can_mod_post": false, "category": null, "content_categories": null, "contest_mode": false, "created_utc": 1672607022, "discussion_type": null, "distinguished": null, "domain": "self.gettingbigger", "edited": false, "gilded": 0, "gildings": {}, "hidden": false, "hide_score": false, "id": "100tdlm", "is_created_from_ads_ui": false, "is_crosspostable": true, "is_meta": false, "is_original_content": false, "is_reddit_media_domain": false, "is_robot_indexable": true, "is_self": true, "is_video": false, "link_flair_background_color": "", "link_flair_css_class": null, "link_flair_richtext": [], "link_flair_text": null, "link_flair_text_color": "dark", "link_flair_type": "text", "locked": false, "media": null, "media_embed": {}, "media_only": false, "name": "t3_100tdlm", "no_follow": false, "num_comments": 27, "num_crossposts": 0, "over_18": true, "parent_whitelist_status": null, "permalink": "/r/gettingbigger/comments/100tdlm/now_that_the_new_total_man_extender_is_complete/", "pinned": false, "pwls": null, "quarantine": false, "removed_by": null, "removed_by_category": null, "retrieved_on": 1676252343, "score": 27, "secure_media": null, "secure_media_embed": {}, "selftext": "", "send_replies": true, "spoiler": false, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_subscribers": 57470, "subreddit_type": "public", "suggested_sort": null, "thumbnail": "nsfw", "thumbnail_height": null, "thumbnail_width": null, "title": "Now that the new total man extender is complete garbage. Which one do you recommend buying instead?", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "upvote_ratio": 0.9200000166893005, "url": "https://www.reddit.com/r/gettingbigger/comments/100tdlm/now_that_the_new_total_man_extender_is_complete/", "view_count": null, "whitelist_status": null, "wls": null, "ups": 27}
{"all_awardings": [{"award_sub_type": "GLOBAL", "award_type": "global", "awardings_required_to_grant_benefits": null, "coin_price": 150, "coin_reward": 0, "count": 1, "days_of_drip_extension": null, "days_of_premium": null, "description": "Thank you stranger. Shows the award.", "end_date": null, "giver_coin_reward": null, "icon_format": null, "icon_height": 2048, "icon_url": "https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png", "icon_width": 2048, "id": "award_f44611f1-b89e-46dc-97fe-892280b13b82", "is_enabled": true, "is_new": false, "name": "Helpful", "penny_donate": null, "penny_price": null, "resized_icons": [{"height": 16, "url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&height=16&auto=webp&v=enabled&s=c670b7d7bc99c03bffde92706ad5ceeda12658f3", "width": 16}, {"height": 32, "url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&height=32&auto=webp&v=enabled&s=63a498673bd4a518a031783179a767cc4135d5f5", "width": 32}, {"height": 48, "url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&height=48&auto=webp&v=enabled&s=e8802df47965bd66370b72ac3cb7639e9eae92ae", "width": 48}, {"height": 64, "url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&height=64&auto=webp&v=enabled&s=fc40ae1c1a18193f190da70a2748d0a48c17a5a9", "width": 64}, {"height": 128, "url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&height=128&auto=webp&v=enabled&s=77ba4d8e862ca183dd8c09e002fd123a6b2f52f5", "width": 128}], "resized_static_icons": [{"height": 16, "url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&height=16&auto=webp&v=enabled&s=c670b7d7bc99c03bffde92706ad5ceeda12658f3", "width": 16}, {"height": 32, "url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&height=32&auto=webp&v=enabled&s=63a498673bd4a518a031783179a767cc4135d5f5", "width": 32}, {"height": 48, "url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&height=48&auto=webp&v=enabled&s=e8802df47965bd66370b72ac3cb7639e9eae92ae", "width": 48}, {"height": 64, "url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&height=64&auto=webp&v=enabled&s=fc40ae1c1a18193f190da70a2748d0a48c17a5a9", "width": 64}, {"height": 128, "url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&height=128&auto=webp&v=enabled&s=77ba4d8e862ca183dd8c09e002fd123a6b2f52f5", "width": 128}], "start_date": null, "static_icon_height": 2048, "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png", "static_icon_width": 2048, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "subreddit_id": null, "tiers_by_required_awardings": null}], "allow_live_comments": false, "archived": false, "author": "Hinkle_McKringlebry", "author_created_utc": 1609881939, "author_flair_background_color": "#94e044", "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": "aa367482-38f3-11eb-b167-0ee88904d009", "author_flair_text": "1x1 Hinkometers ", "author_flair_text_color": "dark", "author_flair_type": "text", "author_fullname": "t2_9odhqyow", "author_patreon_flair": false, "author_premium": true, "awarders": [], "banned_by": null, "can_gild": true, "can_mod_post": false, "category": null, "content_categories": null, "contest_mode": false, "created_utc": 1672621551, "discussion_type": null, "distinguished": null, "domain": "self.gettingbigger", "edited": 1672625306, "gilded": 0, "gildings": {}, "hidden": false, "hide_score": false, "id": "100z1cq", "is_created_from_ads_ui": false, "is_crosspostable": true, "is_meta": false, "is_original_content": false, "is_reddit_media_domain": false, "is_robot_indexable": true, "is_self": true, "is_video": false, "link_flair_background_color": "#7193ff", "link_flair_css_class": "", "link_flair_richtext": [], "link_flair_template_id": "6fa65b48-aa12-11ec-bf3b-8a8d7167d101", "link_flair_text": "\ud83c\udf2cINSPIRATIONAL\u2755", "link_flair_text_color": "light", "link_flair_type": "text", "locked": false, "media": null, "media_embed": {}, "media_only": false, "name": "t3_100z1cq", "no_follow": false, "num_comments": 9, "num_crossposts": 0, "over_18": true, "parent_whitelist_status": null, "permalink": "/r/gettingbigger/comments/100z1cq/my_top_10_rgetting_bigger_posts_of_2022/", "pinned": false, "pwls": null, "quarantine": false, "removed_by": null, "removed_by_category": null, "retrieved_on": 1676252078, "score": 98, "secure_media": null, "secure_media_embed": {}, "selftext": "It\u2019s been a unbelievable year for the sub Reddit with the amount of growth that we have had, not to mention me and BD have launched a successful supplement company and successful YouTube channels. That being said, the post on here are the heart of our content and mission.\n\nThere have been some amazing posts over the year and I\u2019m so proud of what people have done but these were some of the best of the best in my opinion. For conflict of interest purposes I did not include any posts from me or BD on the list. Some had great science, some I just related to, and some made me laugh. \n\nThat being said here\u2019s my list of my favorite posts from 2022. I look forward to seeing what 2023 is going to bring.\n\n\nGold standard for consistent measurement. \nhttps://www.reddit.com/r/gettingbigger/comments/x4zd9x/progress_archive/?utm_source=share&utm_medium=ios_app&utm_name=iossmf\n\n\nGoing above and beyond to bring real science and data to this field \nhttps://www.reddit.com/r/gettingbigger/comments/uz04ky/nitric_oxide_boosters_to_aid_pe_and_erections_i/?utm_source=share&utm_medium=ios_app&utm_name=iossmf\n\nI actually spit out my drink laughing reading this one \nhttps://www.reddit.com/r/gettingbigger/comments/wbfmtn/when_your_dick_is_irreparably_discolored_from/?utm_source=share&utm_medium=ios_app&utm_name=iossmf\n\nThis one hit home for me bc almost exact same thing happened with my partner. She was convinced it was my finasteride making it bigger. \nhttps://www.reddit.com/r/gettingbigger/comments/ykn1xs/the_lady_confirmed/?utm_source=share&utm_medium=ios_app&utm_name=iossmf\n\nAnother well done high effort post. I don\u2019t want every single before and after analyzed like this but great shit  here\nhttps://www.reddit.com/r/gettingbigger/comments/vd106e/not_as_good_as_it_looks_fake_gains_for_sure/?utm_source=share&utm_medium=ios_app&utm_name=iossmf\n\nBrilliant write up. High effort. High proven science. Love it \nhttps://www.reddit.com/r/gettingbigger/comments/xwg9e7/therapeutic_effects_of_heat_cold_and_stretch_on/?utm_source=share&utm_medium=ios_app&utm_name=iossmf\n\nBecause This is a scenario I want so badly for the girls that called me small before. I lived vicariously through the OP. \nhttps://www.reddit.com/r/gettingbigger/comments/w24mxm/had_sex_with_my_highschool_ex/?utm_source=share&utm_medium=ios_app&utm_name=iossmf\n\nThis is the biggest thing that has impacted by dysmorphia was knowing that I could do something about it. As a result I really related to this post\nhttps://www.reddit.com/r/gettingbigger/comments/z3lwzp/i_am_thankful_for_the_ability_to_get_bigger/?utm_source=share&utm_medium=ios_app&utm_name=iossmf\n\nI still think this is a terrible study but the fact that number one a literally famous urologist posted on our sub combine with the fact that it is an actual clinical trial to demonstrate natural enlargement made this easily one of the best posts of the year\nhttps://www.reddit.com/r/gettingbigger/comments/yjkk8s/results_from_the_plong_study_presented_at_the/?utm_source=share&utm_medium=ios_app&utm_name=iossmf\n\nI will never as long as I will forget the first time I heard my significant other sincerely and unprompted say I have a big dick. More importantly I will never forget the first time she said it and I actually believed her. Another one that hit home\nhttps://www.reddit.com/r/gettingbigger/comments/uxon96/for_the_first_time_in_my_life_a_woman_told_me_i/?utm_source=share&utm_medium=ios_app&utm_name=iossmf\n\n\u2014\nPeace and love\n\nHink\n\nEdit: I would love to know what you guys top posts were. Feel free to comment below with the link and why", "send_replies": true, "spoiler": false, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_subscribers": 57471, "subreddit_type": "public", "suggested_sort": null, "thumbnail": "nsfw", "thumbnail_height": null, "thumbnail_width": null, "title": "My top 10 r/getting bigger posts of 2022", "top_awarded_type": null, "total_awards_received": 1, "treatment_tags": [], "upvote_ratio": 0.9800000190734863, "url": "https://www.reddit.com/r/gettingbigger/comments/100z1cq/my_top_10_rgetting_bigger_posts_of_2022/", "view_count": null, "whitelist_status": null, "wls": null, "ups": 98}
{"all_awardings": [], "allow_live_comments": false, "archived": false, "author": "19Expansion2X", "author_created_utc": 1599787511, "author_flair_background_color": null, "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": null, "author_flair_text": null, "author_flair_text_color": null, "author_flair_type": "text", "author_fullname": "t2_81r0llw4", "author_patreon_flair": false, "author_premium": false, "awarders": [], "banned_by": null, "can_gild": true, "can_mod_post": false, "category": null, "content_categories": null, "contest_mode": false, "created_utc": 1672624709, "discussion_type": null, "distinguished": null, "domain": "reddit.com", "edited": false, "gallery_data": {"items": [{"id": 225262257, "media_id": "pvorm2nhej9a1"}, {"id": 225262258, "media_id": "h0bed3nhej9a1"}, {"id": 225262259, "media_id": "hox0w2nhej9a1"}, {"id": 225262260, "media_id": "daow33nhej9a1"}, {"id": 225262261, "media_id": "66nnw2nhej9a1"}, {"id": 225262262, "media_id": "mz4tu3nhej9a1"}]}, "gilded": 0, "gildings": {}, "hidden": false, "hide_score": false, "id": "10106ar", "is_created_from_ads_ui": false, "is_crosspostable": true, "is_gallery": true, "is_meta": false, "is_original_content": false, "is_reddit_media_domain": false, "is_robot_indexable": true, "is_self": false, "is_video": false, "link_flair_background_color": "#ffb000", "link_flair_css_class": "", "link_flair_richtext": [], "link_flair_template_id": "611e213a-38c8-11eb-a3af-0e47ed31ccdd", "link_flair_text": "Discussion\ud83d\udde3", "link_flair_text_color": "dark", "link_flair_type": "text", "locked": false, "media": null, "media_embed": {}, "media_metadata": {"66nnw2nhej9a1": {"e": "Image", "id": "66nnw2nhej9a1", "m": "image/jpg", "o": [{"u": "https://preview.redd.it/66nnw2nhej9a1.jpg?width=1080&blur=40&format=pjpg&auto=webp&v=enabled&s=d7eb55dbccbde99f47fa41f2cee39fb829834329", "x": 1242, "y": 2688}], "p": [{"u": "https://preview.redd.it/66nnw2nhej9a1.jpg?width=108&crop=smart&auto=webp&v=enabled&s=70756c1c7d3c2cfeec22d476570a670e5a02135b", "x": 108, "y": 216}, {"u": "https://preview.redd.it/66nnw2nhej9a1.jpg?width=216&crop=smart&auto=webp&v=enabled&s=b7705d42b5495ad3d9aa3ff32b6ab5fcb86f22c0", "x": 216, "y": 432}, {"u": "https://preview.redd.it/66nnw2nhej9a1.jpg?width=320&crop=smart&auto=webp&v=enabled&s=68c71630f9dff458047d852dbc7e76007516d99f", "x": 320, "y": 640}, {"u": "https://preview.redd.it/66nnw2nhej9a1.jpg?width=640&crop=smart&auto=webp&v=enabled&s=51a8037b1dabcb4cb833b1fa614cb21cf7ca8af3", "x": 640, "y": 1280}, {"u": "https://preview.redd.it/66nnw2nhej9a1.jpg?width=960&crop=smart&auto=webp&v=enabled&s=2699f0667ce3e428c52912df7084912bbaab566d", "x": 960, "y": 1920}, {"u": "https://preview.redd.it/66nnw2nhej9a1.jpg?width=1080&crop=smart&auto=webp&v=enabled&s=c7d731d9856ec6ddc1a6111162b650662edb4c86", "x": 1080, "y": 2160}], "s": {"u": "https://preview.redd.it/66nnw2nhej9a1.jpg?width=1242&format=pjpg&auto=webp&v=enabled&s=b14e5ca70fd2145f12ea6a15d70d7ff23c443709", "x": 1242, "y": 2688}, "status": "valid"}, "daow33nhej9a1": {"e": "Image", "id": "daow33nhej9a1", "m": "image/jpg", "o": [{"u": "https://preview.redd.it/daow33nhej9a1.jpg?width=1080&blur=40&format=pjpg&auto=webp&v=enabled&s=3addfdda7a6ac8de51505b6ca14d95cca65f276d", "x": 1242, "y": 2688}], "p": [{"u": "https://preview.redd.it/daow33nhej9a1.jpg?width=108&crop=smart&auto=webp&v=enabled&s=ce00de31c8046e699fe65ee00016e55a28c56bb1", "x": 108, "y": 216}, {"u": "https://preview.redd.it/daow33nhej9a1.jpg?width=216&crop=smart&auto=webp&v=enabled&s=fe7321ac9ef8d0d381ad19e68d37b466f8a13898", "x": 216, "y": 432}, {"u": "https://preview.redd.it/daow33nhej9a1.jpg?width=320&crop=smart&auto=webp&v=enabled&s=3a9d96c7566dc8f864e4d205c2b4f322d0cbd061", "x": 320, "y": 640}, {"u": "https://preview.redd.it/daow33nhej9a1.jpg?width=640&crop=smart&auto=webp&v=enabled&s=74fa5588fccc56c7a534c5f8317a634a124bbbf0", "x": 640, "y": 1280}, {"u": "https://preview.redd.it/daow33nhej9a1.jpg?width=960&crop=smart&auto=webp&v=enabled&s=c9518b0b888e36c04230a9883e2c95abea42ab7a", "x": 960, "y": 1920}, {"u": "https://preview.redd.it/daow33nhej9a1.jpg?width=1080&crop=smart&auto=webp&v=enabled&s=f1bd75569983520de61e85090165cabffc725d81", "x": 1080, "y": 2160}], "s": {"u": "https://preview.redd.it/daow33nhej9a1.jpg?width=1242&format=pjpg&auto=webp&v=enabled&s=fa181a239bba3340dcd130db57cc1ed473cb5207", "x": 1242, "y": 2688}, "status": "valid"}, "h0bed3nhej9a1": {"e": "Image", "id": "h0bed3nhej9a1", "m": "image/jpg", "o": [{"u": "https://preview.redd.it/h0bed3nhej9a1.jpg?width=1080&blur=40&format=pjpg&auto=webp&v=enabled&s=a7815b04543450de44e1ceeff6b126f6a71323a1", "x": 1242, "y": 2688}], "p": [{"u": "https://preview.redd.it/h0bed3nhej9a1.jpg?width=108&crop=smart&auto=webp&v=enabled&s=477a4da16387c81d7640e84db112a0e0c490213d", "x": 108, "y": 216}, {"u": "https://preview.redd.it/h0bed3nhej9a1.jpg?width=216&crop=smart&auto=webp&v=enabled&s=9fb704dc03283b0e69e4253393477aa92a88b738", "x": 216, "y": 432}, {"u": "https://preview.redd.it/h0bed3nhej9a1.jpg?width=320&crop=smart&auto=webp&v=enabled&s=86df521c44951eb05a4b88cadb8427b57f14c4b9", "x": 320, "y": 640}, {"u": "https://preview.redd.it/h0bed3nhej9a1.jpg?width=640&crop=smart&auto=webp&v=enabled&s=9510a0b05fb029ebd7f0d35ddadf841ec968e8ce", "x": 640, "y": 1280}, {"u": "https://preview.redd.it/h0bed3nhej9a1.jpg?width=960&crop=smart&auto=webp&v=enabled&s=1c6ed6ee8b369daf0e1dbe5a6740a9511f0c87d5", "x": 960, "y": 1920}, {"u": "https://preview.redd.it/h0bed3nhej9a1.jpg?width=1080&crop=smart&auto=webp&v=enabled&s=84af206ec80df51accd54a0504f2825d3dc9fe87", "x": 1080, "y": 2160}], "s": {"u": "https://preview.redd.it/h0bed3nhej9a1.jpg?width=1242&format=pjpg&auto=webp&v=enabled&s=9b1b9981c59537126a5c36756a675fb14b93f884", "x": 1242, "y": 2688}, "status": "valid"}, "hox0w2nhej9a1": {"e": "Image", "id": "hox0w2nhej9a1", "m": "image/jpg", "o": [{"u": "https://preview.redd.it/hox0w2nhej9a1.jpg?width=1080&blur=40&format=pjpg&auto=webp&v=enabled&s=e62ec79620f5b621d99fdfcaaab1254077787e31", "x": 1242, "y": 2688}], "p": [{"u": "https://preview.redd.it/hox0w2nhej9a1.jpg?width=108&crop=smart&auto=webp&v=enabled&s=273e58576e5fce11c9d6f799e6fc548e63f4b2a0", "x": 108, "y": 216}, {"u": "https://preview.redd.it/hox0w2nhej9a1.jpg?width=216&crop=smart&auto=webp&v=enabled&s=47bcff27bfb2b69104721c3cc0c1b88d55df750b", "x": 216, "y": 432}, {"u": "https://preview.redd.it/hox0w2nhej9a1.jpg?width=320&crop=smart&auto=webp&v=enabled&s=f3543d95b1aea44150cc7b74b54b151eba24c736", "x": 320, "y": 640}, {"u": "https://preview.redd.it/hox0w2nhej9a1.jpg?width=640&crop=smart&auto=webp&v=enabled&s=3e97021370810af0b5103811c39029008e09adc5", "x": 640, "y": 1280}, {"u": "https://preview.redd.it/hox0w2nhej9a1.jpg?width=960&crop=smart&auto=webp&v=enabled&s=c5007a4ad1773f3fcbe9eebbb1573bb780a23d2b", "x": 960, "y": 1920}, {"u": "https://preview.redd.it/hox0w2nhej9a1.jpg?width=1080&crop=smart&auto=webp&v=enabled&s=2b8e35cdfe4b520545bf8532a824159f8d1cf378", "x": 1080, "y": 2160}], "s": {"u": "https://preview.redd.it/hox0w2nhej9a1.jpg?width=1242&format=pjpg&auto=webp&v=enabled&s=22a4a8767d339c862707e968cd272fe55626641a", "x": 1242, "y": 2688}, "status": "valid"}, "mz4tu3nhej9a1": {"e": "Image", "id": "mz4tu3nhej9a1", "m": "image/jpg", "o": [{"u": "https://preview.redd.it/mz4tu3nhej9a1.jpg?width=1080&blur=40&format=pjpg&auto=webp&v=enabled&s=f970e67f9c6964c48a050fabdc1eacc2c6b9e479", "x": 1242, "y": 2688}], "p": [{"u": "https://preview.redd.it/mz4tu3nhej9a1.jpg?width=108&crop=smart&auto=webp&v=enabled&s=d4c020f794fa88f01b7a07f59deddc5ce92601e1", "x": 108, "y": 216}, {"u": "https://preview.redd.it/mz4tu3nhej9a1.jpg?width=216&crop=smart&auto=webp&v=enabled&s=7b10acaf1e3d4cba7b306499ff8ddc36f15bc8c1", "x": 216, "y": 432}, {"u": "https://preview.redd.it/mz4tu3nhej9a1.jpg?width=320&crop=smart&auto=webp&v=enabled&s=90ea7c354ecad68eb8d8791e853343b4cb275d78", "x": 320, "y": 640}, {"u": "https://preview.redd.it/mz4tu3nhej9a1.jpg?width=640&crop=smart&auto=webp&v=enabled&s=43804a2021f68ff148363a5a736d29fd701ca423", "x": 640, "y": 1280}, {"u": "https://preview.redd.it/mz4tu3nhej9a1.jpg?width=960&crop=smart&auto=webp&v=enabled&s=9314d8447103ca68e7a273d87c2e40980abdfc44", "x": 960, "y": 1920}, {"u": "https://preview.redd.it/mz4tu3nhej9a1.jpg?width=1080&crop=smart&auto=webp&v=enabled&s=273383557c3e96c799857ddb6c298318a1f05dab", "x": 1080, "y": 2160}], "s": {"u": "https://preview.redd.it/mz4tu3nhej9a1.jpg?width=1242&format=pjpg&auto=webp&v=enabled&s=8940beb178310f72f47e0bdc049376e7f534fdb8", "x": 1242, "y": 2688}, "status": "valid"}, "pvorm2nhej9a1": {"e": "Image", "id": "pvorm2nhej9a1", "m": "image/jpg", "o": [{"u": "https://preview.redd.it/pvorm2nhej9a1.jpg?width=1080&blur=40&format=pjpg&auto=webp&v=enabled&s=9244be4f5b9983b3da9344d9e6d10796dfd6f179", "x": 1242, "y": 2688}], "p": [{"u": "https://preview.redd.it/pvorm2nhej9a1.jpg?width=108&crop=smart&auto=webp&v=enabled&s=c6e3be3d2c6fd6ab30aebe57e74d158ee3c995c1", "x": 108, "y": 216}, {"u": "https://preview.redd.it/pvorm2nhej9a1.jpg?width=216&crop=smart&auto=webp&v=enabled&s=eb996b26d3fa00b01826701f167e185e343dad96", "x": 216, "y": 432}, {"u": "https://preview.redd.it/pvorm2nhej9a1.jpg?width=320&crop=smart&auto=webp&v=enabled&s=d46a5cb824919579af6a9410319cb43a51a179f6", "x": 320, "y": 640}, {"u": "https://preview.redd.it/pvorm2nhej9a1.jpg?width=640&crop=smart&auto=webp&v=enabled&s=d97f683e6e98f9f1861b9356ec1da61bbf359afd", "x": 640, "y": 1280}, {"u": "https://preview.redd.it/pvorm2nhej9a1.jpg?width=960&crop=smart&auto=webp&v=enabled&s=b00ba6270455d01684cffe8fa7f6389d473b29e8", "x": 960, "y": 1920}, {"u": "https://preview.redd.it/pvorm2nhej9a1.jpg?width=1080&crop=smart&auto=webp&v=enabled&s=c6d4736fdad6a9ea47b62e17f6700403d4892c86", "x": 1080, "y": 2160}], "s": {"u": "https://preview.redd.it/pvorm2nhej9a1.jpg?width=1242&format=pjpg&auto=webp&v=enabled&s=dd05fa4664fdcf7e29159a35a0bac5f6f5a00516", "x": 1242, "y": 2688}, "status": "valid"}}, "media_only": false, "name": "t3_10106ar", "no_follow": false, "num_comments": 34, "num_crossposts": 0, "over_18": true, "parent_whitelist_status": null, "permalink": "/r/gettingbigger/comments/10106ar/total_man_review_system_is_a_lil_suspicious_i/", "pinned": false, "pwls": null, "quarantine": false, "removed_by": null, "removed_by_category": null, "retrieved_on": 1676252014, "score": 20, "secure_media": null, "secure_media_embed": {}, "selftext": "", "send_replies": true, "spoiler": false, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_subscribers": 57471, "subreddit_type": "public", "suggested_sort": null, "thumbnail": "nsfw", "thumbnail_height": 140, "thumbnail_width": 140, "title": "\ud83e\udd14Total Man review system is a lil suspicious. I just posted a 2star review on the extender and i don\u2019t see it up. Maybe their hiding all the poor reviews & only making the good ones public. I also found my old recipes. I can\u2019t believe how cheap the 2.0 was", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "upvote_ratio": 0.8799999952316284, "url": "https://www.reddit.com/gallery/10106ar", "url_overridden_by_dest": "https://www.reddit.com/gallery/10106ar", "view_count": null, "whitelist_status": null, "wls": null, "ups": 20}
{"all_awardings": [], "allow_live_comments": false, "archived": false, "author": "Ezio-auditore-di-Me", "author_created_utc": 1669115290, "author_flair_background_color": "", "author_flair_css_class": "", "author_flair_richtext": [], "author_flair_template_id": null, "author_flair_text": "Note: new or low karma account", "author_flair_text_color": "dark", "author_flair_type": "text", "author_fullname": "t2_uh89u0k6", "author_patreon_flair": false, "author_premium": false, "awarders": [], "banned_by": null, "can_gild": true, "can_mod_post": false, "category": null, "content_categories": null, "contest_mode": false, "created_utc": 1672687663, "discussion_type": null, "distinguished": null, "domain": "self.gettingbigger", "edited": false, "gilded": 0, "gildings": {}, "hidden": false, "hide_score": false, "id": "101l6at", "is_created_from_ads_ui": false, "is_crosspostable": true, "is_meta": false, "is_original_content": false, "is_reddit_media_domain": false, "is_robot_indexable": true, "is_self": true, "is_video": false, "link_flair_background_color": "#ea0027", "link_flair_css_class": "", "link_flair_richtext": [], "link_flair_template_id": "51c9c5ec-bc6a-11eb-aaaf-0ec37787e791", "link_flair_text": "HELP \ud83c\udd98", "link_flair_text_color": "light", "link_flair_type": "text", "locked": false, "media": null, "media_embed": {}, "media_only": false, "name": "t3_101l6at", "no_follow": false, "num_comments": 18, "num_crossposts": 0, "over_18": true, "parent_whitelist_status": null, "permalink": "/r/gettingbigger/comments/101l6at/beginner_here_finding_out_pe_has_genuinely_given/", "pinned": false, "pwls": null, "quarantine": false, "removed_by": null, "removed_by_category": null, "retrieved_on": 1676250804, "score": 20, "secure_media": null, "secure_media_embed": {}, "selftext": "", "send_replies": true, "spoiler": false, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_subscribers": 57470, "subreddit_type": "public", "suggested_sort": null, "thumbnail": "nsfw", "thumbnail_height": null, "thumbnail_width": null, "title": "Beginner here, finding out PE has genuinely given me hope as I'm quite small, smaller than maybe all the guys I know. It is keeping me from sleeping around with girls as I fear I'll be mocked. I need help for starting out, I'm too overwhelmed. But too broke right out for equipment, please help.", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "upvote_ratio": 0.949999988079071, "url": "https://www.reddit.com/r/gettingbigger/comments/101l6at/beginner_here_finding_out_pe_has_genuinely_given/", "view_count": null, "whitelist_status": null, "wls": null, "ups": 20}
{"all_awardings": [], "allow_live_comments": false, "archived": false, "author": "growmore96", "author_created_utc": 1668875594, "author_flair_background_color": "#d3d6da", "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": "a8a3dff8-c1e9-11ec-a01e-f27f07d63108", "author_flair_text": "B: NBPEL: 5.7 G: 4.2 C: NBPEL: 6.3 G: 4.4 Goal: 7 L 5 G", "author_flair_text_color": "dark", "author_flair_type": "text", "author_fullname": "t2_uexqgjlt", "author_patreon_flair": false, "author_premium": false, "awarders": [], "banned_by": null, "can_gild": true, "can_mod_post": false, "category": null, "content_categories": null, "contest_mode": false, "created_utc": 1672694705, "discussion_type": null, "distinguished": null, "domain": "self.gettingbigger", "edited": false, "gilded": 0, "gildings": {}, "hidden": false, "hide_score": false, "id": "101o6qz", "is_created_from_ads_ui": false, "is_crosspostable": true, "is_meta": false, "is_original_content": false, "is_reddit_media_domain": false, "is_robot_indexable": true, "is_self": true, "is_video": false, "link_flair_background_color": "", "link_flair_css_class": null, "link_flair_richtext": [], "link_flair_text": null, "link_flair_text_color": "dark", "link_flair_type": "text", "locked": false, "media": null, "media_embed": {}, "media_only": false, "name": "t3_101o6qz", "no_follow": false, "num_comments": 50, "num_crossposts": 0, "over_18": true, "parent_whitelist_status": null, "permalink": "/r/gettingbigger/comments/101o6qz/side_benefits_to_pe/", "pinned": false, "pwls": null, "quarantine": false, "removed_by": null, "removed_by_category": null, "retrieved_on": 1676250638, "score": 55, "secure_media": null, "secure_media_embed": {}, "selftext": "Other than having a longer and thicker manhood, what other benefits have you noticed?", "send_replies": true, "spoiler": false, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_subscribers": 57470, "subreddit_type": "public", "suggested_sort": null, "thumbnail": "nsfw", "thumbnail_height": null, "thumbnail_width": null, "title": "Side benefits to PE", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "upvote_ratio": 1, "url": "https://www.reddit.com/r/gettingbigger/comments/101o6qz/side_benefits_to_pe/", "view_count": null, "whitelist_status": null, "wls": null, "ups": 55}
{"all_awardings": [], "allow_live_comments": false, "archived": false, "author": "hardrockchafe", "author_created_utc": 1565011028, "author_flair_background_color": "#d3d6da", "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": "a8a3dff8-c1e9-11ec-a01e-f27f07d63108", "author_flair_text": "B: 6.25x4.75 C: 6.58x4.88 G: 7.5x5.5", "author_flair_text_color": "dark", "author_flair_type": "text", "author_fullname": "t2_4b27o8vm", "author_patreon_flair": false, "author_premium": false, "awarders": [], "banned_by": null, "can_gild": true, "can_mod_post": false, "category": null, "content_categories": null, "contest_mode": false, "created_utc": 1672719744, "discussion_type": null, "distinguished": null, "domain": "reddit.com", "edited": false, "gallery_data": {"items": [{"caption": "Been going for about a month now actually, but forgot to post my starting stats. I\u2019ll be remeasuring and posting my 1 month update tomorrow! ", "id": 225675665, "media_id": "javk4sx29r9a1"}, {"id": 225675666, "media_id": "vb6qfsx29r9a1"}]}, "gilded": 0, "gildings": {}, "hidden": false, "hide_score": false, "id": "101xyl1", "is_created_from_ads_ui": false, "is_crosspostable": true, "is_gallery": true, "is_meta": false, "is_original_content": false, "is_reddit_media_domain": false, "is_robot_indexable": true, "is_self": false, "is_video": false, "link_flair_background_color": "#a06324", "link_flair_css_class": "", "link_flair_richtext": [], "link_flair_template_id": "c6d8e4da-aa12-11ec-bc07-c20f5c867468", "link_flair_text": "Progress Logs \ud83d\uddc2", "link_flair_text_color": "light", "link_flair_type": "text", "locked": false, "media": null, "media_embed": {}, "media_metadata": {"javk4sx29r9a1": {"e": "Image", "id": "javk4sx29r9a1", "m": "image/jpg", "o": [{"u": "https://preview.redd.it/javk4sx29r9a1.jpg?width=1080&blur=40&format=pjpg&auto=webp&v=enabled&s=fed1b86c326fa20f32b3331a90557c12c31477ee", "x": 3024, "y": 4032}], "p": [{"u": "https://preview.redd.it/javk4sx29r9a1.jpg?width=108&crop=smart&auto=webp&v=enabled&s=65d186a5ca4414c927c721b8d484360dac74352f", "x": 108, "y": 144}, {"u": "https://preview.redd.it/javk4sx29r9a1.jpg?width=216&crop=smart&auto=webp&v=enabled&s=a885c121dc3b596a4296113c4395629f26e56d0e", "x": 216, "y": 288}, {"u": "https://preview.redd.it/javk4sx29r9a1.jpg?width=320&crop=smart&auto=webp&v=enabled&s=d6ca63176a77a75fa2b8d5597f2330cafe95f080", "x": 320, "y": 426}, {"u": "https://preview.redd.it/javk4sx29r9a1.jpg?width=640&crop=smart&auto=webp&v=enabled&s=7642d091b5a73a1de342cbdc5eb625b9fa8ca9e1", "x": 640, "y": 853}, {"u": "https://preview.redd.it/javk4sx29r9a1.jpg?width=960&crop=smart&auto=webp&v=enabled&s=58b1cf0338af278f2e1ea694910a85728c89d7d4", "x": 960, "y": 1280}, {"u": "https://preview.redd.it/javk4sx29r9a1.jpg?width=1080&crop=smart&auto=webp&v=enabled&s=c15a95d610c5391944b27e8e8941f2eae96fb465", "x": 1080, "y": 1440}], "s": {"u": "https://preview.redd.it/javk4sx29r9a1.jpg?width=3024&format=pjpg&auto=webp&v=enabled&s=4d7c955d0064eba3624354600af5ab6c5c941ece", "x": 3024, "y": 4032}, "status": "valid"}, "vb6qfsx29r9a1": {"e": "Image", "id": "vb6qfsx29r9a1", "m": "image/jpg", "o": [{"u": "https://preview.redd.it/vb6qfsx29r9a1.jpg?width=1080&blur=40&format=pjpg&auto=webp&v=enabled&s=4d5b7a1c4c83378e85aaec7ec6686dff78112be0", "x": 3024, "y": 4032}], "p": [{"u": "https://preview.redd.it/vb6qfsx29r9a1.jpg?width=108&crop=smart&auto=webp&v=enabled&s=4ece517383244bb7c734c83663248c67c759486f", "x": 108, "y": 144}, {"u": "https://preview.redd.it/vb6qfsx29r9a1.jpg?width=216&crop=smart&auto=webp&v=enabled&s=50b7fba2ad8286aabe9c89955bf61a298dcb81b1", "x": 216, "y": 288}, {"u": "https://preview.redd.it/vb6qfsx29r9a1.jpg?width=320&crop=smart&auto=webp&v=enabled&s=bf8a7aad981044ab37c6d8e7f62716c1a386cd76", "x": 320, "y": 426}, {"u": "https://preview.redd.it/vb6qfsx29r9a1.jpg?width=640&crop=smart&auto=webp&v=enabled&s=354958e0f483964f488da2718cd18ab138ce2771", "x": 640, "y": 853}, {"u": "https://preview.redd.it/vb6qfsx29r9a1.jpg?width=960&crop=smart&auto=webp&v=enabled&s=983fc2e300639bb4f01b584cbbfc087c451ae80c", "x": 960, "y": 1280}, {"u": "https://preview.redd.it/vb6qfsx29r9a1.jpg?width=1080&crop=smart&auto=webp&v=enabled&s=0b5f9ffa5e8267415dc2978c8f004f35e1610d5f", "x": 1080, "y": 1440}], "s": {"u": "https://preview.redd.it/vb6qfsx29r9a1.jpg?width=3024&format=pjpg&auto=webp&v=enabled&s=57cf3c95607ba0fbda6fd84d2da9a75dbbb4fb6d", "x": 3024, "y": 4032}, "status": "valid"}}, "media_only": false, "name": "t3_101xyl1", "no_follow": false, "num_comments": 15, "num_crossposts": 0, "over_18": true, "parent_whitelist_status": null, "permalink": "/r/gettingbigger/comments/101xyl1/starting_stats_a_month_late/", "pinned": false, "pwls": null, "quarantine": false, "removed_by": null, "removed_by_category": null, "retrieved_on": 1676250037, "score": 24, "secure_media": null, "secure_media_embed": {}, "selftext": "", "send_replies": true, "spoiler": true, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_subscribers": 57470, "subreddit_type": "public", "suggested_sort": null, "thumbnail": "nsfw", "thumbnail_height": 140, "thumbnail_width": 140, "title": "Starting stats (a month late)", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "upvote_ratio": 0.9599999785423279, "url": "https://www.reddit.com/gallery/101xyl1", "url_overridden_by_dest": "https://www.reddit.com/gallery/101xyl1", "view_count": null, "whitelist_status": null, "wls": null, "ups": 24}
{"all_awardings": [], "allow_live_comments": true, "archived": false, "author": "meat_stretcher", "author_created_utc": 1637655904, "author_flair_background_color": "#ffd635", "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": "304d65a8-38f4-11eb-a051-0ecb59bb3fe1", "author_flair_text": "B: 6.75x5 C:7.3x5.1 G:8.5x6", "author_flair_text_color": "dark", "author_flair_type": "text", "author_fullname": "t2_gu4dne84", "author_patreon_flair": false, "author_premium": false, "awarders": [], "banned_by": null, "can_gild": true, "can_mod_post": false, "category": null, "content_categories": null, "contest_mode": false, "created_utc": 1672727409, "discussion_type": null, "distinguished": null, "domain": "reddit.com", "edited": false, "gallery_data": {"items": [{"caption": "See my post history for how I bent my cylinder into this shape. Some people wanted to see what it looked like in use. I\u2019m doing this to help correct a downward curve.", "id": 225706732, "media_id": "uiqofm6vvr9a1"}, {"caption": "The elliptical shape allows for a lot more natural expansion imo. ", "id": 225706733, "media_id": "3g1wtm6vvr9a1"}]}, "gilded": 0, "gildings": {}, "hidden": false, "hide_score": false, "id": "1020grs", "is_created_from_ads_ui": false, "is_crosspostable": true, "is_gallery": true, "is_meta": false, "is_original_content": false, "is_reddit_media_domain": false, "is_robot_indexable": true, "is_self": false, "is_video": false, "link_flair_background_color": "", "link_flair_css_class": null, "link_flair_richtext": [], "link_flair_text": null, "link_flair_text_color": "dark", "link_flair_type": "text", "locked": false, "media": null, "media_embed": {}, "media_metadata": {"3g1wtm6vvr9a1": {"e": "Image", "id": "3g1wtm6vvr9a1", "m": "image/jpg", "o": [{"u": "https://preview.redd.it/3g1wtm6vvr9a1.jpg?width=1080&blur=40&format=pjpg&auto=webp&v=enabled&s=a08f9b155581f41072b5c28cf2599a3a0e7c3d22", "x": 3024, "y": 4032}], "p": [{"u": "https://preview.redd.it/3g1wtm6vvr9a1.jpg?width=108&crop=smart&auto=webp&v=enabled&s=410f4d52ac254ebe1f1cf5c38979f5e9089fbb81", "x": 108, "y": 144}, {"u": "https://preview.redd.it/3g1wtm6vvr9a1.jpg?width=216&crop=smart&auto=webp&v=enabled&s=f75fa1c7d63c19fe9d14ea577162b5e6f3dcf997", "x": 216, "y": 288}, {"u": "https://preview.redd.it/3g1wtm6vvr9a1.jpg?width=320&crop=smart&auto=webp&v=enabled&s=b3c1ec15972d53cfa99b908841fdf689f051f726", "x": 320, "y": 426}, {"u": "https://preview.redd.it/3g1wtm6vvr9a1.jpg?width=640&crop=smart&auto=webp&v=enabled&s=4a7b26c065c212a03f606317c64f8ea483f17c88", "x": 640, "y": 853}, {"u": "https://preview.redd.it/3g1wtm6vvr9a1.jpg?width=960&crop=smart&auto=webp&v=enabled&s=0f898e4825265d0fff211b04e084834eb32aac8e", "x": 960, "y": 1280}, {"u": "https://preview.redd.it/3g1wtm6vvr9a1.jpg?width=1080&crop=smart&auto=webp&v=enabled&s=78fc073d0616f81d381aa7fa983b25bbf7215cba", "x": 1080, "y": 1440}], "s": {"u": "https://preview.redd.it/3g1wtm6vvr9a1.jpg?width=3024&format=pjpg&auto=webp&v=enabled&s=11d0f83928d63649234d0f716ce9edd644c3b2d2", "x": 3024, "y": 4032}, "status": "valid"}, "uiqofm6vvr9a1": {"e": "Image", "id": "uiqofm6vvr9a1", "m": "image/jpg", "o": [{"u": "https://preview.redd.it/uiqofm6vvr9a1.jpg?width=1080&blur=40&format=pjpg&auto=webp&v=enabled&s=6ef7aa1564c58fc6e7f3d8f536240b05f8c226c2", "x": 3024, "y": 4032}], "p": [{"u": "https://preview.redd.it/uiqofm6vvr9a1.jpg?width=108&crop=smart&auto=webp&v=enabled&s=5d995187faccf9c3d77e4dc686be152dab9ee731", "x": 108, "y": 144}, {"u": "https://preview.redd.it/uiqofm6vvr9a1.jpg?width=216&crop=smart&auto=webp&v=enabled&s=64e8f2ab8fda50c808e3f9b366adf4d582584f08", "x": 216, "y": 288}, {"u": "https://preview.redd.it/uiqofm6vvr9a1.jpg?width=320&crop=smart&auto=webp&v=enabled&s=864d81c92c5a287cff72f9c2cec776c85a66fed1", "x": 320, "y": 426}, {"u": "https://preview.redd.it/uiqofm6vvr9a1.jpg?width=640&crop=smart&auto=webp&v=enabled&s=73961cb46c8ceac625ee0f7e8d7f50cd05f82536", "x": 640, "y": 853}, {"u": "https://preview.redd.it/uiqofm6vvr9a1.jpg?width=960&crop=smart&auto=webp&v=enabled&s=bcdf9ea17bb32046c70eef228bc3864b47716fb1", "x": 960, "y": 1280}, {"u": "https://preview.redd.it/uiqofm6vvr9a1.jpg?width=1080&crop=smart&auto=webp&v=enabled&s=4088e9d31e4174f247749b670178f07a55610bcd", "x": 1080, "y": 1440}], "s": {"u": "https://preview.redd.it/uiqofm6vvr9a1.jpg?width=3024&format=pjpg&auto=webp&v=enabled&s=225f3ce4d895ca2c950c99095561358639da96e2", "x": 3024, "y": 4032}, "status": "valid"}}, "media_only": false, "name": "t3_1020grs", "no_follow": false, "num_comments": 34, "num_crossposts": 0, "over_18": true, "parent_whitelist_status": null, "permalink": "/r/gettingbigger/comments/1020grs/pumping_with_my_newly_modified_curved_leluv/", "pinned": false, "pwls": null, "quarantine": false, "removed_by": null, "removed_by_category": null, "retrieved_on": 1676249907, "score": 50, "secure_media": null, "secure_media_embed": {}, "selftext": "", "send_replies": true, "spoiler": true, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_subscribers": 57471, "subreddit_type": "public", "suggested_sort": null, "thumbnail": "nsfw", "thumbnail_height": 140, "thumbnail_width": 140, "title": "Pumping With My Newly Modified Curved LeLuv Cylinder", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "upvote_ratio": 0.9700000286102295, "url": "https://www.reddit.com/gallery/1020grs", "url_overridden_by_dest": "https://www.reddit.com/gallery/1020grs", "view_count": null, "whitelist_status": null, "wls": null, "ups": 50}
{"all_awardings": [], "allow_live_comments": true, "archived": false, "author": "und8658", "author_created_utc": 1620907915, "author_flair_background_color": null, "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": null, "author_flair_text": null, "author_flair_text_color": null, "author_flair_type": "text", "author_fullname": "t2_c3woe2kt", "author_patreon_flair": false, "author_premium": false, "awarders": [], "banned_by": null, "can_gild": true, "can_mod_post": false, "category": null, "content_categories": null, "contest_mode": false, "created_utc": 1672753462, "discussion_type": null, "distinguished": null, "domain": "self.gettingbigger", "edited": false, "gilded": 0, "gildings": {}, "hidden": false, "hide_score": false, "id": "1028lvn", "is_created_from_ads_ui": false, "is_crosspostable": true, "is_meta": false, "is_original_content": false, "is_reddit_media_domain": false, "is_robot_indexable": true, "is_self": true, "is_video": false, "link_flair_background_color": "#dadada", "link_flair_css_class": "", "link_flair_richtext": [], "link_flair_template_id": "0f34328c-38f6-11eb-97fe-0ee00d3fadaf", "link_flair_text": "Newbie Question \u2753", "link_flair_text_color": "dark", "link_flair_type": "text", "locked": false, "media": null, "media_embed": {}, "media_only": false, "name": "t3_1028lvn", "no_follow": false, "num_comments": 59, "num_crossposts": 0, "over_18": true, "parent_whitelist_status": null, "permalink": "/r/gettingbigger/comments/1028lvn/from_grower_to_shower/", "pinned": false, "pwls": null, "quarantine": false, "removed_by": null, "removed_by_category": null, "retrieved_on": 1676249618, "score": 28, "secure_media": null, "secure_media_embed": {}, "selftext": "hey, it might be a stupid question but since I'll start with pe, my question is if the gained length and girt will stay while soft and so I'm getting a shower instead of a grower?\nalso how much is the maximum that I can gain?", "send_replies": true, "spoiler": false, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_subscribers": 57470, "subreddit_type": "public", "suggested_sort": null, "thumbnail": "nsfw", "thumbnail_height": null, "thumbnail_width": null, "title": "from grower to shower?", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "upvote_ratio": 1, "url": "https://www.reddit.com/r/gettingbigger/comments/1028lvn/from_grower_to_shower/", "view_count": null, "whitelist_status": null, "wls": null, "ups": 28}
{"all_awardings": [], "allow_live_comments": true, "archived": false, "author": "Cultural_Aardvark_85", "author_created_utc": 1646960624, "author_flair_background_color": "#d3d6da", "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": "a8a3dff8-c1e9-11ec-a01e-f27f07d63108", "author_flair_text": "\u200c", "author_flair_text_color": "dark", "author_flair_type": "text", "author_fullname": "t2_kkfx8wfx", "author_patreon_flair": false, "author_premium": false, "awarders": [], "banned_by": null, "can_gild": true, "can_mod_post": false, "category": null, "content_categories": null, "contest_mode": false, "created_utc": 1672779529, "discussion_type": null, "distinguished": null, "domain": "self.gettingbigger", "edited": false, "gilded": 0, "gildings": {}, "hidden": false, "hide_score": false, "id": "102j8bw", "is_created_from_ads_ui": false, "is_crosspostable": true, "is_meta": false, "is_original_content": false, "is_reddit_media_domain": false, "is_robot_indexable": true, "is_self": true, "is_video": false, "link_flair_background_color": "#7193ff", "link_flair_css_class": "", "link_flair_richtext": [], "link_flair_template_id": "6fa65b48-aa12-11ec-bf3b-8a8d7167d101", "link_flair_text": "\ud83c\udf2cINSPIRATIONAL\u2755", "link_flair_text_color": "light", "link_flair_type": "text", "locked": false, "media": null, "media_embed": {}, "media_only": false, "name": "t3_102j8bw", "no_follow": false, "num_comments": 32, "num_crossposts": 0, "over_18": true, "parent_whitelist_status": null, "permalink": "/r/gettingbigger/comments/102j8bw/wife_noticed_this_morning_sort_of/", "pinned": false, "pwls": null, "quarantine": false, "removed_by": null, "removed_by_category": null, "retrieved_on": 1676249239, "score": 56, "secure_media": null, "secure_media_embed": {}, "selftext": "This morning my wife saw me putting the condom on and said \u201cthose condoms are smaller, are you ok using it like that?\u201d  She insisted she bought the same kind but these MUST be smaller. \n\nIn 3 months I\u2019ve gained 1.25 cm in length. I haven\u2019t tracked girth as closely because I was already 6.25\u201d BEG a shade under 6 MSEG. Now I measure at 6 1/8\u201d MSEG and almost 6.5\u201d BEG.", "send_replies": true, "spoiler": false, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_subscribers": 57470, "subreddit_type": "public", "suggested_sort": null, "thumbnail": "nsfw", "thumbnail_height": null, "thumbnail_width": null, "title": "Wife noticed this morning. Sort of\u2026", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "upvote_ratio": 0.9800000190734863, "url": "https://www.reddit.com/r/gettingbigger/comments/102j8bw/wife_noticed_this_morning_sort_of/", "view_count": null, "whitelist_status": null, "wls": null, "ups": 56}

================
File: src/data/data_loader.py
================
import json
import logging
from typing import Tuple, List, Dict, Any
from pathlib import Path

logger = logging.getLogger(__name__)

class DataLoader:
    def __init__(self, posts_file: str, comments_file: str):
        self.posts_file = Path(posts_file)
        self.comments_file = Path(comments_file)

    def load_data(self) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """Load posts and comments from JSON files in chunks"""
        try:
            logger.info(f"Loading posts from {self.posts_file}")
            posts = []
            with open(self.posts_file, 'r') as f:
                for line in f:
                    if line.strip():
                        try:
                            posts.append(json.loads(line.strip()))
                        except json.JSONDecodeError:
                            logger.warning(f"Skipped invalid JSON line in posts file")
                    if len(posts) % 1000 == 0:
                        logger.info(f"Loaded {len(posts)} posts...")
                        
            logger.info(f"Loading comments from {self.comments_file}")
            comments = []
            with open(self.comments_file, 'r') as f:
                for line in f:
                    if line.strip():
                        try:
                            comments.append(json.loads(line.strip()))
                        except json.JSONDecodeError:
                            logger.warning(f"Skipped invalid JSON line in comments file")
                    if len(comments) % 5000 == 0:
                        logger.info(f"Loaded {len(comments)} comments...")
                        
            logger.info(f"Loaded {len(posts)} posts and {len(comments)} comments")
            return posts, comments
        except Exception as e:
            logger.error(f"Error loading data: {e}")
            raise

================
File: src/models/data_classes.py
================
from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Optional

@dataclass
class RedditComment:
    """Data class for storing normalized Reddit comment data"""
    comment_id: str
    post_id: str  # The ID of the parent post (link_id)
    parent_id: str  # Could be post_id or another comment_id
    content: str
    author: str
    timestamp: datetime
    score: int
    edited: bool
    intent: str = None
    topics: List[dict[str, float]] = field(default_factory=list)  # List of {topic_id: probability} mappings
    dominant_topic: Optional[int] = None  # ID of the topic with highest probability
    topic_probabilities: dict[int, float] = field(default_factory=dict)  # Full topic distribution
    replies: List['RedditComment'] = field(default_factory=list) # List to store replies
    



@dataclass
class RedditPost:
    """Data class for storing normalized Reddit post data"""
    post_id: str
    title: str
    content: str
    author: str
    timestamp: datetime
    score: int
    num_comments: int
    upvote_ratio: float
    over_18: bool
    edited: bool
    comments: List[RedditComment] = field(default_factory=list)
    intent: str = None
     # New topic-related fields
    topics: List[dict[str, float]] = field(default_factory=list)  # List of {topic_id: probability} mappings
    title_topics: List[dict[str, float]] = field(default_factory=list)  # Topic distribution for title
    content_topics: List[dict[str, float]] = field(default_factory=list)  # Topic distribution for content
    dominant_topic: Optional[int] = None  # ID of the topic with highest probability
    topic_probabilities: dict[int, float] = field(default_factory=dict)  # Full topic distribution
    conversation_analysis: Optional[List[dict[str, any]]] = None # Add field for conversation analysis

================
File: src/nlp/analyzer.py
================
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import torch
import logging
from typing import List, Any
import logging
import pandas as pd
import numpy as np
from typing import List, Dict, Any
from sklearn.cluster import KMeans  # Or DBSCAN, AgglomerativeClustering, etc.
from sklearn.multioutput import MultiOutputClassifier
from sklearn.linear_model import LogisticRegression  # Or other multi-label classifiers
from sentence_transformers import SentenceTransformer, util
import umap
import networkx as nx
from pyvis.network import Network
import matplotlib.pyplot as plt
from collections import Counter
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
import hdbscan
nltk.download('stopwords', quiet=True)
nltk.download('punkt', quiet=True)

logger = logging.getLogger(__name__)

class NLPAnalyzer:
    def __init__(self, batch_size: int = 128, use_gpu: bool = True):
        self.batch_size = batch_size
        self.device = torch.device("cuda" if torch.cuda.is_available() and use_gpu else "cpu")
        
        logger.info(f"Initializing NLP models on {self.device}")
        
        # Initialize tokenizer for length checking
        self.tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
        self.max_length = 512  # Maximum sequence length for the model
        
        
        self.intent_classifier = pipeline(
            "zero-shot-classification",
            model="facebook/bart-large-mnli",
            device=0 if torch.cuda.is_available() and use_gpu else -1,
            batch_size=self.batch_size
        )

    def _truncate_text(self, text: str) -> str:
        """Truncate text to fit within model's maximum sequence length"""
        tokens = self.tokenizer.encode(text, add_special_tokens=True)
        if len(tokens) > self.max_length:
            logger.debug(f"Truncating text from {len(tokens)} tokens to {self.max_length} tokens")
            truncated_tokens = tokens[:self.max_length - 1] + [tokens[-1]]  # Keep [SEP] token
            return self.tokenizer.decode(truncated_tokens, skip_special_tokens=True)
        return text

    def detect_intent(self, text: str) -> str:
        """Detect the intent of the text using zero-shot classification"""
        if not text.strip():
            return "unknown"
            
        try:
            truncated_text = self._truncate_text(text)
            result = self.intent_classifier(
                truncated_text,
                candidate_labels=["question", "opinion", "answer", "discussion"],
                hypothesis_template="This text is expressing a {}."
            )
            return result['labels'][0]
        except Exception as e:
            logger.warning(f"Error detecting intent: {e}")
            return "unknown"
    

    def process_batch(self, texts: List[str], processor_fn) -> List[Any]:
        """Process a batch of texts using the specified processor function"""
        results = []
        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]
            batch_results = [processor_fn(text) for text in batch]
            results.extend(batch_results)
        return results
    
    
class RedditDataAnalyzer:
    def __init__(self, posts_data: List[Dict], comments_data: List[Dict], conversation_trees: Dict[str, Any]):
        self.posts_data = posts_data
        self.comments_data = comments_data
        self.conversation_trees = conversation_trees
        self.sbert_model = SentenceTransformer('all-mpnet-base-v2')
        self.target_topics = ["Injury", "Routine", "Supplements", "Lifestyle", "Personal Stories", "Size Discussion"]
        self.stop_words = set(stopwords.words('english'))

    def embed_content(self, content: str) -> np.ndarray:
        return self.sbert_model.encode(content)

    def cluster_posts(self, embeddings: np.ndarray, n_clusters: int = 6) -> np.ndarray:
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        return kmeans.fit_predict(embeddings)

    def classify_posts(self, embeddings: np.ndarray, labels: np.ndarray) -> Any:
        classifier = MultiOutputClassifier(LogisticRegression())
        classifier.fit(embeddings, self._convert_to_multilabel(labels))
        return classifier

    def _convert_to_multilabel(self, labels):
        multi_labels = np.zeros((len(labels), len(self.target_topics)))
        for i, label in enumerate(labels):
            multi_labels[i, label] = 1
        return multi_labels

    def extract_key_phrases(self, text):
        # Ensure the text is not empty before processing
        if not text.strip():
            print("Text is empty, skipping TF-IDF processing.")
            return []  # Return an empty list if no valid text is available

        # Tokenize the words and filter out stop words
        words = word_tokenize(text)
        filtered_words = [w for w in words if w not in self.stop_words and w.isalnum()]
        
        # Join the filtered words back into a single string
        text = " ".join(filtered_words)
        
        if not text.strip():  # Check if the filtered text is empty
            print("Filtered text is empty, skipping TF-IDF processing.")
            return []  # Return an empty list if filtered text is empty
        
        # Apply TF-IDF Vectorization
        vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5)
        tfidf_matrix = vectorizer.fit_transform([text])
        
        feature_names = vectorizer.get_feature_names_out()
        phrase_scores = tfidf_matrix.toarray()[0]
        
        # Get key phrases with a non-zero score
        key_phrases = [phrase for phrase, score in zip(feature_names, phrase_scores) if score > 0]
        
        return key_phrases

    def analyze_conversation_tree(self, post_id: str):
        tree = self.conversation_trees.get(post_id)
        if not tree:
            return None

        conversation_flow = []
        root_embedding = self.embed_content(tree.content)
        MAX_DEPTH = 10

        def traverse_tree(node, parent_embedding, depth=0):
            if depth > MAX_DEPTH or not hasattr(node, 'comments'):
                return
            for comment in node.comments:
                comment_embedding = self.embed_content(comment.content)
                similarity = util.cos_sim(parent_embedding, comment_embedding).item()
                conversation_flow.append({
                    'comment': comment.content,
                    'similarity_to_parent': similarity
                })
                traverse_tree(comment, comment_embedding, depth + 1)

    def visualize_clusters(self, embeddings, labels):
        reducer = umap.UMAP(n_components=2, random_state=42)
        embeddings_2d = reducer.fit_transform(embeddings)

        plt.figure(figsize=(10, 8))
        for i in np.unique(labels):
            plt.scatter(embeddings_2d[labels == i, 0], embeddings_2d[labels == i, 1], label=str(i))
        plt.legend()
        plt.title('UMAP Visualization of Post Clusters')
        plt.show()

    def visualize_conversation_tree(self, post_id):
        net = Network(notebook=True, cdn_resources='in_line')
        tree = self.conversation_trees[post_id]

        def add_to_graph(node, parent_id=None):
            node_id = node.post_id if hasattr(node, 'post_id') else node.comment_id
            content = getattr(node, 'content', '')
            net.add_node(node_id, label=self.get_shortened_text(content))
            if parent_id:
                net.add_edge(parent_id, node_id)

            for comment in getattr(node, 'comments', []):
                add_to_graph(comment, node_id)

        add_to_graph(tree)
        net.show("conversation_tree.html")

    def get_shortened_text(self, text, max_length=20):
        if len(text) <= max_length:
            return text
        return text[:max_length] + "..."

    def export_to_markdown(self, clusters, output_path='analysis_summary.md'):
        with open(output_path, 'w') as md_file:
            md_file.write("# Analysis Summary\n\n")
            md_file.write(f"## Total Clusters: {len(clusters)}\n\n")
            for idx, cluster in enumerate(clusters):
                md_file.write(f"### Cluster {idx + 1}\n")
                md_file.write(f"- **Cluster Name**: {cluster['name']}\n")
                md_file.write(f"- **Number of Conversations**: {len(cluster['conversations'])}\n")
                md_file.write("- **Top Terms**:\n")
                for term in cluster['top_terms']:
                    md_file.write(f"  - {term}\n")
                md_file.write("\n#### Conversation Trees\n")
                for conv in cluster['conversations']:
                    self._write_conversation_tree(conv, md_file)
            logger.info(f"Exported analysis summary to {output_path}")

    def _write_conversation_tree(self, conversation, md_file):
        for comment in conversation['comments']:
            md_file.write(f"  - {comment['text']} (by {comment['user']})\n")

    def run_analysis(self):
        post_embeddings = [self.embed_content(post.content) for post in self.posts_data]
        post_embeddings = np.array(post_embeddings)
        cluster_labels = self.cluster_posts(post_embeddings)
        classifier = self.classify_posts(post_embeddings, cluster_labels)
        predicted_multi_labels = classifier.predict(post_embeddings)
        
        clustered_posts = {}
        for i, label in enumerate(cluster_labels):
            if label not in clustered_posts:
                clustered_posts[label] = []
            clustered_posts[label].append(self.posts_data[i])

        cluster_keywords = {}
        for label, posts in clustered_posts.items():
            all_text = " ".join([post.content for post in posts])
            keywords = self.extract_key_phrases(all_text)
            cluster_keywords[label] = keywords
        
        for label, keywords in cluster_keywords.items():
            logger.info(f"Cluster {label}: {keywords}")

        for post in self.posts_data:
            conversation_analysis = self.analyze_conversation_tree(post.post_id)
            post.conversation_analysis = conversation_analysis
            if conversation_analysis:
                logger.info(f"Conversation analysis for post {post['post_id']}: {conversation_analysis}")

        self.visualize_clusters(post_embeddings, cluster_labels)
        for post_id in self.conversation_trees:
            self.visualize_conversation_tree(post_id)
            break

================
File: src/processors/comment_processor.py
================
from datetime import datetime
from typing import Dict, Any
import logging
from ..models.data_classes import RedditComment
from ..utils.text_processor import TextProcessor
from ..nlp.analyzer import NLPAnalyzer

logger = logging.getLogger(__name__)

class CommentProcessor:
    def __init__(self, nlp_analyzer: NLPAnalyzer):
        self.nlp_analyzer = nlp_analyzer

    def process_comment(self, comment_data: Dict[str, Any]) -> RedditComment:
        """Process a single Reddit comment"""
        try:
            comment_id = TextProcessor.parse_reddit_id(comment_data.get('name', ''))
            post_id = TextProcessor.parse_reddit_id(comment_data.get('link_id', ''))
            parent_id = TextProcessor.parse_reddit_id(comment_data.get('parent_id', ''))
            
            # Clean the content
            cleaned_content = TextProcessor.clean_text(comment_data.get('body', ''))
            
            comment = RedditComment(
                comment_id=comment_id,
                post_id=post_id,
                parent_id=parent_id,
                content=cleaned_content,
                author=comment_data.get('author', '[deleted]'),
                timestamp=datetime.fromtimestamp(comment_data.get('created_utc', 0)),
                score=comment_data.get('score', 0),
                edited=bool(comment_data.get('edited', False))
            )
            
            # Add intent and sentiment analysis
            #if cleaned_content:
            #    comment.intent = self.nlp_analyzer.detect_intent(cleaned_content)
                                
            return comment
        except Exception as e:
            logger.error(f"Error processing comment {comment_data.get('name', 'unknown')}: {e}")
            raise

================
File: src/processors/conversation_processor.py
================
from typing import Dict, List, Any
from ..models.data_classes import RedditPost, RedditComment

class ConversationProcessor:
    @staticmethod
    def build_conversation_tree(posts: List[RedditPost], comments: List[RedditComment]) -> Dict[str, RedditPost]:
        """Builds a complete conversation tree, including nested replies, and sorts comments by score."""

        posts_dict = {post.post_id: post for post in posts}
        comments_dict = {comment.comment_id: comment for comment in comments}

        # Build the tree recursively
        def attach_replies(comment_id: str, current_level: list):
            replies = [c for c in comments if c.parent_id == comment_id]
            # Sort replies by score in descending order
            replies.sort(key=lambda x: x.score, reverse=True)
            for reply in replies:
                current_level.append(reply)
                attach_replies(reply.comment_id, reply.replies)  # Add nested replies

        for post in posts_dict.values():
            top_level_comments = [c for c in comments if c.parent_id == post.post_id]
            top_level_comments.sort(key=lambda x: x.score, reverse=True)  # Sort by score
            post.comments = top_level_comments
            for comment in top_level_comments:
                comment.replies = [] #Initialize replies for each comment
                attach_replies(comment.comment_id, comment.replies)
        return posts_dict


    @staticmethod
    def create_conversation_pairs(posts_dict: Dict[str, RedditPost]) -> List[Dict[str, Any]]:
        """Creates conversation pairs, handling nested replies."""
        conversation_pairs = []

        def traverse_tree(context: str, context_intent: str, context_author: str, current_level: list, post_id: str):
            for item in current_level:
                pair = {
                    'post_id': post_id,
                    'context': context,
                    'response': item.content,
                    'context_intent': context_intent or 'unknown',
                    'response_intent': item.intent or 'unknown',
                    'context_author': context_author,
                    'response_author': item.author,
                    'score': item.score,
                    'timestamp': item.timestamp.isoformat()
                }
                conversation_pairs.append(pair)
                traverse_tree(item.content, item.intent, item.author, item.replies, post_id)

        for post in posts_dict.values():
            for comment in post.comments:
                pair = {
                    'post_id': post.post_id,
                    'context': post.content,
                    'response': comment.content,
                    'context_intent': post.intent,
                    'response_intent': comment.intent or 'unknown',
                    'context_author': post.author or 'unknown',
                    'response_author': comment.author,
                    'score': comment.score,
                    'timestamp': comment.timestamp.isoformat()
                }
                conversation_pairs.append(pair)
                traverse_tree(comment.content, comment.intent, comment.author, comment.replies, post.post_id)

        return conversation_pairs

================
File: src/processors/post_processor.py
================
from datetime import datetime
from typing import Dict, Any
import logging
from ..models.data_classes import RedditPost
from ..utils.text_processor import TextProcessor
from ..nlp.analyzer import NLPAnalyzer

logger = logging.getLogger(__name__)

class PostProcessor:
    def __init__(self, nlp_analyzer: NLPAnalyzer):
        self.nlp_analyzer = nlp_analyzer

    def process_post(self, post_data: Dict[str, Any]) -> RedditPost:
        """Process a single Reddit post"""
        try:
            post_id = TextProcessor.parse_reddit_id(post_data.get('name', ''))
            
            # Clean the content
            cleaned_content = TextProcessor.clean_text(post_data.get('selftext', ''))
            cleaned_title = TextProcessor.clean_text(post_data.get('title', ''))
            
            post = RedditPost(
                post_id=post_id,
                title=cleaned_title,
                content=cleaned_content,
                author=post_data.get('author', '[deleted]'),
                timestamp=datetime.fromtimestamp(post_data.get('created_utc', 0)),
                score=post_data.get('score', 0),
                num_comments=post_data.get('num_comments', 0),
                upvote_ratio=post_data.get('upvote_ratio', 0.0),
                over_18=post_data.get('over_18', False),
                edited=bool(post_data.get('edited', False)),
                comments=[]
            )
            
            # Add intent and sentiment analysis
            if cleaned_content:
                post.intent = self.nlp_analyzer.detect_intent(cleaned_content)
                
            return post
        except Exception as e:
            logger.error(f"Error processing post {post_data.get('name', 'unknown')}: {e}")
            raise

================
File: src/utils/text_processor.py
================
import re
import emoji
from typing import List, Any
import logging

logger = logging.getLogger(__name__)

class TextProcessor:
    @staticmethod
    def clean_text(text: str) -> str:
        """Clean and normalize text content"""
        if not isinstance(text, str):
            return ""
            
        # Remove URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        
        # Remove emoji but keep the text representation
        text = emoji.demojize(text)
        
        # Remove special characters but keep basic punctuation
        text = re.sub(r'[^\w\s.,!?]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()

    @staticmethod
    def parse_reddit_id(full_id: str) -> str:
        """Extract the base ID from Reddit's fullname format"""
        if full_id and '_' in full_id:
            return full_id.split('_')[1]
        return full_id

================
File: src/data_processor.py
================
# src/data_processor.py
from .data.data_loader import DataLoader
from .nlp.analyzer import NLPAnalyzer
from .processors.post_processor import PostProcessor
from .processors.comment_processor import CommentProcessor
from .processors.conversation_processor import ConversationProcessor
from .models.data_classes import RedditPost, RedditComment


import logging
import json
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any, Generator, Iterator, Optional
from sklearn.model_selection import train_test_split
from dataclasses import asdict
import torch
from tqdm import tqdm
import gc
from datetime import datetime
import time
from .nlp.analyzer import NLPAnalyzer
from .data.data_loader import DataLoader
from .processors.post_processor import PostProcessor
from .processors.comment_processor import CommentProcessor
from .processors.conversation_processor import ConversationProcessor
from .models.data_classes import RedditPost, RedditComment
import sys

logger = logging.getLogger(__name__)



def setup_logging(output_dir: Path) -> logging.Logger:
    """Configure enhanced logging with rich handler and file output"""
    log_file = output_dir / f"processing_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s | %(levelname)s | %(message)s",
        handlers=[
            RichHandler(rich_tracebacks=True, console=console),
            logging.FileHandler(log_file)
        ]
    )
    
    return logging.getLogger(__name__)


class ProcessingStats:
    """Track processing statistics, timing, and system performance"""
    def __init__(self):
        self.start_time = time.time()
        self.operation_times = {}
        self.operation_counts = {}
        self.errors = []
        self.warnings = []
        self.monitoring_interval = 1.0  # seconds
        self.last_monitored = time.time()
        
    def _get_gpu_metrics(self) -> tuple[Optional[float], Optional[float]]:
        """Get GPU utilization and memory usage if available"""
        if torch.cuda.is_available():
            try:
                gpu_util = torch.cuda.utilization()
                gpu_mem = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() * 100
                return gpu_util, gpu_mem
            except:
                return None, None
        return None, None

class ProcessingStats:
    """Track processing statistics and timing"""
    def __init__(self):
        self.start_time = time.time()
        self.operation_times = {}
        self.operation_counts = {}
        self.errors = []
        self.warnings = []
        
    def start_operation(self, operation_name: str):
        """Start timing an operation"""
        self.operation_times[operation_name] = {'start': time.time()}
        
    def end_operation(self, operation_name: str, success: bool = True):
        """End timing an operation and record statistics"""
        if operation_name in self.operation_times:
            end_time = time.time()
            duration = end_time - self.operation_times[operation_name]['start']
            self.operation_times[operation_name]['duration'] = duration
            self.operation_times[operation_name]['success'] = success
            
    def add_error(self, operation: str, error: str):
        """Record an error"""
        self.errors.append({
            'timestamp': datetime.now(),
            'operation': operation,
            'error': error
        })
        
    def add_warning(self, operation: str, warning: str):
        """Record a warning"""
        self.warnings.append({
            'timestamp': datetime.now(),
            'operation': operation,
            'warning': warning
        })
        
    def generate_report(self) -> Dict:
        """Generate comprehensive processing report"""
        total_duration = time.time() - self.start_time
        
        return {
            'total_duration': total_duration,
            'operations': self.operation_times,
            'error_count': len(self.errors),
            'warning_count': len(self.warnings),
            'errors': self.errors,
            'warnings': self.warnings
        }


class GPUOptimizedProcessor:
    def __init__(self, 
                 posts_file: str, 
                 comments_file: str, 
                 output_dir: str, 
                 batch_size: int = 128,
                 chunk_size: int = 1000):
        """
        Initialize the Reddit data processor optimized for GPU processing.
        
        Args:
            posts_file (str): Path to the posts JSON file
            comments_file (str): Path to the comments JSON file
            output_dir (str): Directory for output files
            batch_size (int): Size of batches for NLP processing
            chunk_size (int): Size of chunks for data processing
        """
        self.data_loader = DataLoader(posts_file, comments_file)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # GPU optimized settings
        self.CHUNK_SIZE = chunk_size
        self.BATCH_SIZE = batch_size
        
        # Initialize components
        self.nlp_analyzer = self._initialize_nlp()
        self.post_processor = PostProcessor(self.nlp_analyzer)
        self.comment_processor = CommentProcessor(self.nlp_analyzer)
        self.conversation_processor = ConversationProcessor()
        
        # Initialize state tracking
        self.processed_posts: Optional[List[RedditPost]] = None
        self.processed_comments: Optional[List[RedditComment]] = None
        self.posts_dict: Optional[Dict[str, RedditPost]] = None

    def _initialize_nlp(self) -> NLPAnalyzer:
        """Initialize NLP analyzer with GPU support"""
        if torch.cuda.is_available():
            logger.info(f"Initializing NLP analyzer with GPU support: {torch.cuda.get_device_name(0)}")
        else:
            logger.warning("GPU not available, falling back to CPU")
        
        return NLPAnalyzer(batch_size=self.BATCH_SIZE)

    def process_in_chunks(self, 
                         items: Iterator, 
                         processor_func, 
                         desc: str,
                         chunk_size: Optional[int] = None) -> Generator:
        """
        Process items in chunks optimized for GPU.
        
        Args:
            items: Iterator of items to process
            processor_func: Function to process each item
            desc: Description for progress bar
            chunk_size: Optional override for chunk size
            
        Yields:
            Processed items one at a time
        """
        chunk_size = chunk_size or self.CHUNK_SIZE
        chunk = []
        total_processed = 0
        
        for item in tqdm(items, desc=desc):
            chunk.append(item)
            if len(chunk) >= chunk_size:
                for processed_item in self._process_chunk(chunk, processor_func):
                    yield processed_item
                    total_processed += 1
                    
                    if total_processed % (chunk_size * 5) == 0:
                        logger.info(f"Processed {total_processed} items")
                        self._cleanup_gpu_memory()
                
                chunk = []
        
        # Process remaining items
        if chunk:
            yield from self._process_chunk(chunk, processor_func)

    def _process_chunk(self, chunk: List, processor_func) -> List:
        """
        Process a single chunk of data with error handling.
        
        Args:
            chunk: List of items to process
            processor_func: Function to process each item
            
        Returns:
            List of processed items
        """
        try:
            processed_items = []
            for item in chunk:
                try:
                    processed = processor_func(item)
                    if processed is not None:
                        processed_items.append(processed)
                except Exception as e:
                    logger.error(f"Error processing individual item: {e}")
            return processed_items
        except Exception as e:
            logger.error(f"Error processing chunk: {e}")
            return []

    def _cleanup_gpu_memory(self):
        """Clean up GPU memory"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

    def save_to_file(self, 
                     data: List[Dict], 
                     filename: str, 
                     mode: str = 'w',
                     chunk_size: int = 1000):
        """
        Save processed data to file with chunking for large datasets.
        
        Args:
            data: List of data to save
            filename: Output filename
            mode: Write mode ('w' for write, 'a' for append)
            chunk_size: Size of chunks for writing
        """
        file_path = self.output_dir / filename
        
        if mode == 'w' and file_path.exists():
            file_path.unlink()
        
        try:
            if filename.endswith('.csv'):
                self._save_csv(data, file_path, mode, chunk_size)
            elif filename.endswith('.json'):
                self._save_json(data, file_path, mode, chunk_size)
            else:
                raise ValueError(f"Unsupported file format: {filename}")
                
        except Exception as e:
            logger.error(f"Error saving to {filename}: {e}")
            raise

    def _save_csv(self, 
                  data: List[Dict], 
                  file_path: Path, 
                  mode: str, 
                  chunk_size: int):
        """Save data to CSV file"""
        for i in range(0, len(data), chunk_size):
            chunk = data[i:i + chunk_size]
            df = pd.DataFrame(chunk)
            df.to_csv(
                file_path, 
                mode=mode, 
                header=(i == 0 or mode == 'w'), 
                index=False
            )

    def _save_json(self, 
                   data: List[Dict], 
                   file_path: Path, 
                   mode: str, 
                   chunk_size: int):
        """Save data to JSON file"""
        if mode == 'w':
            with open(file_path, 'w') as f:
                json.dump(data, f, indent=2)
        else:
            for i in range(0, len(data), chunk_size):
                chunk = data[i:i + chunk_size]
                with open(file_path, 'a') as f:
                    if i == 0:
                        f.write('[\n')
                    json.dump(chunk, f, indent=2)
                    if i + chunk_size < len(data):
                        f.write(',\n')
                    else:
                        f.write('\n]')

    def _process_posts(self, raw_posts: List[Dict]) -> List[RedditPost]:
        """Process all posts"""
        logger.info("Processing posts...")
        return list(self.process_in_chunks(
            raw_posts,
            self.post_processor.process_post,
            "Processing posts"
        ))

    def _process_comments(self, raw_comments: List[Dict]) -> List[RedditComment]:
        """Process all comments"""
        logger.info("Processing comments...")
        return list(self.process_in_chunks(
            raw_comments,
            self.comment_processor.process_comment,
            "Processing comments"
        ))

    def _create_conversation_pairs(self) -> List[Dict]:
        """Create conversation pairs from processed posts and comments"""
        logger.info("Creating conversation pairs...")
        self.posts_dict = self.conversation_processor.build_conversation_tree(
            self.processed_posts,
            self.processed_comments
        )
        return self.conversation_processor.create_conversation_pairs(self.posts_dict)
    

    def _split_and_save_data(self, conversation_pairs):
        """Split data into train/test sets and save if non-empty."""
        if not conversation_pairs:
            logger.warning("No conversation pairs available. Skipping train/test split and save.")
            return

        try:
            # Perform train-test split
            train_pairs, test_pairs = train_test_split(
                conversation_pairs,
                test_size=0.2,
                random_state=42
            )

            # Save training data
            train_path = self.output_dir / 'train_conversations.csv'
            pd.DataFrame(train_pairs).to_csv(train_path, index=False)
            logger.info(f"Training data saved to {train_path}")

            # Save test data
            test_path = self.output_dir / 'test_conversations.csv'
            pd.DataFrame(test_pairs).to_csv(test_path, index=False)
            logger.info(f"Test data saved to {test_path}")

        except ValueError as e:
            logger.error(f"Error splitting conversation pairs: {e}")



    def process_data(self):
        try:
            stats = ProcessingStats()
            logger.info("Starting data processing with GPU optimization...")
            
            # Load raw data
            raw_posts, raw_comments = self.data_loader.load_data()
            
            # Process posts
            self.processed_posts = self._process_posts(raw_posts)
            
            # Process comments
            self.processed_comments = self._process_comments(raw_comments)
                    
            # Create conversation pairs
            conversation_pairs = self._create_conversation_pairs()
            
            # Split and save data
            self._split_and_save_data(conversation_pairs)
            
            # Update performance metrics
                        
            logger.info("Processing completed successfully!")
            logger.info(f"Number of conversation pairs: {len(conversation_pairs)}")
        
        except Exception as e:
            logger.error(f"Error processing data: {e}")
            raise
        
        finally:
            self._cleanup_gpu_memory()
    


    def get_processing_stats(self) -> Dict[str, Any]:
        """Get statistics about the processed data"""
        stats = {
            "total_posts": len(self.processed_posts) if self.processed_posts else 0,
            "total_comments": len(self.processed_comments) if self.processed_comments else 0,
            "gpu_available": torch.cuda.is_available(),
            "gpu_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
            "batch_size": self.BATCH_SIZE,
            "chunk_size": self.CHUNK_SIZE
        }
        
        if self.posts_dict:
            stats["posts_with_comments"] = sum(
                1 for post in self.posts_dict.values() if post.comments
            )
            
        return stats

    def __enter__(self):
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        self._cleanup_gpu_memory()

================
File: src/requirements.txt
================
# Core data processing
pandas>=1.5.0
numpy>=1.21.0

# Natural Language Processing
transformers>=4.30.0
torch>=2.0.0  # Required for transformers
scikit-learn>=1.0.0  # For train_test_split

# Text processing
emoji>=2.2.0
regex>=2023.5.5  # For advanced text processing

# Progress tracking and formatting
tqdm>=4.65.0
rich>=13.0.0  # For better console output

# Type checking
typing-extensions>=4.5.0  # For advanced type hints

# Date handli
python-dateutil>=2.8.2

ujson>=5.7.0  # Faster JSON processing
numba>=0.57.0  # For numerical computations

scikit-learn>=1.0.0
scipy>=1.7.0
nltk 
spacy
sentence-transformers>=2.2.0
umap 
networkx 
pyvis 
hdbscan
umap-learn

================
File: .repomixignore
================
# Add patterns to ignore here, one per line
# Example:
# *.log
# tmp/

================
File: main.py
================
import logging
from src.data_processor import GPUOptimizedProcessor, DataLoader  # Import DataLoader
import torch
import nltk
import typer
from rich.console import Console
from rich.table import Table
from datetime import datetime
from src.nlp.analyzer import RedditDataAnalyzer  # Import the analyzer


app = typer.Typer()
console = Console()

def display_data_stats(posts, comments):
    """Displays statistics about the loaded data in a rich table."""
    table = Table(title="Reddit Data Statistics")
    table.add_column("Data Type", justify="left", style="cyan")
    table.add_column("Count", justify="right", style="green")
    table.add_row("Posts", str(len(posts)))
    table.add_row("Comments", str(len(comments)))
    console.print(table)


@app.command()
def process(
        posts_file: str = "data/gettingbigger_submissions.json",
        comments_file: str = "data/gettingbigger_comments.json",
        output_dir: str = "processed_data",
):

    """Processes Reddit data to create conversation pairs."""
    try:
        # Configure logging (if not already configured)
        log_file = f"processing_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file)
            ]
        )
        logger = logging.getLogger(__name__)


        # Download NLTK resources (if needed)
        nltk.download('stopwords', quiet=True)
        nltk.download('words', quiet=True)
        nltk.download('punkt', quiet=True)  # Use punkt instead of punkt_tab
        nltk.download('averaged_perceptron_tagger', quiet=True)

        # Log GPU availability and information
        logger.info(f"CUDA available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            logger.info(f"GPU device: {torch.cuda.get_device_name(0)}")
            logger.info(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        
        # Load data first to display stats
        data_loader = DataLoader(posts_file, comments_file) # Initialize DataLoader
        raw_posts, raw_comments = data_loader.load_data()

        # Display data statistics
        display_data_stats(raw_posts, raw_comments)


        if typer.confirm("Do you want to start the processing pipeline?", default=True):
            processor = GPUOptimizedProcessor(posts_file, comments_file, output_dir)
            processor.process_data()
            analyzer = RedditDataAnalyzer(processor.processed_posts, processor.processed_comments, processor.posts_dict)
            analyzer.run_analysis()
            processing_stats = processor.get_processing_stats()
            
            logger.info("Processing statistics:")
            for key, value in processing_stats.items():
                logger.info(f"- {key}: {value}")
        else:
            logger.info("Processing pipeline aborted by the user.")

    except Exception as e:
        logger.error(f"Error in processing: {e}")
        raise



if __name__ == "__main__":
    app()

================
File: package.json
================
{
  "dependencies": {
    "malloc": "^1.1.0"
  }
}

================
File: repomix.config.json
================
{
  "output": {
    "filePath": "repomix-output.txt",
    "style": "plain",
    "removeComments": false,
    "removeEmptyLines": false,
    "topFilesLength": 5,
    "showLineNumbers": false,
    "copyToClipboard": false
  },
  "include": [],
  "ignore": {
    "useGitignore": true,
    "useDefaultPatterns": true,
    "customPatterns": []
  },
  "security": {
    "enableSecurityCheck": true
  }
}

This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-11-13T02:28:27.006Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
data/
  gettingbigger_comments.json
  gettingbigger_submissions.json
src/
  data/
    data_loader.py
  models/
    data_classes.py
  nlp/
    analyzer.py
  processors/
    comment_processor.py
    conversation_processor.py
    post_processor.py
  utils/
    text_processor.py
  data_processor.py
  requirements.txt
.repomixignore
main.py
package.json
repomix.config.json

================================================================
Repository Files
================================================================

================
File: data/gettingbigger_comments.json
================
{"all_awardings":[],"associated_award":null,"author":"bd19962015","author_created_utc":null,"author_flair_background_color":"#ffd635","author_flair_css_class":null,"author_flair_richtext":[],"author_flair_template_id":"95259e42-38f3-11eb-bb8e-0ebbfab788fb","author_flair_text":"King Chungus \ud83d\udc51","author_flair_text_color":"dark","author_flair_type":"text","author_fullname":"t2_2w5g9689","author_patreon_flair":false,"author_premium":false,"awarders":[],"body":"So r\/gettingbigger is where I'm going to posting my guides and such from now on. Feel free to subscribe. Still building it out.","can_gild":true,"can_mod_post":false,"collapsed":false,"collapsed_because_crowd_control":null,"collapsed_reason":null,"comment_type":null,"controversiality":0,"created_utc":1607380476,"distinguished":null,"edited":false,"gilded":0,"gildings":{},"id":"gezqh3u","is_submitter":true,"link_id":"t3_k8qlmp","locked":false,"no_follow":false,"parent_id":"t3_k8qlmp","permalink":"\/r\/gettingbigger\/comments\/k8qlmp\/part_1_penis_growth_the_causes_and_phases\/gezqh3u\/","quarantined":false,"removal_reason":null,"retrieved_on":1618512743,"score":22,"send_replies":true,"stickied":false,"subreddit":"gettingbigger","subreddit_id":"t5_3iyfvr","subreddit_name_prefixed":"r\/gettingbigger","subreddit_type":"public","top_awarded_type":null,"total_awards_received":0,"treatment_tags":[]}
{"all_awardings":[],"associated_award":null,"author":"biillybob5","author_created_utc":null,"author_flair_background_color":null,"author_flair_css_class":null,"author_flair_richtext":[],"author_flair_template_id":null,"author_flair_text":null,"author_flair_text_color":null,"author_flair_type":"text","author_fullname":"t2_6lstoync","author_patreon_flair":false,"author_premium":false,"awarders":[],"body":"good info","can_gild":true,"can_mod_post":false,"collapsed":false,"collapsed_because_crowd_control":null,"collapsed_reason":null,"comment_type":null,"controversiality":0,"created_utc":1607383703,"distinguished":null,"edited":false,"gilded":0,"gildings":{},"id":"gezwt6d","is_submitter":false,"link_id":"t3_k8qlmp","locked":false,"no_follow":false,"parent_id":"t3_k8qlmp","permalink":"\/r\/gettingbigger\/comments\/k8qlmp\/part_1_penis_growth_the_causes_and_phases\/gezwt6d\/","quarantined":false,"removal_reason":null,"retrieved_on":1618515700,"score":5,"send_replies":true,"stickied":false,"subreddit":"gettingbigger","subreddit_id":"t5_3iyfvr","subreddit_name_prefixed":"r\/gettingbigger","subreddit_type":"public","top_awarded_type":null,"total_awards_received":0,"treatment_tags":[]}
{"all_awardings":[],"associated_award":null,"author":"GCollector4279","author_created_utc":1477715594,"author_flair_background_color":null,"author_flair_css_class":null,"author_flair_richtext":[],"author_flair_template_id":null,"author_flair_text":null,"author_flair_text_color":null,"author_flair_type":"text","author_fullname":"t2_12g9dw","author_patreon_flair":false,"author_premium":false,"awarders":[],"body":"Excited to learn. Thanks !","can_gild":true,"can_mod_post":false,"collapsed":false,"collapsed_because_crowd_control":null,"collapsed_reason":null,"comment_type":null,"controversiality":0,"created_utc":1607386069,"distinguished":null,"edited":false,"gilded":0,"gildings":{},"id":"gf01a18","is_submitter":false,"link_id":"t3_k8qlmp","locked":false,"no_follow":false,"parent_id":"t3_k8qlmp","permalink":"\/r\/gettingbigger\/comments\/k8qlmp\/part_1_penis_growth_the_causes_and_phases\/gf01a18\/","quarantined":false,"removal_reason":null,"retrieved_on":1618517785,"score":5,"send_replies":true,"stickied":false,"subreddit":"gettingbigger","subreddit_id":"t5_3iyfvr","subreddit_name_prefixed":"r\/gettingbigger","subreddit_type":"public","top_awarded_type":null,"total_awards_received":0,"treatment_tags":[]}
{"all_awardings":[],"associated_award":null,"author":"flappityflop","author_created_utc":null,"author_flair_background_color":null,"author_flair_css_class":null,"author_flair_richtext":[],"author_flair_template_id":null,"author_flair_text":null,"author_flair_text_color":null,"author_flair_type":"text","author_fullname":"t2_4opdosho","author_patreon_flair":false,"author_premium":false,"awarders":[],"body":"If I'm mostly interested in girth gains, will some manual stretching between girth exercises help to make the tunica more malleable? Or is it wasted effort for girth only?","can_gild":true,"can_mod_post":false,"collapsed":false,"collapsed_because_crowd_control":null,"collapsed_reason":null,"comment_type":null,"controversiality":0,"created_utc":1607392745,"distinguished":null,"edited":false,"gilded":0,"gildings":{},"id":"gf0ds8f","is_submitter":false,"link_id":"t3_k8qlmp","locked":false,"no_follow":true,"parent_id":"t3_k8qlmp","permalink":"\/r\/gettingbigger\/comments\/k8qlmp\/part_1_penis_growth_the_causes_and_phases\/gf0ds8f\/","quarantined":false,"removal_reason":null,"retrieved_on":1618523618,"score":2,"send_replies":true,"stickied":false,"subreddit":"gettingbigger","subreddit_id":"t5_3iyfvr","subreddit_name_prefixed":"r\/gettingbigger","subreddit_type":"public","top_awarded_type":null,"total_awards_received":0,"treatment_tags":[]}
{"all_awardings":[],"associated_award":null,"author":"bd19962015","author_created_utc":null,"author_flair_background_color":"#ffd635","author_flair_css_class":null,"author_flair_richtext":[],"author_flair_template_id":"95259e42-38f3-11eb-bb8e-0ebbfab788fb","author_flair_text":"King Chungus \ud83d\udc51","author_flair_text_color":"dark","author_flair_type":"text","author_fullname":"t2_2w5g9689","author_patreon_flair":false,"author_premium":false,"awarders":[],"body":"A few things you can do\n\nSemi erect bends.\nBundled stretches and it's variants. \nStretching will help slightly. \n\nI would not call it waste if time but there are more effective exercises for tunica targeting\n\nIncorporate those before clamping or mod jelqs for best results.","can_gild":true,"can_mod_post":false,"collapsed":false,"collapsed_because_crowd_control":null,"collapsed_reason":null,"comment_type":null,"controversiality":0,"created_utc":1607392888,"distinguished":null,"edited":false,"gilded":0,"gildings":{},"id":"gf0e1t7","is_submitter":true,"link_id":"t3_k8qlmp","locked":false,"no_follow":true,"parent_id":"t1_gf0ds8f","permalink":"\/r\/gettingbigger\/comments\/k8qlmp\/part_1_penis_growth_the_causes_and_phases\/gf0e1t7\/","quarantined":false,"removal_reason":null,"retrieved_on":1618523739,"score":2,"send_replies":true,"stickied":false,"subreddit":"gettingbigger","subreddit_id":"t5_3iyfvr","subreddit_name_prefixed":"r\/gettingbigger","subreddit_type":"public","top_awarded_type":null,"total_awards_received":0,"treatment_tags":[]}
{"all_awardings":[],"associated_award":null,"author":"Dickmaxxer","author_created_utc":null,"author_flair_background_color":null,"author_flair_css_class":null,"author_flair_richtext":[],"author_flair_template_id":null,"author_flair_text":null,"author_flair_text_color":null,"author_flair_type":"text","author_fullname":"t2_8kbo7t96","author_patreon_flair":false,"author_premium":false,"awarders":[],"body":"For length I think doing the basic stretching exercises found on the wiki combined with Angion method are great combo.","can_gild":true,"can_mod_post":false,"collapsed":false,"collapsed_because_crowd_control":null,"collapsed_reason":null,"comment_type":null,"controversiality":0,"created_utc":1607397259,"distinguished":null,"edited":false,"gilded":0,"gildings":{},"id":"gf0lzic","is_submitter":false,"link_id":"t3_k8qlmp","locked":false,"no_follow":true,"parent_id":"t3_k8qlmp","permalink":"\/r\/gettingbigger\/comments\/k8qlmp\/part_1_penis_growth_the_causes_and_phases\/gf0lzic\/","quarantined":false,"removal_reason":null,"retrieved_on":1618527434,"score":1,"send_replies":true,"stickied":false,"subreddit":"gettingbigger","subreddit_id":"t5_3iyfvr","subreddit_name_prefixed":"r\/gettingbigger","subreddit_type":"public","top_awarded_type":null,"total_awards_received":0,"treatment_tags":[]}
{"all_awardings":[],"associated_award":null,"author":"MonkeyDcock","author_created_utc":null,"author_flair_background_color":null,"author_flair_css_class":null,"author_flair_richtext":[],"author_flair_template_id":null,"author_flair_text":null,"author_flair_text_color":null,"author_flair_type":"text","author_fullname":"t2_68xbf8vj","author_patreon_flair":false,"author_premium":false,"awarders":[],"body":"And where is the cell growth?","can_gild":true,"can_mod_post":false,"collapsed":false,"collapsed_because_crowd_control":null,"collapsed_reason":null,"comment_type":null,"controversiality":0,"created_utc":1607399173,"distinguished":null,"edited":false,"gilded":0,"gildings":{},"id":"gf0pep0","is_submitter":false,"link_id":"t3_k8qlmp","locked":false,"no_follow":true,"parent_id":"t3_k8qlmp","permalink":"\/r\/gettingbigger\/comments\/k8qlmp\/part_1_penis_growth_the_causes_and_phases\/gf0pep0\/","quarantined":false,"removal_reason":null,"retrieved_on":1618529042,"score":1,"send_replies":true,"stickied":false,"subreddit":"gettingbigger","subreddit_id":"t5_3iyfvr","subreddit_name_prefixed":"r\/gettingbigger","subreddit_type":"public","top_awarded_type":null,"total_awards_received":0,"treatment_tags":[]}
{"all_awardings":[],"associated_award":null,"author":"GroundbreakingView11","author_created_utc":null,"author_flair_background_color":null,"author_flair_css_class":null,"author_flair_richtext":[],"author_flair_template_id":null,"author_flair_text":null,"author_flair_text_color":null,"author_flair_type":"text","author_fullname":"t2_85i8rkzc","author_patreon_flair":false,"author_premium":false,"awarders":[],"body":"Will definitely love to see the information to come on this sub","can_gild":true,"can_mod_post":false,"collapsed":false,"collapsed_because_crowd_control":null,"collapsed_reason":null,"comment_type":null,"controversiality":0,"created_utc":1607401281,"distinguished":null,"edited":false,"gilded":0,"gildings":{},"id":"gf0t1xa","is_submitter":false,"link_id":"t3_k8qlmp","locked":false,"no_follow":true,"parent_id":"t3_k8qlmp","permalink":"\/r\/gettingbigger\/comments\/k8qlmp\/part_1_penis_growth_the_causes_and_phases\/gf0t1xa\/","quarantined":false,"removal_reason":null,"retrieved_on":1618530747,"score":1,"send_replies":true,"stickied":false,"subreddit":"gettingbigger","subreddit_id":"t5_3iyfvr","subreddit_name_prefixed":"r\/gettingbigger","subreddit_type":"public","top_awarded_type":null,"total_awards_received":0,"treatment_tags":[]}
{"all_awardings":[],"associated_award":null,"author":"ransomh","author_created_utc":null,"author_flair_background_color":null,"author_flair_css_class":null,"author_flair_richtext":[],"author_flair_template_id":null,"author_flair_text":null,"author_flair_text_color":null,"author_flair_type":"text","author_fullname":"t2_8hx7c74u","author_patreon_flair":false,"author_premium":false,"awarders":[],"body":"I'm new to this, can you tell me what wiki u're referring to?","can_gild":true,"can_mod_post":false,"collapsed":false,"collapsed_because_crowd_control":null,"collapsed_reason":null,"comment_type":null,"controversiality":0,"created_utc":1607403145,"distinguished":null,"edited":false,"gilded":0,"gildings":{},"id":"gf0w1xj","is_submitter":false,"link_id":"t3_k8qlmp","locked":false,"no_follow":false,"parent_id":"t1_gf0lzic","permalink":"\/r\/gettingbigger\/comments\/k8qlmp\/part_1_penis_growth_the_causes_and_phases\/gf0w1xj\/","quarantined":false,"removal_reason":null,"retrieved_on":1618532152,"score":2,"send_replies":true,"stickied":false,"subreddit":"gettingbigger","subreddit_id":"t5_3iyfvr","subreddit_name_prefixed":"r\/gettingbigger","subreddit_type":"public","top_awarded_type":null,"total_awards_received":0,"treatment_tags":[]}
{"all_awardings":[],"associated_award":null,"author":"coedpokepron","author_created_utc":1455065860,"author_flair_background_color":null,"author_flair_css_class":null,"author_flair_richtext":[],"author_flair_template_id":null,"author_flair_text":null,"author_flair_text_color":null,"author_flair_type":"text","author_fullname":"t2_ulo7j","author_patreon_flair":false,"author_premium":false,"awarders":[],"body":"Same","can_gild":true,"can_mod_post":false,"collapsed":false,"collapsed_because_crowd_control":null,"collapsed_reason":null,"comment_type":null,"controversiality":0,"created_utc":1607404699,"distinguished":null,"edited":false,"gilded":0,"gildings":{},"id":"gf0yd51","is_submitter":false,"link_id":"t3_k8qlmp","locked":false,"no_follow":true,"parent_id":"t1_gf0w1xj","permalink":"\/r\/gettingbigger\/comments\/k8qlmp\/part_1_penis_growth_the_causes_and_phases\/gf0yd51\/","quarantined":false,"removal_reason":null,"retrieved_on":1618533224,"score":1,"send_replies":true,"stickied":false,"subreddit":"gettingbigger","subreddit_id":"t5_3iyfvr","subreddit_name_prefixed":"r\/gettingbigger","subreddit_type":"public","top_awarded_type":null,"total_awards_received":0,"treatment_tags":[]}

================
File: data/gettingbigger_submissions.json
================
{"_meta": {"retrieved_2nd_on": 1703876276}, "all_awardings": [], "allow_live_comments": false, "approved_at_utc": null, "approved_by": null, "archived": false, "author": "Mrloudvet", "author_flair_background_color": "#0aa18f", "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": "c914e58c-38f3-11eb-ac01-0e7dc858415f", "author_flair_text": "B : 7.1x4.7  G: 7.4x5.1", "author_flair_text_color": "light", "author_flair_type": "text", "author_fullname": "t2_t30b01n3", "author_is_blocked": false, "author_patreon_flair": false, "author_premium": false, "awarders": [], "banned_at_utc": null, "banned_by": null, "can_gild": false, "can_mod_post": false, "category": null, "clicked": false, "content_categories": null, "contest_mode": false, "created": 1703746612.0, "created_utc": 1703746612.0, "discussion_type": null, "distinguished": null, "domain": "self.gettingbigger", "downs": 0, "edited": false, "gilded": 0, "gildings": {}, "hidden": false, "hide_score": false, "id": "18snn56", "is_created_from_ads_ui": false, "is_crosspostable": true, "is_meta": false, "is_original_content": false, "is_reddit_media_domain": false, "is_robot_indexable": true, "is_self": true, "is_video": false, "likes": null, "link_flair_background_color": "#ffb000", "link_flair_css_class": "", "link_flair_richtext": [], "link_flair_template_id": "611e213a-38c8-11eb-a3af-0e47ed31ccdd", "link_flair_text": "Discussion\ud83d\udde3", "link_flair_text_color": "dark", "link_flair_type": "text", "locked": false, "media": null, "media_embed": {}, "media_only": false, "mod_note": null, "mod_reason_by": null, "mod_reason_title": null, "mod_reports": [], "name": "t3_18snn56", "no_follow": true, "num_comments": 4, "num_crossposts": 0, "num_reports": 0, "over_18": true, "parent_whitelist_status": null, "permalink": "/r/gettingbigger/comments/18snn56/eq_question/", "pinned": false, "pwls": null, "quarantine": false, "removal_reason": null, "removed_by": null, "removed_by_category": null, "report_reasons": [], "retrieved_on": 1703746627, "saved": false, "score": 2, "secure_media": null, "secure_media_embed": {}, "selftext": "How\u2019s your EQ is every erection 100% 80%+ or hit or miss or even non existent ? I\u2019m trying to see if I have ED. I don\u2019t wake up with extremely hard morning wood but sometimes I get 70% morning wood. My penis isn\u2019t as hyper as I would like him to be at times. Sometimes i have good erections sometimes their 80% and sometimes their 60-70% and usually when this happens it only last for I\u2019d say a period of the day then later I\u2019m back to normal. In regard to my weight Im usually 240ish but holidays got me around 250ish  I\u2019m curious if my EQ will improve if I drop to 215 I\u2019m 6\u20192. No major health issues besides that only thing is my sleep is trash.", "send_replies": true, "spoiler": false, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_subscribers": 96406, "subreddit_type": "public", "suggested_sort": "top", "thumbnail": "nsfw", "thumbnail_height": null, "thumbnail_width": null, "title": "EQ question", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "updated_on": 1703746648, "ups": 2, "upvote_ratio": 1.0, "url": "https://www.reddit.com/r/gettingbigger/comments/18snn56/eq_question/", "user_reports": [], "view_count": null, "visited": false, "whitelist_status": null, "wls": null}
{"_meta": {"retrieved_2nd_on": 1703878211}, "all_awardings": [], "allow_live_comments": false, "approved_at_utc": null, "approved_by": null, "archived": false, "author": "Joejoes1276", "author_flair_background_color": "#d3d6da", "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": "a8a3dff8-c1e9-11ec-a01e-f27f07d63108", "author_flair_text": " B: 5.75x5.1C: 6.0x5.2G:7.5x6", "author_flair_text_color": "dark", "author_flair_type": "text", "author_fullname": "t2_j23vaiv3", "author_is_blocked": false, "author_patreon_flair": false, "author_premium": false, "awarders": [], "banned_at_utc": null, "banned_by": null, "can_gild": false, "can_mod_post": false, "category": null, "clicked": false, "content_categories": null, "contest_mode": false, "created": 1703748603.0, "created_utc": 1703748603.0, "discussion_type": null, "distinguished": null, "domain": "self.gettingbigger", "downs": 0, "edited": false, "gilded": 0, "gildings": {}, "hidden": false, "hide_score": false, "id": "18so5vj", "is_created_from_ads_ui": false, "is_crosspostable": true, "is_meta": false, "is_original_content": false, "is_reddit_media_domain": false, "is_robot_indexable": true, "is_self": true, "is_video": false, "likes": null, "link_flair_background_color": "#ffb000", "link_flair_css_class": "", "link_flair_richtext": [], "link_flair_template_id": "611e213a-38c8-11eb-a3af-0e47ed31ccdd", "link_flair_text": "Discussion\ud83d\udde3", "link_flair_text_color": "dark", "link_flair_type": "text", "locked": false, "media": null, "media_embed": {}, "media_only": false, "mod_note": null, "mod_reason_by": null, "mod_reason_title": null, "mod_reports": [], "name": "t3_18so5vj", "no_follow": true, "num_comments": 8, "num_crossposts": 0, "num_reports": 0, "over_18": true, "parent_whitelist_status": null, "permalink": "/r/gettingbigger/comments/18so5vj/a_big_thanks_to_bd_and_hink/", "pinned": false, "pwls": null, "quarantine": false, "removal_reason": null, "removed_by": null, "removed_by_category": null, "report_reasons": [], "retrieved_on": 1703748619, "saved": false, "score": 15, "secure_media": null, "secure_media_embed": {}, "selftext": "I posted pre-pump gains measurements a few days ago, check profile if your curious. Just had an amazing pumping session and hit 6.75x5.3. My no pump measurements are 6.5x5.15. Road to 7 around the corner, even had a .1 girth increase from my usual pump numbers.\nA quarter in in length is a good ass pump sesh.\nI want to thank BD and Hink for this subreddit.\nThis is life changing stuff and I may not follow their routines to the T, but I take what they preach and apply what works best for me. Not everything works for everyone, there are multiple right and wrong ways to do this I believe. Everyones body responds differently to PE. Im starting to think pumping alone is enough to illicit growth for me, just bought an Apex, haven't even used it yet, may not need to.\nAlso PeakMalePhysique pumps are goated, bar none, people on this sub keep asking me what pump I use like cmon, you should know.", "send_replies": true, "spoiler": false, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_subscribers": 96407, "subreddit_type": "public", "suggested_sort": "top", "thumbnail": "nsfw", "thumbnail_height": null, "thumbnail_width": null, "title": "A Big Thanks To BD And Hink", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "updated_on": 1703748641, "ups": 15, "upvote_ratio": 0.86, "url": "https://www.reddit.com/r/gettingbigger/comments/18so5vj/a_big_thanks_to_bd_and_hink/", "user_reports": [], "view_count": null, "visited": false, "whitelist_status": null, "wls": null}
{"_meta": {"retrieved_2nd_on": 1703879376}, "all_awardings": [], "allow_live_comments": false, "approved_at_utc": null, "approved_by": null, "archived": false, "author": "ConferenceNo8221", "author_flair_background_color": "", "author_flair_css_class": "", "author_flair_richtext": [], "author_flair_template_id": null, "author_flair_text": "Note: new or low karma account", "author_flair_text_color": "dark", "author_flair_type": "text", "author_fullname": "t2_nl2d3hs99", "author_is_blocked": false, "author_patreon_flair": false, "author_premium": false, "awarders": [], "banned_at_utc": null, "banned_by": null, "can_gild": false, "can_mod_post": false, "category": null, "clicked": false, "content_categories": null, "contest_mode": false, "created": 1703749767.0, "created_utc": 1703749767.0, "discussion_type": null, "distinguished": null, "domain": "i.redd.it", "downs": 0, "edited": false, "gilded": 0, "gildings": {}, "hidden": false, "hide_score": false, "id": "18soghw", "is_created_from_ads_ui": false, "is_crosspostable": true, "is_meta": false, "is_original_content": false, "is_reddit_media_domain": true, "is_robot_indexable": true, "is_self": false, "is_video": false, "likes": null, "link_flair_background_color": "#dadada", "link_flair_css_class": "", "link_flair_richtext": [], "link_flair_template_id": "0f34328c-38f6-11eb-97fe-0ee00d3fadaf", "link_flair_text": "Question \u2753", "link_flair_text_color": "dark", "link_flair_type": "text", "locked": false, "media": null, "media_embed": {}, "media_only": false, "mod_note": null, "mod_reason_by": null, "mod_reason_title": null, "mod_reports": [], "name": "t3_18soghw", "no_follow": true, "num_comments": 8, "num_crossposts": 0, "num_reports": 0, "over_18": true, "parent_whitelist_status": null, "permalink": "/r/gettingbigger/comments/18soghw/got_a_2x9_la_pump_and_dont_expand_like_usual/", "pinned": false, "post_hint": "image", "preview": {"enabled": true, "images": [{"id": "AlE4FK4HEw4CSRm6OZbl9iU9VzOeqU0xNaxBpIn41vo", "resolutions": [{"height": 81, "url": "https://preview.redd.it/d0cnlcisqz8c1.jpeg?width=108&crop=smart&auto=webp&s=b52d354c8748b292842223c8f3543d894ebd3d10", "width": 108}, {"height": 162, "url": "https://preview.redd.it/d0cnlcisqz8c1.jpeg?width=216&crop=smart&auto=webp&s=0296eecbf7be1ab614ed147cb7d9dd69c7f0c7fd", "width": 216}, {"height": 240, "url": "https://preview.redd.it/d0cnlcisqz8c1.jpeg?width=320&crop=smart&auto=webp&s=29f7bd0a8ae809b965b34f04fbc8536cc8e6a332", "width": 320}, {"height": 480, "url": "https://preview.redd.it/d0cnlcisqz8c1.jpeg?width=640&crop=smart&auto=webp&s=c4237560cfc08ceabc3ab6382bd3aaaaeb91b0e2", "width": 640}, {"height": 720, "url": "https://preview.redd.it/d0cnlcisqz8c1.jpeg?width=960&crop=smart&auto=webp&s=d697dfafd76ca66c1c59b17258e1b3b9decdcef9", "width": 960}, {"height": 810, "url": "https://preview.redd.it/d0cnlcisqz8c1.jpeg?width=1080&crop=smart&auto=webp&s=fd4df659f38d564be078cc1477420fe5e67bcd03", "width": 1080}], "source": {"height": 3024, "url": "https://preview.redd.it/d0cnlcisqz8c1.jpeg?auto=webp&s=75a39f523f0faef2848be73d9822484053675e2d", "width": 4032}, "variants": {"nsfw": {"resolutions": [{"height": 81, "url": "https://preview.redd.it/d0cnlcisqz8c1.jpeg?width=108&crop=smart&blur=10&format=pjpg&auto=webp&s=93592e2d987872f7d08b03b94ce6431f9fb96c61", "width": 108}, {"height": 162, "url": "https://preview.redd.it/d0cnlcisqz8c1.jpeg?width=216&crop=smart&blur=21&format=pjpg&auto=webp&s=15ed13b6f885ffddbcda745a1c5af1fe8e51b85f", "width": 216}, {"height": 240, "url": "https://preview.redd.it/d0cnlcisqz8c1.jpeg?width=320&crop=smart&blur=32&format=pjpg&auto=webp&s=4af3e925b1c26c766500d16a0a41c44a77c378fe", "width": 320}, {"height": 480, "url": "https://preview.redd.it/d0cnlcisqz8c1.jpeg?width=640&crop=smart&blur=40&format=pjpg&auto=webp&s=8013d21a5a6cd3b6d86a5d83e0b81a2223e8d055", "width": 640}, {"height": 720, "url": "https://preview.redd.it/d0cnlcisqz8c1.jpeg?width=960&crop=smart&blur=40&format=pjpg&auto=webp&s=4ab67d7842e381e9e0407a86ec2c813d8e55538d", "width": 960}, {"height": 810, "url": "https://preview.redd.it/d0cnlcisqz8c1.jpeg?width=1080&crop=smart&blur=40&format=pjpg&auto=webp&s=74dcc1dded30042241addb1aa3ec28f10b84d188", "width": 1080}], "source": {"height": 3024, "url": "https://preview.redd.it/d0cnlcisqz8c1.jpeg?blur=40&format=pjpg&auto=webp&s=65ede1cca9db01a6f3486d03e53999a584402461", "width": 4032}}, "obfuscated": {"resolutions": [{"height": 81, "url": "https://preview.redd.it/d0cnlcisqz8c1.jpeg?width=108&crop=smart&blur=10&format=pjpg&auto=webp&s=93592e2d987872f7d08b03b94ce6431f9fb96c61", "width": 108}, {"height": 162, "url": "https://preview.redd.it/d0cnlcisqz8c1.jpeg?width=216&crop=smart&blur=21&format=pjpg&auto=webp&s=15ed13b6f885ffddbcda745a1c5af1fe8e51b85f", "width": 216}, {"height": 240, "url": "https://preview.redd.it/d0cnlcisqz8c1.jpeg?width=320&crop=smart&blur=32&format=pjpg&auto=webp&s=4af3e925b1c26c766500d16a0a41c44a77c378fe", "width": 320}, {"height": 480, "url": "https://preview.redd.it/d0cnlcisqz8c1.jpeg?width=640&crop=smart&blur=40&format=pjpg&auto=webp&s=8013d21a5a6cd3b6d86a5d83e0b81a2223e8d055", "width": 640}, {"height": 720, "url": "https://preview.redd.it/d0cnlcisqz8c1.jpeg?width=960&crop=smart&blur=40&format=pjpg&auto=webp&s=4ab67d7842e381e9e0407a86ec2c813d8e55538d", "width": 960}, {"height": 810, "url": "https://preview.redd.it/d0cnlcisqz8c1.jpeg?width=1080&crop=smart&blur=40&format=pjpg&auto=webp&s=74dcc1dded30042241addb1aa3ec28f10b84d188", "width": 1080}], "source": {"height": 3024, "url": "https://preview.redd.it/d0cnlcisqz8c1.jpeg?blur=40&format=pjpg&auto=webp&s=65ede1cca9db01a6f3486d03e53999a584402461", "width": 4032}}}}]}, "pwls": null, "quarantine": false, "removal_reason": null, "removed_by": null, "removed_by_category": null, "report_reasons": [], "retrieved_on": 1703749781, "saved": false, "score": 2, "secure_media": null, "secure_media_embed": {}, "selftext": "I usually hit almost 8 in the pump only 7 in this tight pump", "send_replies": true, "spoiler": true, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_subscribers": 96409, "subreddit_type": "public", "suggested_sort": "top", "thumbnail": "nsfw", "thumbnail_height": 105, "thumbnail_width": 140, "title": "Got a 2x9 La pump and don\u2019t expand like usual", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "updated_on": 1703749799, "ups": 2, "upvote_ratio": 0.63, "url": "https://i.redd.it/d0cnlcisqz8c1.jpeg", "url_overridden_by_dest": "https://i.redd.it/d0cnlcisqz8c1.jpeg", "user_reports": [], "view_count": null, "visited": false, "whitelist_status": null, "wls": null}
{"_meta": {"retrieved_2nd_on": 1703880701}, "all_awardings": [], "allow_live_comments": false, "approved_at_utc": null, "approved_by": null, "archived": false, "author": "vindicator331", "author_flair_background_color": null, "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": null, "author_flair_text": null, "author_flair_text_color": null, "author_flair_type": "text", "author_fullname": "t2_8vujd9f6", "author_is_blocked": false, "author_patreon_flair": false, "author_premium": false, "awarders": [], "banned_at_utc": null, "banned_by": null, "can_gild": false, "can_mod_post": false, "category": null, "clicked": false, "content_categories": null, "contest_mode": false, "created": 1703751090.0, "created_utc": 1703751090.0, "discussion_type": null, "distinguished": null, "domain": "self.gettingbigger", "downs": 0, "edited": false, "gilded": 0, "gildings": {}, "hidden": false, "hide_score": false, "id": "18sota8", "is_created_from_ads_ui": false, "is_crosspostable": true, "is_meta": false, "is_original_content": false, "is_reddit_media_domain": false, "is_robot_indexable": true, "is_self": true, "is_video": false, "likes": null, "link_flair_background_color": "#dadada", "link_flair_css_class": "", "link_flair_richtext": [], "link_flair_template_id": "0f34328c-38f6-11eb-97fe-0ee00d3fadaf", "link_flair_text": "Question \u2753", "link_flair_text_color": "dark", "link_flair_type": "text", "locked": false, "media": null, "media_embed": {}, "media_only": false, "mod_note": null, "mod_reason_by": null, "mod_reason_title": null, "mod_reports": [], "name": "t3_18sota8", "no_follow": true, "num_comments": 5, "num_crossposts": 0, "num_reports": 0, "over_18": true, "parent_whitelist_status": null, "permalink": "/r/gettingbigger/comments/18sota8/edema_around_circ_scar/", "pinned": false, "pwls": null, "quarantine": false, "removal_reason": null, "removed_by": null, "removed_by_category": null, "report_reasons": [], "retrieved_on": 1703751105, "saved": false, "score": 1, "secure_media": null, "secure_media_embed": {}, "selftext": "I am a newbee to pumping, sticking to 7hg to start with a mustang 1.75\" tube air pump, 7hg feels fine and dont think it will be long before i progress higher. First two 7 mins sets are fine, but after the third I'm noticing a fair bit of edema build up around my circ scar. Is this normal?", "send_replies": true, "spoiler": false, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_subscribers": 96411, "subreddit_type": "public", "suggested_sort": "top", "thumbnail": "nsfw", "thumbnail_height": null, "thumbnail_width": null, "title": "Edema around circ scar", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "updated_on": 1703751124, "ups": 1, "upvote_ratio": 0.67, "url": "https://www.reddit.com/r/gettingbigger/comments/18sota8/edema_around_circ_scar/", "user_reports": [], "view_count": null, "visited": false, "whitelist_status": null, "wls": null}
{"_meta": {"retrieved_2nd_on": 1703882338}, "all_awardings": [], "allow_live_comments": false, "approved_at_utc": null, "approved_by": null, "archived": false, "author": "Cutelife11", "author_flair_background_color": "", "author_flair_css_class": "", "author_flair_richtext": [], "author_flair_template_id": null, "author_flair_text": "Note: new or low karma account", "author_flair_text_color": "dark", "author_flair_type": "text", "author_fullname": "t2_lemmlloh8", "author_is_blocked": false, "author_patreon_flair": false, "author_premium": false, "awarders": [], "banned_at_utc": null, "banned_by": null, "can_gild": false, "can_mod_post": false, "category": null, "clicked": false, "content_categories": null, "contest_mode": false, "created": 1703752728.0, "created_utc": 1703752728.0, "discussion_type": null, "distinguished": null, "domain": "self.gettingbigger", "downs": 0, "edited": false, "gilded": 0, "gildings": {}, "hidden": false, "hide_score": false, "id": "18sp7rf", "is_created_from_ads_ui": false, "is_crosspostable": true, "is_meta": false, "is_original_content": false, "is_reddit_media_domain": false, "is_robot_indexable": true, "is_self": true, "is_video": false, "likes": null, "link_flair_background_color": "#ffb000", "link_flair_css_class": "", "link_flair_richtext": [], "link_flair_template_id": "611e213a-38c8-11eb-a3af-0e47ed31ccdd", "link_flair_text": "Discussion\ud83d\udde3", "link_flair_text_color": "dark", "link_flair_type": "text", "locked": false, "media": null, "media_embed": {}, "media_only": false, "mod_note": null, "mod_reason_by": null, "mod_reason_title": null, "mod_reports": [], "name": "t3_18sp7rf", "no_follow": true, "num_comments": 19, "num_crossposts": 0, "num_reports": 0, "over_18": true, "parent_whitelist_status": null, "permalink": "/r/gettingbigger/comments/18sp7rf/112023_to_now_gained_you/", "pinned": false, "pwls": null, "quarantine": false, "removal_reason": null, "removed_by": null, "removed_by_category": null, "report_reasons": [], "retrieved_on": 1703752744, "saved": false, "score": 8, "secure_media": null, "secure_media_embed": {}, "selftext": "1-1-2023 to now gained you ?\nComment 2023 your gained length and girth?", "send_replies": true, "spoiler": false, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_subscribers": 96414, "subreddit_type": "public", "suggested_sort": "top", "thumbnail": "nsfw", "thumbnail_height": null, "thumbnail_width": null, "title": "1-1-2023 to now gained you ?", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "updated_on": 1703752767, "ups": 8, "upvote_ratio": 0.78, "url": "https://www.reddit.com/r/gettingbigger/comments/18sp7rf/112023_to_now_gained_you/", "user_reports": [], "view_count": null, "visited": false, "whitelist_status": null, "wls": null}
{"_meta": {"retrieved_2nd_on": 1703884901}, "all_awardings": [], "allow_live_comments": false, "approved_at_utc": null, "approved_by": null, "archived": false, "author": "JustOpening2051", "author_flair_background_color": "", "author_flair_css_class": "", "author_flair_richtext": [], "author_flair_template_id": null, "author_flair_text": "Note: new or low karma account", "author_flair_text_color": "dark", "author_flair_type": "text", "author_fullname": "t2_5853plat", "author_is_blocked": false, "author_patreon_flair": false, "author_premium": false, "awarders": [], "banned_at_utc": null, "banned_by": null, "can_gild": false, "can_mod_post": false, "category": null, "clicked": false, "content_categories": null, "contest_mode": false, "created": 1703755290.0, "created_utc": 1703755290.0, "discussion_type": null, "distinguished": null, "domain": "self.gettingbigger", "downs": 0, "edited": false, "gilded": 0, "gildings": {}, "hidden": false, "hide_score": false, "id": "18spumx", "is_created_from_ads_ui": false, "is_crosspostable": true, "is_meta": false, "is_original_content": false, "is_reddit_media_domain": false, "is_robot_indexable": true, "is_self": true, "is_video": false, "likes": null, "link_flair_background_color": "#dadada", "link_flair_css_class": "", "link_flair_richtext": [], "link_flair_template_id": "0f34328c-38f6-11eb-97fe-0ee00d3fadaf", "link_flair_text": "Question \u2753", "link_flair_text_color": "dark", "link_flair_type": "text", "locked": false, "media": null, "media_embed": {}, "media_only": false, "mod_note": null, "mod_reason_by": null, "mod_reason_title": null, "mod_reports": [], "name": "t3_18spumx", "no_follow": true, "num_comments": 7, "num_crossposts": 0, "num_reports": 0, "over_18": true, "parent_whitelist_status": null, "permalink": "/r/gettingbigger/comments/18spumx/stirlingcooper/", "pinned": false, "pwls": null, "quarantine": false, "removal_reason": null, "removed_by": null, "removed_by_category": null, "report_reasons": [], "retrieved_on": 1703755304, "saved": false, "score": 1, "secure_media": null, "secure_media_embed": {}, "selftext": "I\u2019m new and I don\u2019t want to sound stupid asking this, but have any of your read his book, How to Increase your Penis Length? Are these the methods he talks about?", "send_replies": true, "spoiler": false, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_subscribers": 96422, "subreddit_type": "public", "suggested_sort": "top", "thumbnail": "nsfw", "thumbnail_height": null, "thumbnail_width": null, "title": "StirlingCooper", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "updated_on": 1703755323, "ups": 1, "upvote_ratio": 0.5, "url": "https://www.reddit.com/r/gettingbigger/comments/18spumx/stirlingcooper/", "user_reports": [], "view_count": null, "visited": false, "whitelist_status": null, "wls": null}
{"_meta": {"retrieved_2nd_on": 1703888285}, "all_awardings": [], "allow_live_comments": false, "approved_at_utc": null, "approved_by": null, "archived": false, "author": "RoflNewb", "author_flair_background_color": null, "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": null, "author_flair_text": null, "author_flair_text_color": null, "author_flair_type": "text", "author_fullname": "t2_eox53", "author_is_blocked": false, "author_patreon_flair": false, "author_premium": false, "awarders": [], "banned_at_utc": null, "banned_by": null, "can_gild": false, "can_mod_post": false, "category": null, "clicked": false, "content_categories": null, "contest_mode": false, "created": 1703758675.0, "created_utc": 1703758675.0, "discussion_type": null, "distinguished": null, "domain": "self.gettingbigger", "downs": 0, "edited": false, "gilded": 0, "gildings": {}, "hidden": false, "hide_score": false, "id": "18sqpa2", "is_created_from_ads_ui": false, "is_crosspostable": true, "is_meta": false, "is_original_content": false, "is_reddit_media_domain": false, "is_robot_indexable": true, "is_self": true, "is_video": false, "likes": null, "link_flair_background_color": "#dadada", "link_flair_css_class": "", "link_flair_richtext": [], "link_flair_template_id": "0f34328c-38f6-11eb-97fe-0ee00d3fadaf", "link_flair_text": "Question \u2753", "link_flair_text_color": "dark", "link_flair_type": "text", "locked": false, "media": null, "media_embed": {}, "media_only": false, "mod_note": null, "mod_reason_by": null, "mod_reason_title": null, "mod_reports": [], "name": "t3_18sqpa2", "no_follow": true, "num_comments": 5, "num_crossposts": 0, "num_reports": 0, "over_18": true, "parent_whitelist_status": null, "permalink": "/r/gettingbigger/comments/18sqpa2/options_for_someone_with_substantial_foreskin/", "pinned": false, "pwls": null, "quarantine": false, "removal_reason": null, "removed_by": null, "removed_by_category": null, "report_reasons": [], "retrieved_on": 1703758692, "saved": false, "score": 1, "secure_media": null, "secure_media_embed": {}, "selftext": "First post after lurking for ages and getting envious of some of the gains here.\n\nHavent done any consistent work but am looking to get started. Mainly looking for length gains, with girth being secondary. \n\nPurchased the BD Guide but wanted to get some opinions on where the low hanging fruit is and best course of action long term. \n\nThanks", "send_replies": true, "spoiler": false, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_subscribers": 96425, "subreddit_type": "public", "suggested_sort": "top", "thumbnail": "nsfw", "thumbnail_height": null, "thumbnail_width": null, "title": "Options for someone with substantial foreskin?", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "updated_on": 1703845082, "ups": 1, "upvote_ratio": 0.67, "url": "https://www.reddit.com/r/gettingbigger/comments/18sqpa2/options_for_someone_with_substantial_foreskin/", "user_reports": [], "view_count": null, "visited": false, "whitelist_status": null, "wls": null}
{"_meta": {"retrieved_2nd_on": 1703888422}, "all_awardings": [], "allow_live_comments": false, "approved_at_utc": null, "approved_by": null, "archived": false, "author": "Express-Extreme7246", "author_flair_background_color": "#d3d6da", "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": "a8a3dff8-c1e9-11ec-a01e-f27f07d63108", "author_flair_text": "\u200c", "author_flair_text_color": "dark", "author_flair_type": "text", "author_fullname": "t2_7gjczncy9", "author_is_blocked": false, "author_patreon_flair": false, "author_premium": false, "awarders": [], "banned_at_utc": null, "banned_by": null, "can_gild": false, "can_mod_post": false, "category": null, "clicked": false, "content_categories": null, "contest_mode": false, "created": 1703758814.0, "created_utc": 1703758814.0, "discussion_type": null, "distinguished": null, "domain": "self.gettingbigger", "downs": 0, "edited": false, "gilded": 0, "gildings": {}, "hidden": false, "hide_score": false, "id": "18sqqm6", "is_created_from_ads_ui": false, "is_crosspostable": true, "is_meta": false, "is_original_content": false, "is_reddit_media_domain": false, "is_robot_indexable": true, "is_self": true, "is_video": false, "likes": null, "link_flair_background_color": "#dadada", "link_flair_css_class": "", "link_flair_richtext": [], "link_flair_template_id": "0f34328c-38f6-11eb-97fe-0ee00d3fadaf", "link_flair_text": "Question \u2753", "link_flair_text_color": "dark", "link_flair_type": "text", "locked": false, "media": null, "media_embed": {}, "media_only": false, "mod_note": null, "mod_reason_by": null, "mod_reason_title": null, "mod_reports": [], "name": "t3_18sqqm6", "no_follow": false, "num_comments": 13, "num_crossposts": 0, "num_reports": 0, "over_18": true, "parent_whitelist_status": null, "permalink": "/r/gettingbigger/comments/18sqqm6/girth_gains_with_a_pump_much_bigger_than_your/", "pinned": false, "pwls": null, "quarantine": false, "removal_reason": null, "removed_by": null, "removed_by_category": null, "report_reasons": [], "retrieved_on": 1703758833, "saved": false, "score": 1, "secure_media": null, "secure_media_embed": {}, "selftext": "So with my current girth I should be using a pump with a cylinder diameter of 1.75 inches but it's very cumbersome to ship that where I'm located. Instead I bought a tube with a 2.2 inch diameter with a rubber ring so my balls don't get sucked in. Will I still be able to make girth gains??", "send_replies": true, "spoiler": false, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_subscribers": 96425, "subreddit_type": "public", "suggested_sort": "top", "thumbnail": "nsfw", "thumbnail_height": null, "thumbnail_width": null, "title": "Girth gains with a pump much bigger than your current girth?", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "updated_on": 1703758852, "ups": 1, "upvote_ratio": 0.5, "url": "https://www.reddit.com/r/gettingbigger/comments/18sqqm6/girth_gains_with_a_pump_much_bigger_than_your/", "user_reports": [], "view_count": null, "visited": false, "whitelist_status": null, "wls": null}
{"_meta": {"retrieved_2nd_on": 1703890266}, "all_awardings": [], "allow_live_comments": false, "approved_at_utc": null, "approved_by": null, "archived": false, "author": "Damned_lurker8", "author_flair_background_color": null, "author_flair_css_class": null, "author_flair_richtext": [], "author_flair_template_id": null, "author_flair_text": null, "author_flair_text_color": null, "author_flair_type": "text", "author_fullname": "t2_4r5j4f5b", "author_is_blocked": false, "author_patreon_flair": false, "author_premium": false, "awarders": [], "banned_at_utc": null, "banned_by": null, "can_gild": false, "can_mod_post": false, "category": null, "clicked": false, "content_categories": null, "contest_mode": false, "created": 1703760656.0, "created_utc": 1703760656.0, "discussion_type": null, "distinguished": null, "domain": "self.gettingbigger", "downs": 0, "edited": false, "gilded": 0, "gildings": {}, "hidden": false, "hide_score": false, "id": "18sr7i6", "is_created_from_ads_ui": false, "is_crosspostable": true, "is_meta": false, "is_original_content": false, "is_reddit_media_domain": false, "is_robot_indexable": true, "is_self": true, "is_video": false, "likes": null, "link_flair_background_color": "#dadada", "link_flair_css_class": "", "link_flair_richtext": [], "link_flair_template_id": "0f34328c-38f6-11eb-97fe-0ee00d3fadaf", "link_flair_text": "Question \u2753", "link_flair_text_color": "dark", "link_flair_type": "text", "locked": false, "media": null, "media_embed": {}, "media_only": false, "mod_note": null, "mod_reason_by": null, "mod_reason_title": null, "mod_reports": [], "name": "t3_18sr7i6", "no_follow": false, "num_comments": 1, "num_crossposts": 0, "num_reports": 0, "over_18": true, "parent_whitelist_status": null, "permalink": "/r/gettingbigger/comments/18sr7i6/hydroquinone_recommendation/", "pinned": false, "pwls": null, "quarantine": false, "removal_reason": null, "removed_by": null, "removed_by_category": null, "report_reasons": [], "retrieved_on": 1703760676, "saved": false, "score": 1, "secure_media": null, "secure_media_embed": {}, "selftext": "I've got discoloration due tu masturbation and pumping. After asking on the sub seems like hydroquinone is the answer. Do you know any good brand of it?", "send_replies": true, "spoiler": false, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_subscribers": 96427, "subreddit_type": "public", "suggested_sort": "top", "thumbnail": "nsfw", "thumbnail_height": null, "thumbnail_width": null, "title": "Hydroquinone recommendation", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "updated_on": 1703847061, "ups": 1, "upvote_ratio": 0.6, "url": "https://www.reddit.com/r/gettingbigger/comments/18sr7i6/hydroquinone_recommendation/", "user_reports": [], "view_count": null, "visited": false, "whitelist_status": null, "wls": null}
{"_meta": {"is_edited": true, "removal_type": "deleted", "retrieved_2nd_on": 1703893167, "was_deleted_later": true}, "all_awardings": [], "allow_live_comments": false, "approved_at_utc": null, "approved_by": null, "archived": false, "author": "BumblebeeWorried4815", "author_flair_background_color": "", "author_flair_css_class": "", "author_flair_richtext": [], "author_flair_template_id": null, "author_flair_text": "Note: new or low karma account", "author_flair_text_color": "dark", "author_flair_type": "text", "author_fullname": "t2_lqdd0xemi", "author_is_blocked": false, "author_patreon_flair": false, "author_premium": false, "awarders": [], "banned_at_utc": null, "banned_by": null, "can_gild": false, "can_mod_post": false, "category": null, "clicked": false, "content_categories": null, "contest_mode": false, "created": 1703763557.0, "created_utc": 1703763557.0, "discussion_type": null, "distinguished": null, "domain": "self.gettingbigger", "downs": 0, "edited": false, "gilded": 0, "gildings": {}, "hidden": false, "hide_score": false, "id": "18srytu", "is_created_from_ads_ui": false, "is_crosspostable": true, "is_meta": false, "is_original_content": false, "is_reddit_media_domain": false, "is_robot_indexable": true, "is_self": true, "is_video": false, "likes": null, "link_flair_background_color": "#dadada", "link_flair_css_class": "", "link_flair_richtext": [], "link_flair_template_id": "0f34328c-38f6-11eb-97fe-0ee00d3fadaf", "link_flair_text": "Question \u2753", "link_flair_text_color": "dark", "link_flair_type": "text", "locked": false, "media": null, "media_embed": {}, "media_only": false, "mod_note": null, "mod_reason_by": null, "mod_reason_title": null, "mod_reports": [], "name": "t3_18srytu", "no_follow": true, "num_comments": 7, "num_crossposts": 0, "num_reports": 0, "over_18": true, "parent_whitelist_status": null, "permalink": "/r/gettingbigger/comments/18srytu/question_about_dick_growth/", "pinned": false, "pwls": null, "quarantine": false, "removal_reason": null, "removed_by": null, "removed_by_category": null, "report_reasons": [], "retrieved_on": 1703763573, "saved": false, "score": 5, "secure_media": null, "secure_media_embed": {}, "selftext": "\nTwo years when I was 18 I was a little short of 6 inches about 5.7-5.8ish. I\u2019m now close to 8 inches long about 7.8 BP. I\u2019m now 20 and is it common to grow this much at this age? I\u2019m a late bloomer so I think that plays a part in this growth and I also started lifting this last year and before the gym I had pretty much no physical exercise. I\u2019m just curious if this like a rare sorta thing or if it\u2019s common to keep growing in your 20s. I didn\u2019t know where else to post this but I figured this might be a good sub.", "send_replies": true, "spoiler": false, "stickied": false, "subreddit": "gettingbigger", "subreddit_id": "t5_3iyfvr", "subreddit_name_prefixed": "r/gettingbigger", "subreddit_subscribers": 96430, "subreddit_type": "public", "suggested_sort": "top", "thumbnail": "nsfw", "thumbnail_height": null, "thumbnail_width": null, "title": "Question about dick growth", "top_awarded_type": null, "total_awards_received": 0, "treatment_tags": [], "updated_on": 1703763595, "ups": 5, "upvote_ratio": 0.69, "url": "https://www.reddit.com/r/gettingbigger/comments/18srytu/question_about_dick_growth/", "user_reports": [], "view_count": null, "visited": false, "whitelist_status": null, "wls": null}
{"all_awardings":[],"allow_live_comments":false,"archived":false,"author":"literallituation","author_created_utc":1556551519,"author_flair_background_color":null,"author_flair_css_class":null,"author_flair_richtext":[],"author_flair_template_id":null,"author_flair_text":null,"author_flair_text_color":null,"author_flair_type":"text","author_fullname":"t2_3oomoqsu","author_patreon_flair":false,"author_premium":false,"can_gild":true,"category":null,"content_categories":null,"contest_mode":false,"created_utc":1617920729,"discussion_type":null,"distinguished":null,"domain":"self.gettingbigger","edited":false,"gilded":0,"gildings":{},"hidden":false,"hide_score":false,"id":"mn368b","is_created_from_ads_ui":false,"is_crosspostable":true,"is_meta":false,"is_original_content":false,"is_reddit_media_domain":false,"is_robot_indexable":true,"is_self":true,"is_video":false,"link_flair_background_color":"","link_flair_css_class":null,"link_flair_richtext":[],"link_flair_text":null,"link_flair_text_color":"dark","link_flair_type":"text","locked":false,"media":null,"media_embed":{},"media_only":false,"name":"t3_mn368b","no_follow":true,"num_comments":2,"num_crossposts":0,"over_18":true,"parent_whitelist_status":null,"permalink":"/r/gettingbigger/comments/mn368b/when_to_start_incorporating_different_angles_to/","pinned":false,"pwls":null,"quarantine":false,"removed_by_category":null,"retrieved_utc":1623458270,"score":1,"secure_media":null,"secure_media_embed":{},"selftext":"I recently started hanging with weight at a downward angle. When would it be beneficial to start hanging at different angles? Is BTC and straight down best for initial hanging gains?","send_replies":true,"spoiler":false,"stickied":false,"subreddit":"gettingbigger","subreddit_id":"t5_3iyfvr","subreddit_subscribers":8096,"subreddit_type":"public","suggested_sort":null,"thumbnail":"nsfw","thumbnail_height":null,"thumbnail_width":null,"title":"When to start incorporating different angles to hanging?","top_awarded_type":null,"total_awards_received":0,"treatment_tags":[],"upvote_ratio":0.6700000166893005,"url":"https://www.reddit.com/r/gettingbigger/comments/mn368b/when_to_start_incorporating_different_angles_to/","whitelist_status":null,"wls":null,"ups":1}

================
File: src/data/data_loader.py
================
import json
import logging
from typing import Tuple, List, Dict, Any
from pathlib import Path

logger = logging.getLogger(__name__)

class DataLoader:
    def __init__(self, posts_file: str, comments_file: str):
        self.posts_file = Path(posts_file)
        self.comments_file = Path(comments_file)

    def load_data(self) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """Load posts and comments from JSON files in chunks"""
        try:
            logger.info(f"Loading posts from {self.posts_file}")
            posts = []
            with open(self.posts_file, 'r') as f:
                for line in f:
                    if line.strip():
                        try:
                            posts.append(json.loads(line.strip()))
                        except json.JSONDecodeError:
                            logger.warning(f"Skipped invalid JSON line in posts file")
                    if len(posts) % 1000 == 0:
                        logger.info(f"Loaded {len(posts)} posts...")
                        
            logger.info(f"Loading comments from {self.comments_file}")
            comments = []
            with open(self.comments_file, 'r') as f:
                for line in f:
                    if line.strip():
                        try:
                            comments.append(json.loads(line.strip()))
                        except json.JSONDecodeError:
                            logger.warning(f"Skipped invalid JSON line in comments file")
                    if len(comments) % 5000 == 0:
                        logger.info(f"Loaded {len(comments)} comments...")
                        
            logger.info(f"Loaded {len(posts)} posts and {len(comments)} comments")
            return posts, comments
        except Exception as e:
            logger.error(f"Error loading data: {e}")
            raise

================
File: src/models/data_classes.py
================
from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Optional

@dataclass
class RedditComment:
    """Data class for storing normalized Reddit comment data"""
    comment_id: str
    post_id: str  # The ID of the parent post (link_id)
    parent_id: str  # Could be post_id or another comment_id
    content: str
    author: str
    timestamp: datetime
    score: int
    edited: bool
    intent: str = None
     # New topic-related fields
    topics: List[dict[str, float]] = field(default_factory=list)  # List of {topic_id: probability} mappings
    dominant_topic: Optional[int] = None  # ID of the topic with highest probability
    topic_probabilities: dict[int, float] = field(default_factory=dict)  # Full topic distribution


@dataclass
class RedditPost:
    """Data class for storing normalized Reddit post data"""
    post_id: str
    title: str
    content: str
    author: str
    timestamp: datetime
    score: int
    num_comments: int
    upvote_ratio: float
    over_18: bool
    edited: bool
    comments: List[RedditComment] = field(default_factory=list)
    intent: str = None
     # New topic-related fields
    topics: List[dict[str, float]] = field(default_factory=list)  # List of {topic_id: probability} mappings
    title_topics: List[dict[str, float]] = field(default_factory=list)  # Topic distribution for title
    content_topics: List[dict[str, float]] = field(default_factory=list)  # Topic distribution for content
    dominant_topic: Optional[int] = None  # ID of the topic with highest probability
    topic_probabilities: dict[int, float] = field(default_factory=dict)  # Full topic distribution

================
File: src/nlp/analyzer.py
================
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import torch
import logging
from typing import List, Any

logger = logging.getLogger(__name__)

class NLPAnalyzer:
    def __init__(self, batch_size: int = 128, use_gpu: bool = True):
        self.batch_size = batch_size
        self.device = torch.device("cuda" if torch.cuda.is_available() and use_gpu else "cpu")
        
        logger.info(f"Initializing NLP models on {self.device}")
        
        # Initialize tokenizer for length checking
        self.tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
        self.max_length = 512  # Maximum sequence length for the model
        
        
        self.intent_classifier = pipeline(
            "zero-shot-classification",
            model="facebook/bart-large-mnli",
            device=0 if torch.cuda.is_available() and use_gpu else -1,
            batch_size=self.batch_size
        )

    def _truncate_text(self, text: str) -> str:
        """Truncate text to fit within model's maximum sequence length"""
        tokens = self.tokenizer.encode(text, add_special_tokens=True)
        if len(tokens) > self.max_length:
            logger.debug(f"Truncating text from {len(tokens)} tokens to {self.max_length} tokens")
            truncated_tokens = tokens[:self.max_length - 1] + [tokens[-1]]  # Keep [SEP] token
            return self.tokenizer.decode(truncated_tokens, skip_special_tokens=True)
        return text

    def detect_intent(self, text: str) -> str:
        """Detect the intent of the text using zero-shot classification"""
        if not text.strip():
            return "unknown"
            
        try:
            truncated_text = self._truncate_text(text)
            result = self.intent_classifier(
                truncated_text,
                candidate_labels=["question", "opinion", "answer", "discussion"],
                hypothesis_template="This text is expressing a {}."
            )
            return result['labels'][0]
        except Exception as e:
            logger.warning(f"Error detecting intent: {e}")
            return "unknown"
    

    def process_batch(self, texts: List[str], processor_fn) -> List[Any]:
        """Process a batch of texts using the specified processor function"""
        results = []
        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]
            batch_results = [processor_fn(text) for text in batch]
            results.extend(batch_results)
        return results

================
File: src/processors/comment_processor.py
================
from datetime import datetime
from typing import Dict, Any
import logging
from ..models.data_classes import RedditComment
from ..utils.text_processor import TextProcessor
from ..nlp.analyzer import NLPAnalyzer

logger = logging.getLogger(__name__)

class CommentProcessor:
    def __init__(self, nlp_analyzer: NLPAnalyzer):
        self.nlp_analyzer = nlp_analyzer

    def process_comment(self, comment_data: Dict[str, Any]) -> RedditComment:
        """Process a single Reddit comment"""
        try:
            comment_id = TextProcessor.parse_reddit_id(comment_data.get('name', ''))
            post_id = TextProcessor.parse_reddit_id(comment_data.get('link_id', ''))
            parent_id = TextProcessor.parse_reddit_id(comment_data.get('parent_id', ''))
            
            # Clean the content
            cleaned_content = TextProcessor.clean_text(comment_data.get('body', ''))
            
            comment = RedditComment(
                comment_id=comment_id,
                post_id=post_id,
                parent_id=parent_id,
                content=cleaned_content,
                author=comment_data.get('author', '[deleted]'),
                timestamp=datetime.fromtimestamp(comment_data.get('created_utc', 0)),
                score=comment_data.get('score', 0),
                edited=bool(comment_data.get('edited', False))
            )
            
            # Add intent and sentiment analysis
            #if cleaned_content:
            #    comment.intent = self.nlp_analyzer.detect_intent(cleaned_content)
                                
            return comment
        except Exception as e:
            logger.error(f"Error processing comment {comment_data.get('name', 'unknown')}: {e}")
            raise

================
File: src/processors/conversation_processor.py
================
from typing import Dict, List, Any
from ..models.data_classes import RedditPost, RedditComment

class ConversationProcessor:
    @staticmethod
    def build_conversation_tree(posts: List[RedditPost], comments: List[RedditComment]) -> Dict[str, RedditPost]:
        """Build conversation trees by linking comments to their posts and parent comments"""
        # Create a dictionary of posts for easy lookup
        posts_dict = {post.post_id: post for post in posts}
        
        # Create a dictionary of comments for easy lookup
        comments_dict = {comment.comment_id: comment for comment in comments}
        
        # Link comments to their parents
        for comment in comments:
            if comment.post_id in posts_dict:
                # If parent is a post, add to post's comments
                if comment.parent_id == comment.post_id:
                    posts_dict[comment.post_id].comments.append(comment)
                # If parent is another comment, it's a reply
                elif comment.parent_id in comments_dict:
                    parent_comment = comments_dict[comment.parent_id]
                    if not hasattr(parent_comment, 'replies'):
                        parent_comment.replies = []
                    parent_comment.replies.append(comment)
        
        return posts_dict

    @staticmethod
    def create_conversation_pairs(posts_dict: Dict[str, RedditPost]) -> List[Dict[str, Any]]:
        """Create conversation pairs from posts and comments"""
        conversation_pairs = []
        
        for post in posts_dict.values():
            # Create pairs between post and direct comments
            for comment in post.comments:
                pair = {
                    'post_id': post.post_id,
                    'context': post.content,
                    'response': comment.content,
                    'context_intent': post.intent,
                    'response_intent': comment.intent,
                    'context_author': post.author,
                    'response_author': comment.author,
                    'score': comment.score,
                    'timestamp': comment.timestamp.isoformat()
                }
                conversation_pairs.append(pair)
                
                # If comment has replies, create pairs between comment and replies
                if hasattr(comment, 'replies'):
                    for reply in comment.replies:
                        pair = {
                            'post_id': post.post_id,
                            'context': comment.content,
                            'response': reply.content,
                            'context_intent': comment.intent,
                            'response_intent': reply.intent,
                            'context_author': comment.author,
                            'response_author': reply.author,
                            'score': reply.score,
                            'timestamp': reply.timestamp.isoformat()
                        }
                        conversation_pairs.append(pair)
        
        return conversation_pairs

================
File: src/processors/post_processor.py
================
from datetime import datetime
from typing import Dict, Any
import logging
from ..models.data_classes import RedditPost
from ..utils.text_processor import TextProcessor
from ..nlp.analyzer import NLPAnalyzer

logger = logging.getLogger(__name__)

class PostProcessor:
    def __init__(self, nlp_analyzer: NLPAnalyzer):
        self.nlp_analyzer = nlp_analyzer

    def process_post(self, post_data: Dict[str, Any]) -> RedditPost:
        """Process a single Reddit post"""
        try:
            post_id = TextProcessor.parse_reddit_id(post_data.get('name', ''))
            
            # Clean the content
            cleaned_content = TextProcessor.clean_text(post_data.get('selftext', ''))
            cleaned_title = TextProcessor.clean_text(post_data.get('title', ''))
            
            post = RedditPost(
                post_id=post_id,
                title=cleaned_title,
                content=cleaned_content,
                author=post_data.get('author', '[deleted]'),
                timestamp=datetime.fromtimestamp(post_data.get('created_utc', 0)),
                score=post_data.get('score', 0),
                num_comments=post_data.get('num_comments', 0),
                upvote_ratio=post_data.get('upvote_ratio', 0.0),
                over_18=post_data.get('over_18', False),
                edited=bool(post_data.get('edited', False)),
                comments=[]
            )
            
            # Add intent and sentiment analysis
            if cleaned_content:
                post.intent = self.nlp_analyzer.detect_intent(cleaned_content)
                
            return post
        except Exception as e:
            logger.error(f"Error processing post {post_data.get('name', 'unknown')}: {e}")
            raise

================
File: src/utils/text_processor.py
================
import re
import emoji
from typing import List, Any
import logging

logger = logging.getLogger(__name__)

class TextProcessor:
    @staticmethod
    def clean_text(text: str) -> str:
        """Clean and normalize text content"""
        if not isinstance(text, str):
            return ""
            
        # Remove URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        
        # Remove emoji but keep the text representation
        text = emoji.demojize(text)
        
        # Remove special characters but keep basic punctuation
        text = re.sub(r'[^\w\s.,!?]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()

    @staticmethod
    def parse_reddit_id(full_id: str) -> str:
        """Extract the base ID from Reddit's fullname format"""
        if full_id and '_' in full_id:
            return full_id.split('_')[1]
        return full_id

================
File: src/data_processor.py
================
# src/data_processor.py
from .data.data_loader import DataLoader
from .nlp.analyzer import NLPAnalyzer
from .processors.post_processor import PostProcessor
from .processors.comment_processor import CommentProcessor
from .processors.conversation_processor import ConversationProcessor
from .models.data_classes import RedditPost, RedditComment


import logging
import json
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any, Generator, Iterator, Optional
from sklearn.model_selection import train_test_split
from dataclasses import asdict
import torch
from tqdm import tqdm
import gc
from datetime import datetime
import time
from .nlp.analyzer import NLPAnalyzer
from .data.data_loader import DataLoader
from .processors.post_processor import PostProcessor
from .processors.comment_processor import CommentProcessor
from .processors.conversation_processor import ConversationProcessor
from .models.data_classes import RedditPost, RedditComment
import sys

logger = logging.getLogger(__name__)



def setup_logging(output_dir: Path) -> logging.Logger:
    """Configure enhanced logging with rich handler and file output"""
    log_file = output_dir / f"processing_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s | %(levelname)s | %(message)s",
        handlers=[
            RichHandler(rich_tracebacks=True, console=console),
            logging.FileHandler(log_file)
        ]
    )
    
    return logging.getLogger(__name__)


class ProcessingStats:
    """Track processing statistics, timing, and system performance"""
    def __init__(self):
        self.start_time = time.time()
        self.operation_times = {}
        self.operation_counts = {}
        self.errors = []
        self.warnings = []
        self.monitoring_interval = 1.0  # seconds
        self.last_monitored = time.time()
        
    def _get_gpu_metrics(self) -> tuple[Optional[float], Optional[float]]:
        """Get GPU utilization and memory usage if available"""
        if torch.cuda.is_available():
            try:
                gpu_util = torch.cuda.utilization()
                gpu_mem = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() * 100
                return gpu_util, gpu_mem
            except:
                return None, None
        return None, None

class ProcessingStats:
    """Track processing statistics and timing"""
    def __init__(self):
        self.start_time = time.time()
        self.operation_times = {}
        self.operation_counts = {}
        self.errors = []
        self.warnings = []
        
    def start_operation(self, operation_name: str):
        """Start timing an operation"""
        self.operation_times[operation_name] = {'start': time.time()}
        
    def end_operation(self, operation_name: str, success: bool = True):
        """End timing an operation and record statistics"""
        if operation_name in self.operation_times:
            end_time = time.time()
            duration = end_time - self.operation_times[operation_name]['start']
            self.operation_times[operation_name]['duration'] = duration
            self.operation_times[operation_name]['success'] = success
            
    def add_error(self, operation: str, error: str):
        """Record an error"""
        self.errors.append({
            'timestamp': datetime.now(),
            'operation': operation,
            'error': error
        })
        
    def add_warning(self, operation: str, warning: str):
        """Record a warning"""
        self.warnings.append({
            'timestamp': datetime.now(),
            'operation': operation,
            'warning': warning
        })
        
    def generate_report(self) -> Dict:
        """Generate comprehensive processing report"""
        total_duration = time.time() - self.start_time
        
        return {
            'total_duration': total_duration,
            'operations': self.operation_times,
            'error_count': len(self.errors),
            'warning_count': len(self.warnings),
            'errors': self.errors,
            'warnings': self.warnings
        }


class GPUOptimizedProcessor:
    def __init__(self, 
                 posts_file: str, 
                 comments_file: str, 
                 output_dir: str, 
                 batch_size: int = 128,
                 chunk_size: int = 1000):
        """
        Initialize the Reddit data processor optimized for GPU processing.
        
        Args:
            posts_file (str): Path to the posts JSON file
            comments_file (str): Path to the comments JSON file
            output_dir (str): Directory for output files
            batch_size (int): Size of batches for NLP processing
            chunk_size (int): Size of chunks for data processing
        """
        self.data_loader = DataLoader(posts_file, comments_file)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # GPU optimized settings
        self.CHUNK_SIZE = chunk_size
        self.BATCH_SIZE = batch_size
        
        # Initialize components
        self.nlp_analyzer = self._initialize_nlp()
        self.post_processor = PostProcessor(self.nlp_analyzer)
        self.comment_processor = CommentProcessor(self.nlp_analyzer)
        self.conversation_processor = ConversationProcessor()
        
        # Initialize state tracking
        self.processed_posts: Optional[List[RedditPost]] = None
        self.processed_comments: Optional[List[RedditComment]] = None
        self.posts_dict: Optional[Dict[str, RedditPost]] = None

    def _initialize_nlp(self) -> NLPAnalyzer:
        """Initialize NLP analyzer with GPU support"""
        if torch.cuda.is_available():
            logger.info(f"Initializing NLP analyzer with GPU support: {torch.cuda.get_device_name(0)}")
        else:
            logger.warning("GPU not available, falling back to CPU")
        
        return NLPAnalyzer(batch_size=self.BATCH_SIZE)

    def process_in_chunks(self, 
                         items: Iterator, 
                         processor_func, 
                         desc: str,
                         chunk_size: Optional[int] = None) -> Generator:
        """
        Process items in chunks optimized for GPU.
        
        Args:
            items: Iterator of items to process
            processor_func: Function to process each item
            desc: Description for progress bar
            chunk_size: Optional override for chunk size
            
        Yields:
            Processed items one at a time
        """
        chunk_size = chunk_size or self.CHUNK_SIZE
        chunk = []
        total_processed = 0
        
        for item in tqdm(items, desc=desc):
            chunk.append(item)
            if len(chunk) >= chunk_size:
                for processed_item in self._process_chunk(chunk, processor_func):
                    yield processed_item
                    total_processed += 1
                    
                    if total_processed % (chunk_size * 5) == 0:
                        logger.info(f"Processed {total_processed} items")
                        self._cleanup_gpu_memory()
                
                chunk = []
        
        # Process remaining items
        if chunk:
            yield from self._process_chunk(chunk, processor_func)

    def _process_chunk(self, chunk: List, processor_func) -> List:
        """
        Process a single chunk of data with error handling.
        
        Args:
            chunk: List of items to process
            processor_func: Function to process each item
            
        Returns:
            List of processed items
        """
        try:
            processed_items = []
            for item in chunk:
                try:
                    processed = processor_func(item)
                    if processed is not None:
                        processed_items.append(processed)
                except Exception as e:
                    logger.error(f"Error processing individual item: {e}")
            return processed_items
        except Exception as e:
            logger.error(f"Error processing chunk: {e}")
            return []

    def _cleanup_gpu_memory(self):
        """Clean up GPU memory"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

    def save_to_file(self, 
                     data: List[Dict], 
                     filename: str, 
                     mode: str = 'w',
                     chunk_size: int = 1000):
        """
        Save processed data to file with chunking for large datasets.
        
        Args:
            data: List of data to save
            filename: Output filename
            mode: Write mode ('w' for write, 'a' for append)
            chunk_size: Size of chunks for writing
        """
        file_path = self.output_dir / filename
        
        if mode == 'w' and file_path.exists():
            file_path.unlink()
        
        try:
            if filename.endswith('.csv'):
                self._save_csv(data, file_path, mode, chunk_size)
            elif filename.endswith('.json'):
                self._save_json(data, file_path, mode, chunk_size)
            else:
                raise ValueError(f"Unsupported file format: {filename}")
                
        except Exception as e:
            logger.error(f"Error saving to {filename}: {e}")
            raise

    def _save_csv(self, 
                  data: List[Dict], 
                  file_path: Path, 
                  mode: str, 
                  chunk_size: int):
        """Save data to CSV file"""
        for i in range(0, len(data), chunk_size):
            chunk = data[i:i + chunk_size]
            df = pd.DataFrame(chunk)
            df.to_csv(
                file_path, 
                mode=mode, 
                header=(i == 0 or mode == 'w'), 
                index=False
            )

    def _save_json(self, 
                   data: List[Dict], 
                   file_path: Path, 
                   mode: str, 
                   chunk_size: int):
        """Save data to JSON file"""
        if mode == 'w':
            with open(file_path, 'w') as f:
                json.dump(data, f, indent=2)
        else:
            for i in range(0, len(data), chunk_size):
                chunk = data[i:i + chunk_size]
                with open(file_path, 'a') as f:
                    if i == 0:
                        f.write('[\n')
                    json.dump(chunk, f, indent=2)
                    if i + chunk_size < len(data):
                        f.write(',\n')
                    else:
                        f.write('\n]')

    def _process_posts(self, raw_posts: List[Dict]) -> List[RedditPost]:
        """Process all posts"""
        logger.info("Processing posts...")
        return list(self.process_in_chunks(
            raw_posts,
            self.post_processor.process_post,
            "Processing posts"
        ))

    def _process_comments(self, raw_comments: List[Dict]) -> List[RedditComment]:
        """Process all comments"""
        logger.info("Processing comments...")
        return list(self.process_in_chunks(
            raw_comments,
            self.comment_processor.process_comment,
            "Processing comments"
        ))

    def _create_conversation_pairs(self) -> List[Dict]:
        """Create conversation pairs from processed posts and comments"""
        logger.info("Creating conversation pairs...")
        self.posts_dict = self.conversation_processor.build_conversation_tree(
            self.processed_posts,
            self.processed_comments
        )
        return self.conversation_processor.create_conversation_pairs(self.posts_dict)
    

    def _split_and_save_data(self, conversation_pairs):
        """Split data into train/test sets and save if non-empty."""
        if not conversation_pairs:
            logger.warning("No conversation pairs available. Skipping train/test split and save.")
            return

        try:
            # Perform train-test split
            train_pairs, test_pairs = train_test_split(
                conversation_pairs,
                test_size=0.2,
                random_state=42
            )

            # Save training data
            train_path = self.output_dir / 'train_conversations.csv'
            pd.DataFrame(train_pairs).to_csv(train_path, index=False)
            logger.info(f"Training data saved to {train_path}")

            # Save test data
            test_path = self.output_dir / 'test_conversations.csv'
            pd.DataFrame(test_pairs).to_csv(test_path, index=False)
            logger.info(f"Test data saved to {test_path}")

        except ValueError as e:
            logger.error(f"Error splitting conversation pairs: {e}")



    def process_data(self):
        try:
            stats = ProcessingStats()
            logger.info("Starting data processing with GPU optimization...")
            
            # Load raw data
            raw_posts, raw_comments = self.data_loader.load_data()
            
            # Process posts
            self.processed_posts = self._process_posts(raw_posts)
            
            # Process comments
            self.processed_comments = self._process_comments(raw_comments)
                    
            # Create conversation pairs
            conversation_pairs = self._create_conversation_pairs()
            
            # Split and save data
            self._split_and_save_data(conversation_pairs)
            
            # Update performance metrics
                        
            logger.info("Processing completed successfully!")
            logger.info(f"Number of conversation pairs: {len(conversation_pairs)}")
        
        except Exception as e:
            logger.error(f"Error processing data: {e}")
            raise
        
        finally:
            self._cleanup_gpu_memory()
    


    def get_processing_stats(self) -> Dict[str, Any]:
        """Get statistics about the processed data"""
        stats = {
            "total_posts": len(self.processed_posts) if self.processed_posts else 0,
            "total_comments": len(self.processed_comments) if self.processed_comments else 0,
            "gpu_available": torch.cuda.is_available(),
            "gpu_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
            "batch_size": self.BATCH_SIZE,
            "chunk_size": self.CHUNK_SIZE
        }
        
        if self.posts_dict:
            stats["posts_with_comments"] = sum(
                1 for post in self.posts_dict.values() if post.comments
            )
            
        return stats

    def __enter__(self):
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        self._cleanup_gpu_memory()

================
File: src/requirements.txt
================
# Core data processing
pandas>=1.5.0
numpy>=1.21.0

# Natural Language Processing
transformers>=4.30.0
torch>=2.0.0  # Required for transformers
scikit-learn>=1.0.0  # For train_test_split

# Text processing
emoji>=2.2.0
regex>=2023.5.5  # For advanced text processing

# Progress tracking and formatting
tqdm>=4.65.0
rich>=13.0.0  # For better console output

# Type checking
typing-extensions>=4.5.0  # For advanced type hints

# Date handli
python-dateutil>=2.8.2

ujson>=5.7.0  # Faster JSON processing
numba>=0.57.0  # For numerical computations

scikit-learn>=1.0.0
scipy>=1.7.0
nltk 
spacy
sentence-transformers>=2.2.0

================
File: .repomixignore
================
# Add patterns to ignore here, one per line
# Example:
# *.log
# tmp/

================
File: main.py
================
import logging
from src.data_processor import GPUOptimizedProcessor
import torch
import nltk
nltk.download('stopwords')
nltk.download('words')
nltk.download('punkt_tab')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def main():
    """Main function with GPU optimization"""
    try:
        # Configure paths
        posts_file = "data/gettingbigger_submissions.json"
        comments_file = "data/gettingbigger_comments.json"
        output_dir = "processed_data"
        
        # Log GPU availability
        logger.info(f"CUDA available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            logger.info(f"GPU device: {torch.cuda.get_device_name(0)}")
            logger.info(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        
        # Initialize and run the GPU-optimized processor
        processor = GPUOptimizedProcessor(posts_file, comments_file, output_dir)
        processor.process_data()
            
    except Exception as e:
        logger.error(f"Error in main: {e}")
        raise

if __name__ == "__main__":
    main()

================
File: package.json
================
{
  "dependencies": {
    "malloc": "^1.1.0"
  }
}

================
File: repomix.config.json
================
{
  "output": {
    "filePath": "repomix-output.txt",
    "style": "plain",
    "removeComments": false,
    "removeEmptyLines": false,
    "topFilesLength": 5,
    "showLineNumbers": false,
    "copyToClipboard": false
  },
  "include": [],
  "ignore": {
    "useGitignore": true,
    "useDefaultPatterns": true,
    "customPatterns": []
  },
  "security": {
    "enableSecurityCheck": true
  }
}
